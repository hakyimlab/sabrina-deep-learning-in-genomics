{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Comparing Enformer Prediction Averages\n",
        "description: 'We compare two methods of predicting the epigenome of an individual: the first was generated by running enformer on both haplotypes, then averaging the results, and the second by running enformer on the average of the haplotypes.'\n",
        "author: Sabrina Mi\n",
        "date: 7/20/23\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tensorflow as tf\n",
        "# Make sure the GPU is enabled \n",
        "assert tf.config.list_physical_devices('GPU'), 'Start the colab kernel with GPU: Runtime -> Change runtime type -> GPU'\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "import tensorflow_hub as hub # for interacting with saved models and tensorflow hub\n",
        "import joblib\n",
        "import gzip # for manipulating compressed files\n",
        "import kipoiseq # for manipulating fasta files\n",
        "from kipoiseq import Interval # same as above, really\n",
        "import pyfaidx # to index our reference genome file\n",
        "import pandas as pd # for manipulating dataframes\n",
        "import numpy as np # for numerical computations\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "import matplotlib as mpl # for plotting\n",
        "import seaborn as sns # for plotting\n",
        "import pickle # for saving large objects\n",
        "import os, sys # functions for interacting with the operating system\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "transform_path = 'gs://dm-enformer/models/enformer.finetuned.SAD.robustscaler-PCA500-robustscaler.transform.pkl'\n",
        "model_path = 'https://tfhub.dev/deepmind/enformer/1'\n",
        "fasta_file = '/home/s1mi/enformer_tutorial/genome.fa'\n",
        "targets_txt = 'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_human.txt'\n",
        "df_targets = pd.read_csv(targets_txt, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title `Enformer`, `EnformerScoreVariantsNormalized`, `EnformerScoreVariantsPCANormalized`,\n",
        "SEQUENCE_LENGTH = 393216\n",
        "\n",
        "class Enformer:\n",
        "\n",
        "  def __init__(self, tfhub_url):\n",
        "    self._model = hub.load(tfhub_url).model\n",
        "\n",
        "  def predict_on_batch(self, inputs):\n",
        "    predictions = self._model.predict_on_batch(inputs)\n",
        "    return {k: v.numpy() for k, v in predictions.items()}\n",
        "\n",
        "  @tf.function\n",
        "  def contribution_input_grad(self, input_sequence,\n",
        "                              target_mask, output_head='human'):\n",
        "    input_sequence = input_sequence[tf.newaxis]\n",
        "\n",
        "    target_mask_mass = tf.reduce_sum(target_mask)\n",
        "    with tf.GradientTape() as tape:\n",
        "      tape.watch(input_sequence)\n",
        "      prediction = tf.reduce_sum(\n",
        "          target_mask[tf.newaxis] *\n",
        "          self._model.predict_on_batch(input_sequence)[output_head]) / target_mask_mass\n",
        "\n",
        "    input_grad = tape.gradient(prediction, input_sequence) * input_sequence\n",
        "    input_grad = tf.squeeze(input_grad, axis=0)\n",
        "    return tf.reduce_sum(input_grad, axis=-1)\n",
        "\n",
        "\n",
        "class EnformerScoreVariantsRaw:\n",
        "\n",
        "  def __init__(self, tfhub_url, organism='human'):\n",
        "    self._model = Enformer(tfhub_url)\n",
        "    self._organism = organism\n",
        "\n",
        "  def predict_on_batch(self, inputs):\n",
        "    ref_prediction = self._model.predict_on_batch(inputs['ref'])[self._organism]\n",
        "    alt_prediction = self._model.predict_on_batch(inputs['alt'])[self._organism]\n",
        "\n",
        "    return alt_prediction.mean(axis=1) - ref_prediction.mean(axis=1)\n",
        "\n",
        "\n",
        "class EnformerScoreVariantsNormalized:\n",
        "\n",
        "  def __init__(self, tfhub_url, transform_pkl_path,\n",
        "               organism='human'):\n",
        "    assert organism == 'human', 'Transforms only compatible with organism=human'\n",
        "    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n",
        "    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n",
        "      transform_pipeline = joblib.load(f)\n",
        "    self._transform = transform_pipeline.steps[0][1]  # StandardScaler.\n",
        "\n",
        "  def predict_on_batch(self, inputs):\n",
        "    scores = self._model.predict_on_batch(inputs)\n",
        "    return self._transform.transform(scores)\n",
        "\n",
        "\n",
        "class EnformerScoreVariantsPCANormalized:\n",
        "\n",
        "  def __init__(self, tfhub_url, transform_pkl_path,\n",
        "               organism='human', num_top_features=500):\n",
        "    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n",
        "    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n",
        "      self._transform = joblib.load(f)\n",
        "    self._num_top_features = num_top_features\n",
        "\n",
        "  def predict_on_batch(self, inputs):\n",
        "    scores = self._model.predict_on_batch(inputs)\n",
        "    return self._transform.transform(scores)[:, :self._num_top_features]\n",
        "\n",
        "\n",
        "# TODO(avsec): Add feature description: Either PCX, or full names.\n",
        "\n",
        "\n",
        "# @title `variant_centered_sequences`\n",
        "\n",
        "class FastaStringExtractor:\n",
        "\n",
        "    def __init__(self, fasta_file):\n",
        "        self.fasta = pyfaidx.Fasta(fasta_file)\n",
        "        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n",
        "    #import pd.Interval as Interval\n",
        "    def extract(self, interval: Interval, **kwargs) -> str:\n",
        "        # Truncate interval if it extends beyond the chromosome lengths.\n",
        "        chromosome_length = self._chromosome_sizes[interval.chrom]\n",
        "        trimmed_interval = Interval(interval.chrom,\n",
        "                                    max(interval.start, 0),\n",
        "                                    min(interval.end, chromosome_length),\n",
        "                                    )\n",
        "        # pyfaidx wants a 1-based interval\n",
        "        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n",
        "                                          trimmed_interval.start + 1,\n",
        "                                          trimmed_interval.stop).seq).upper()\n",
        "        # Fill truncated values with N's.\n",
        "        pad_upstream = 'N' * max(-interval.start, 0)\n",
        "        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n",
        "        return pad_upstream + sequence + pad_downstream\n",
        "\n",
        "    def close(self):\n",
        "        return self.fasta.close()\n",
        "\n",
        "\n",
        "def one_hot_encode(sequence):\n",
        "  return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)\n",
        "\n",
        "\n",
        "\n",
        "# @title `plot_tracks`\n",
        "\n",
        "def plot_tracks(tracks, interval, height=1.5):\n",
        "  fig, axes = plt.subplots(len(tracks), 1, figsize=(20, height * len(tracks)), sharex=True)\n",
        "  for ax, (title, y) in zip(axes, tracks.items()):\n",
        "    ax.fill_between(np.linspace(interval.start, interval.end, num=len(y)), y)\n",
        "    ax.set_title(title)\n",
        "    sns.despine(top=True, right=True, bottom=True)\n",
        "  ax.set_xlabel(str(interval))\n",
        "  plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import Bio\n",
        "\n",
        "from Bio.Seq import Seq\n",
        "def create_rev_complement(dna_string):\n",
        "    return(str(Seq(dna_string).reverse_complement()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def prepare_for_quantify_prediction_per_TSS(predictions, gene, tss_df):\n",
        "\n",
        "  '''\n",
        "\n",
        "  Parameters:\n",
        "          predicitions (A numpy array): All predictions from the track\n",
        "          gene (a gene name, character): a gene\n",
        "          tss_df: a list of dataframe of genes and their transcription start sites\n",
        "  Returns:\n",
        "          A dictionary of cage experiment predictions and a list of transcription start sites\n",
        "\n",
        "  '''\n",
        "\n",
        "  output = dict()\n",
        "  for tdf in tss_df:\n",
        "    if gene not in tdf.genes.values:\n",
        "      continue\n",
        "    gene_tss_list = tdf[tdf.genes == gene].txStart_Sites.apply(str).values\n",
        "    gene_tss_list = [t.split(', ') for t in gene_tss_list]\n",
        "    gene_tss_list = [int(item) for nestedlist in gene_tss_list for item in nestedlist]\n",
        "    gene_tss_list = list(set(gene_tss_list))\n",
        "  output['cage_predictions'] = predictions[:, 5110] # a numpy array\n",
        "  output['gene_TSS'] = gene_tss_list # a list\n",
        "\n",
        "\n",
        "  return(output) # a dictionary\n",
        "\n",
        "def quantify_prediction_per_TSS(low_range, TSS, cage_predictions):\n",
        "\n",
        "  '''\n",
        "  Parameters:\n",
        "          low_range (int): The lower interval\n",
        "          TSS (list of integers): A list of TSS for a gene\n",
        "          cage_predictions: A 1D numpy array or a vector of predictions from enformer corresponding to track 5110 or CAGE predictions\n",
        "  Returns:\n",
        "          A dictionary of gene expression predictions for each TSS for a gene\n",
        "    '''\n",
        "  tss_predictions = dict()\n",
        "  for tss in TSS:\n",
        "    bin_start = low_range + ((768 + 320) * 128)\n",
        "    count = -1\n",
        "    while bin_start < tss:\n",
        "      bin_start = bin_start + 128\n",
        "      count += 1\n",
        "    if count >= len(cage_predictions)-1:\n",
        "      continue\n",
        "    cage_preds = cage_predictions[count - 1] + cage_predictions[count] + cage_predictions[count + 1]\n",
        "    tss_predictions[tss] = cage_preds\n",
        "\n",
        "  return(tss_predictions)\n",
        "\n",
        "def collect_intervals(chromosomes = [\"22\"], gene_list=None):\n",
        "\n",
        "  '''\n",
        "    Parameters :\n",
        "      chromosomes : a list of chromosome numbers; each element should be a string format\n",
        "      gene_list : a list of genes; the genes should be located on those chromosomes\n",
        "\n",
        "    Returns :\n",
        "      A dictionary of genes (from gene_list) and their intervals within their respective chromosomes\n",
        "  '''\n",
        "\n",
        "  gene_intervals = {} # Collect intervals for our genes of interest\n",
        "\n",
        "  for chrom in chromosomes:\n",
        "    with open(\"/home/s1mi/enformer_tutorial/gene_chroms/gene_\"+ chrom + \".txt\", \"r\") as chrom_genes:\n",
        "      for line in chrom_genes:\n",
        "        split_line = line.strip().split(\"\\t\")\n",
        "        gene_intervals[split_line[2]] = [\n",
        "                                          split_line[0],\n",
        "                                          int(split_line[3]),\n",
        "                                          int(split_line[4])\n",
        "                                        ]\n",
        "\n",
        "  if isinstance(gene_list, list): # if the user has supplied a list of genes they are interested in\n",
        "    use_genes = dict((k, gene_intervals[k]) for k in gene_list if k in gene_intervals)\n",
        "    return(use_genes)\n",
        "  elif isinstance(gene_list, type(None)):\n",
        "    return(gene_intervals)\n",
        "\n",
        "\n",
        "def run_predictions(gene_intervals, tss_dataframe, individuals_list=None):\n",
        "  '''\n",
        "  Parameters :\n",
        "    gene_intervals : the results from calling `collect_intervals`\n",
        "    tss_dataframe : a list of the TSSs dataframes i.e. the TSS for the genes in the chromosomes\n",
        "    individuals_list : a list of individuals on which we want to make predictions; defaults to None\n",
        "\n",
        "  Returns :\n",
        "    A list of predictions; the first element is the predictions around the TSS for each gene. The second is the prediction across CAGE tracks\n",
        "  '''\n",
        "\n",
        "  gene_output = dict()\n",
        "  gene_predictions = dict()\n",
        "\n",
        "  for gene in gene_intervals.keys():\n",
        "    gene_interval = gene_intervals[gene]\n",
        "    target_interval = kipoiseq.Interval(\"chr\" + gene_interval[0],\n",
        "                                        gene_interval[1],\n",
        "                                        gene_interval[2]) # creates an interval to select the right sequences\n",
        "    target_fa = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))  # extracts the fasta sequences, and resizes such that it is compatible with the sequence_length\n",
        "    window_coords = target_interval.resize(SEQUENCE_LENGTH) # we also need information about the start and end locations after resizing\n",
        "    try:\n",
        "      cur_gene_vars = pd.read_csv(\"/home/s1mi/enformer_tutorial/individual_beds/chr\" + gene_interval[0] + \"/chr\" + gene_interval[0] + \"_\"+ gene + \".bed\", sep=\"\\t\", header=0) # read in the appropriate bed file for the gene\n",
        "    except:\n",
        "      continue\n",
        "    individual_results = dict()\n",
        "    individual_prediction = dict()\n",
        "\n",
        "    if isinstance(individuals_list, list) or isinstance(individuals_list, type(np.empty([1, 1]))):\n",
        "      use_individuals = individuals_list\n",
        "    elif isinstance(individuals_list, type(None)):\n",
        "      use_individuals = cur_gene_vars.columns[4:]\n",
        "\n",
        "    for individual in use_individuals:\n",
        "      print('Currently on gene {}, and predicting on individual {}...'.format(gene, individual))\n",
        "      # two haplotypes per individual\n",
        "      haplo_1 = list(target_fa[:])\n",
        "      haplo_2 = list(target_fa[:])\n",
        "\n",
        "      ref_mismatch_count = 0\n",
        "      for i,row in cur_gene_vars.iterrows():\n",
        "\n",
        "        geno = row[individual].split(\"|\")\n",
        "        if (row[\"POS\"]-window_coords.start-1) >= len(haplo_2):\n",
        "          continue\n",
        "        if (row[\"POS\"]-window_coords.start-1) < 0:\n",
        "          continue\n",
        "        if geno[0] == \"1\":\n",
        "          haplo_1[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n",
        "        if geno[1] == \"1\":\n",
        "          haplo_2[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n",
        "\n",
        "      # predict on the individual's two haplotypes\n",
        "      prediction_1 = model.predict_on_batch(one_hot_encode(\"\".join(haplo_1))[np.newaxis])['human'][0]\n",
        "      prediction_2 = model.predict_on_batch(one_hot_encode(\"\".join(haplo_2))[np.newaxis])['human'][0]\n",
        "\n",
        "      temp_predictions = [prediction_1[:, 5110], prediction_2[:, 5110]] # CAGE predictions we are interested in\n",
        "      individual_prediction[individual] = temp_predictions\n",
        "\n",
        "      # Calculate TSS CAGE expression which correspond to column 5110 of the predictions above\n",
        "      temp_list = list()\n",
        "\n",
        "      pred_prepared_1 = prepare_for_quantify_prediction_per_TSS(predictions=prediction_1, gene=gene, tss_df=tss_dataframe)\n",
        "      tss_predictions_1 = quantify_prediction_per_TSS(low_range = window_coords.start, TSS=pred_prepared_1['gene_TSS'], cage_predictions=pred_prepared_1['cage_predictions'])\n",
        "\n",
        "      pred_prepared_2 = prepare_for_quantify_prediction_per_TSS(predictions=prediction_2, gene=gene, tss_df=tss_dataframe)\n",
        "      tss_predictions_2 = quantify_prediction_per_TSS(low_range = window_coords.start, TSS=pred_prepared_2['gene_TSS'], cage_predictions=pred_prepared_2['cage_predictions'])\n",
        "\n",
        "      temp_list.append(tss_predictions_1)\n",
        "      temp_list.append(tss_predictions_2) # results here are a dictionary for each TSS for each haplotype\n",
        "\n",
        "      individual_results[individual] = temp_list # save for the individual\n",
        "\n",
        "    gene_output[gene] = individual_results\n",
        "    gene_predictions[gene] = individual_prediction\n",
        "\n",
        "  return([gene_output, gene_predictions])\n",
        "\n",
        "\n",
        "def collect_target_intervals(gene_intervals):\n",
        "\n",
        "  '''\n",
        "  Returns a dictionary of Interval objects (from kipoiseq) for each gene corresponding to the locations of the gene\n",
        "  '''\n",
        "\n",
        "  target_intervals_dict = dict()\n",
        "\n",
        "  for gene in gene_intervals.keys():\n",
        "    gene_interval = gene_intervals[gene]\n",
        "    target_interval = kipoiseq.Interval(\"chr\" + gene_interval[0],\n",
        "                                        gene_interval[1],\n",
        "                                        gene_interval[2])\n",
        "    target_intervals_dict[gene] = target_interval\n",
        "\n",
        "  return(target_intervals_dict)\n",
        "\n",
        "def prepare_for_plot_tracks(gene, individual, all_predictions, chromosome=['22']):\n",
        "\n",
        "  '''\n",
        "  This returns a dictionary of gene tracks and gene intervals, prepared for the function plot_tracks.\n",
        "\n",
        "  Parameters:\n",
        "    - gene\n",
        "    - individual\n",
        "    - all_predictions\n",
        "  '''\n",
        "\n",
        "  haplo_predictions = all_predictions[gene][individual]\n",
        "  gene_tracks = {gene + ' | ' + individual + ' | haplotype 1': np.log10(1 + haplo_predictions[0]),\n",
        "                gene + ' | ' + individual + ' | haplotype 2': np.log10(1 + haplo_predictions[1])}\n",
        "\n",
        "  gene_intervals = collect_intervals(chromosomes=chromosome, gene_list=[gene])\n",
        "  gene_intervals = collect_target_intervals(gene_intervals)\n",
        "\n",
        "  output = dict()\n",
        "  output['gene_tracks'] = gene_tracks\n",
        "  output['gene_intervals'] = gene_intervals[gene]\n",
        "\n",
        "  return(output)\n",
        "\n",
        "def check_individuals(path_to_bed_file, list_of_individuals):\n",
        "\n",
        "  '''\n",
        "  Checks if an individual is missing in bed variation files.\n",
        "  These individuals should be removed prior to training\n",
        "  '''\n",
        "\n",
        "  myfile = open(path_to_bed_file, 'r')\n",
        "  myline = myfile.readline()\n",
        "  bed_names = myline.split('\\t')[4:]\n",
        "  myfile.close()\n",
        "\n",
        "  if set(list_of_individuals).issubset(set(bed_names)) == False:\n",
        "    missing = list(set(list_of_individuals).difference(bed_names))\n",
        "    print('This (or these) individual(s) is/are not present: {}'.format(missing))\n",
        "  else:\n",
        "    missing = []\n",
        "    print('All individuals are present in the bed file.')\n",
        "\n",
        "  return(missing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def geno_to_seq(gene, individual):\n",
        "      # two haplotypes per individual\n",
        "  haplo_1 = list(target_fa[:])\n",
        "  haplo_2 = list(target_fa[:])\n",
        "\n",
        "  ref_mismatch_count = 0\n",
        "  for i,row in cur_gene_vars.iterrows():\n",
        "\n",
        "    geno = row[individual].split(\"|\")\n",
        "    if (row[\"POS\"]-window_coords.start-1) >= len(haplo_2):\n",
        "      continue\n",
        "    if (row[\"POS\"]-window_coords.start-1) < 0:\n",
        "      continue\n",
        "    if geno[0] == \"1\":\n",
        "      haplo_1[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n",
        "    if geno[1] == \"1\":\n",
        "      haplo_2[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n",
        "  return haplo_1, haplo_2\n",
        "\n",
        "      # predict on the individual's two haplotypes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare input data\n",
        "\n",
        "We want to predict epigenome around ERAP2 TSS on chromosome 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "chr5_tss = pd.read_table('/home/s1mi/enformer_tutorial/tss_by_chr/chr5_tss_by_gene.txt', sep='\\t')\n",
        "erap2_variations = pd.read_table('/home/s1mi/enformer_tutorial/individual_beds/chr5/chr5_ERAP2.bed', sep='\\t')\n",
        "geuvadis_gene_expression = pd.read_table('https://uchicago.box.com/shared/static/5vwc7pjw9qmtv7298c4rc7bcuicoyemt.gz', sep='\\t',\n",
        "                                         dtype={'gene_id': str, 'gene_name':str, 'TargetID':str, 'Chr':str})\n",
        "geuvadis_gene_expression.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gene_intervals = collect_intervals(chromosomes=['5'], gene_list=['ERAP2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = Enformer(model_path) # here we load the model architecture.\n",
        "\n",
        "fasta_extractor = FastaStringExtractor(fasta_file) # we define a class called fasta_extractor to help us extra raw sequence data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Predictions\n",
        "\n",
        "We'll pick one individual at random."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rand_individual = np.random.choice(a=geuvadis_gene_expression.columns[6:-1], replace=False) # individuals we are interested in\n",
        "rand_individual"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gene = 'ERAP2'\n",
        "gene_interval = gene_intervals[gene]\n",
        "target_interval = kipoiseq.Interval(\"chr\" + gene_interval[0],\n",
        "                                        gene_interval[1],\n",
        "                                        gene_interval[2])\n",
        "target_fa = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))\n",
        "window_coords = target_interval.resize(SEQUENCE_LENGTH)\n",
        "cur_gene_vars = pd.read_csv(\"/home/s1mi/enformer_tutorial/individual_beds/chr\" + gene_interval[0] + \"/chr\" + gene_interval[0] + \"_\"+ gene + \".bed\", sep=\"\\t\", header=0) # read in the appropriate bed file for the gene"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "haplo_1, haplo_2 = geno_to_seq('ERAP2', rand_individual)\n",
        "\n",
        "haplo_1_enc = one_hot_encode(\"\".join(haplo_1))[np.newaxis]\n",
        "haplo_2_enc = one_hot_encode(\"\".join(haplo_2))[np.newaxis]\n",
        "average_enc = np.add(haplo_1_enc, haplo_2_enc) / 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prediction_1 = model.predict_on_batch(haplo_1_enc)['human'][0]\n",
        "prediction_2 = model.predict_on_batch(haplo_2_enc)['human'][0]\n",
        "\n",
        "pre_average = model.predict_on_batch(average_enc)['human'][0]\n",
        "post_average = (prediction_1 + prediction_2) / 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(pre_average)\n",
        "print(post_average)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing across tracks\n",
        "\n",
        "We compute correlation for each track."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cors = []\n",
        "for i in range(5313):\n",
        "    pre_track = pre_average[:, i]\n",
        "    post_track = post_average[:, i]\n",
        "    cor = np.corrcoef(pre_track, post_track)[0][1]\n",
        "    cors.append(cor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results from both methods are nearly identical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(min(cors), max(cors))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we examine the outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "diff = pre_average - post_average\n",
        "print(diff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mses = []\n",
        "for i in range(5313):\n",
        "    mse = np.sum(np.square(diff[:, i]))\n",
        "    mses.append(mse)\n",
        "\n",
        "quartiles = np.percentile(mses, [25, 50, 75])\n",
        "print(\"Min: \", min(mses))\n",
        "print(\"Q1: \", quartiles[0]) \n",
        "print(\"Median: \", quartiles[1])\n",
        "print(\"Q3: \", quartiles[2])\n",
        "print(\"Max: \", max(mses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.boxplot(mses)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "desc_index = [i for i, x in sorted(enumerate(mses), key=lambda x: x[1], reverse=True)]\n",
        "print(\"Tracks from highest to lowest MSE:\", desc_index)\n",
        "\n",
        "mse_outs = [x for x in mses if x > 1]\n",
        "print(\"Number of tracks with MSE > 1:\", len(mse_outs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a closer look at the 10 tracks with highest MSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Lowest MSEs\")\n",
        "for i in range(10):\n",
        "    track = desc_index[i]\n",
        "    print(f\"Track: {track}, MSE: {mses[track]}, Description: {df_targets.iloc[track, -1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "They are all ChIP-Seq tracks. \n",
        "\n",
        "Next, we look at the distribution of differences for each track."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_diff = pd.DataFrame(abs(diff))\n",
        "sums = df_diff.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first sort the tracks by biggest difference (absolute value) across the bins."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "desc_col = sorted(range(5313), reverse = True, key = lambda x: sums.loc['max', x]) # diff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we print summary statistics for tracks with the largest differences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sums.iloc[:, [i for i in desc_col[:10]]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we plot distributions for the tracks with largest (absolute) differences. Take note that this plot shows distributions of differences in both directions. This plot confirms the table above, that despite having outliers their mean difference is still very close to 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = [diff[:,i] for i in desc_col[:10]]\n",
        "plt.boxplot(data)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "torch",
      "language": "python",
      "display_name": "torch"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
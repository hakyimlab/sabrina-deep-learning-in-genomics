#!/bin/bash -l
#PBS -l select=3:ncpus=32
#PBS -l place=scatter
#PBS -l walltime=01:00:00
#PBS -l filesystems=home:eagle
#PBS -q debug-scaling
#PBS -A AIHPC4EDU
#PBS -N prepare_qtl2_inputs
#PBS -j oe

# Load modules
module use /soft/modulefiles
module load conda
conda activate ml-python
module load cray-mpich


# Change to working directory (where you submit)
cd ${PBS_O_WORKDIR}  

# Get number of allocated nodes
NNODES=$(wc -l < $PBS_NODEFILE)

# MPI ranks per node
NRANKS_PER_NODE=32

# Total MPI ranks = nodes * ranks/node
NTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))

echo "NUM_OF_NODES=${NNODES} TOTAL_NUM_RANKS=${NTOTRANKS} RANKS_PER_NODE=${NRANKS_PER_NODE}"

# Setup environment variables if using OpenMP (1 core per MPI rank here)
export OMP_NUM_THREADS=1



# Paths and variables
export data_dir=/home/s1mi/qtl2_data
export scratch=/eagle/AIHPC4Edu/sabrina/scratch
export samples_dir=$scratch/bodylen_bmi_VCFs_for_qtl2
export founders_dir=/home/s1mi/enformer_rat_data/genotypes/FounderVCFs
export output_dir=$scratch/qtl2_outputs
export code_dir=/home/s1mi/Github/deep-learning-in-genomics/posts/2025-07-22-bmi-bodylen-rats-haplotype-probabilities
export job_list=${PBS_O_WORKDIR}/job_list.txt

mpiexec -n $NTOTRANKS -ppn $NRANKS_PER_NODE bash -c '
  n_tasks=$(wc -l < '"$job_list"')
  for task_id in $(seq $((PMI_RANK+1)) $NTOTRANKS $n_tasks); do
    read i j < <(sed -n "${task_id}p" '"$job_list"')
    # echo "Rank $PMI_RANK running task for Chr${i}, Batch${j}"

    python $code_dir/make_qtl2_inputs.py \
        $samples_dir/chr${i}_by_batch/batch${j}.vcf.gz \
        $founders_dir/chr${i}.vcf.gz \
        $output_dir/chr${i}_by_batch/batch${j}_probs.rds \
        --working-dir $scratch/chr${i}_qtl2_founder_haps/batch${j} \
        --gmap-dir $data_dir/genetic_map \
        --cores 32
    echo "Rank $PMI_RANK finished writing qtl2 inputs for Chr${i}, Batch${j}"
  done
'
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Comparing the predicted reference epigenome to Enformer runs centered at the TSS and ground truth\n",
    "author: Sabrina Mi\n",
    "date: 8/28/2023\n",
    "---\n",
    "\n",
    "We are trying to debug low correlation values between predicted reference CAGE values and reference observed gene expression by inspecting a few genes.\n",
    "\n",
    "## Select Genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 23:16:08.973066: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-29 23:16:10.470647: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/soft/libraries/trt/TensorRT-8.5.2.2.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.16.2-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/pe/gcc/11.2.0/snos/lib64:/opt/cray/pe/papi/6.0.0.14/lib64:/opt/cray/libfabric/1.11.0.4.125/lib64:/dbhome/db2cat/sqllib/lib64:/dbhome/db2cat/sqllib/lib64/gskit:/dbhome/db2cat/sqllib/lib32:${LD_LIBRARY_PATH}\n",
      "2023-08-29 23:16:10.470962: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/soft/libraries/trt/TensorRT-8.5.2.2.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.16.2-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/pe/gcc/11.2.0/snos/lib64:/opt/cray/pe/papi/6.0.0.14/lib64:/opt/cray/libfabric/1.11.0.4.125/lib64:/dbhome/db2cat/sqllib/lib64:/dbhome/db2cat/sqllib/lib64/gskit:/dbhome/db2cat/sqllib/lib32:${LD_LIBRARY_PATH}\n",
      "2023-08-29 23:16:10.470975: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "import tensorflow as tf\n",
    "# Make sure the GPU is enabled \n",
    "assert tf.config.list_physical_devices('GPU'), 'Start the colab kernel with GPU: Runtime -> Change runtime type -> GPU'\n",
    "import tensorflow_hub as hub\n",
    "import joblib\n",
    "import gzip\n",
    "import kipoiseq\n",
    "from kipoiseq import Interval\n",
    "import pyfaidx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "transform_path = 'gs://dm-enformer/models/enformer.finetuned.SAD.robustscaler-PCA500-robustscaler.transform.pkl'\n",
    "model_path = 'https://tfhub.dev/deepmind/enformer/1'\n",
    "fasta_file = '/home/s1mi/enformer_rat_data/reference_genome/hg38_genome.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_df = pd.read_csv(\"/home/s1mi/enformer_rat_data/annotation/hg38.protein_coding_TSS.txt\", sep=\"\\t\")\n",
    "genes = annot_df.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Enformer at the TSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "SEQUENCE_LENGTH = 393216\n",
    "\n",
    "class Enformer:\n",
    "\n",
    "  def __init__(self, tfhub_url):\n",
    "    self._model = hub.load(tfhub_url).model\n",
    "\n",
    "  def predict_on_batch(self, inputs):\n",
    "    predictions = self._model.predict_on_batch(inputs)\n",
    "    return {k: v.numpy() for k, v in predictions.items()}\n",
    "\n",
    "  @tf.function\n",
    "  def contribution_input_grad(self, input_sequence,\n",
    "                              target_mask, output_head='human'):\n",
    "    input_sequence = input_sequence[tf.newaxis]\n",
    "\n",
    "    target_mask_mass = tf.reduce_sum(target_mask)\n",
    "    with tf.GradientTape() as tape:\n",
    "      tape.watch(input_sequence)\n",
    "      prediction = tf.reduce_sum(\n",
    "          target_mask[tf.newaxis] *\n",
    "          self._model.predict_on_batch(input_sequence)[output_head]) / target_mask_mass\n",
    "\n",
    "    input_grad = tape.gradient(prediction, input_sequence) * input_sequence\n",
    "    input_grad = tf.squeeze(input_grad, axis=0)\n",
    "    return tf.reduce_sum(input_grad, axis=-1)\n",
    "  \n",
    "def plot_tracks(tracks, interval, height=1.5):\n",
    "  fig, axes = plt.subplots(len(tracks), 1, figsize=(20, height * len(tracks)), sharex=True)\n",
    "  for ax, (title, y) in zip(axes, tracks.items()):\n",
    "    ax.fill_between(np.linspace(interval.start, interval.end, num=len(y)), y)\n",
    "    ax.set_title(title)\n",
    "    sns.despine(top=True, right=True, bottom=True)\n",
    "  ax.set_xlabel(str(interval))\n",
    "  plt.tight_layout()\n",
    "  \n",
    "class FastaStringExtractor:\n",
    "    \n",
    "    def __init__(self, fasta_file):\n",
    "        self.fasta = pyfaidx.Fasta(fasta_file)\n",
    "        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n",
    "\n",
    "    def extract(self, interval: Interval, **kwargs) -> str:\n",
    "        # Truncate interval if it extends beyond the chromosome lengths.\n",
    "        chromosome_length = self._chromosome_sizes[interval.chrom]\n",
    "        trimmed_interval = Interval(interval.chrom,\n",
    "                                    max(interval.start, 0),\n",
    "                                    min(interval.end, chromosome_length),\n",
    "                                    )\n",
    "        # pyfaidx wants a 1-based interval\n",
    "        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n",
    "                                          trimmed_interval.start + 1,\n",
    "                                          trimmed_interval.stop).seq).upper()\n",
    "        # Fill truncated values with N's.\n",
    "        pad_upstream = 'N' * max(-interval.start, 0)\n",
    "        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n",
    "        return pad_upstream + sequence + pad_downstream\n",
    "\n",
    "    def close(self):\n",
    "        return self.fasta.close()\n",
    "    \n",
    "def one_hot_encode(sequence):\n",
    "  return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Enformer(model_path)\n",
    "fasta_extractor = FastaStringExtractor(fasta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Interval object for each TSS\n",
    "intervals = [kipoiseq.Interval('chr'+ row['chromosome_name'], row['transcription_start_site'], row['transcription_start_site'])\n",
    "            for _, row in genes.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss_predictions = {}\n",
    "for index, gene in enumerate(genes['ensembl_gene_id']):\n",
    "    tss = intervals[index]\n",
    "    sequence_one_hot = one_hot_encode(fasta_extractor.extract(tss.resize(SEQUENCE_LENGTH)))\n",
    "    tss_predictions[gene] = model.predict_on_batch(sequence_one_hot[np.newaxis])['human'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Predicted Reference Epigenome at the TSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "import h5py\n",
    "\n",
    "enfref_dir = \"/grand/TFXcan/imlab/users/lvairus/reftile_project/enformer-reference-epigenome\"\n",
    "\n",
    "def query_epigenome(chr_num, center_bp, num_bins=3, tracks=-1):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        path_to_enfref (str): path to the directory containing the concatenated reference enformer files\n",
    "        chr_num (int/string): chromosome number\n",
    "        center_bp (int): center base pair position (1-indexed)\n",
    "        num_bins (int): number of bins to extract centered around center_bp (default: 896) \n",
    "            note: if the number of bins is even, the center bin will be in the second half of the array\n",
    "        tracks (int list): list of tracks to extract (default: all 5313 tracks)\n",
    "\n",
    "    Returns:\n",
    "        epigen (np.array): enformer predictions centered at center_bp of shape (num_bins, len(tracks))\n",
    "    \"\"\"\n",
    "\n",
    "    # from position choose center bin\n",
    "    center_ind = center_bp - 1\n",
    "    center_bin = center_ind // 128\n",
    "    \n",
    "    half_bins = num_bins // 2\n",
    "    start_bin = center_bin - half_bins\n",
    "    end_bin = center_bin + half_bins\n",
    "    if num_bins % 2 != 0: # if num_bins is odd\n",
    "        end_bin += 1\n",
    "\n",
    "    with h5py.File(f\"{enfref_dir}/chr{chr_num}_cat.h5\", \"r\") as f:\n",
    "        # get tracks if list provided\n",
    "        if tracks == -1:\n",
    "            epigen = f[f'chr{chr_num}'][start_bin:end_bin, :] \n",
    "        else:\n",
    "            epigen = f[f'chr{chr_num}'][start_bin:end_bin, tracks] \n",
    "\n",
    "    return epigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_reference_epigenome = {}\n",
    "for index, gene in enumerate(genes['ensembl_gene_id']):\n",
    "    chr = genes.iloc[index]['chromosome_name']\n",
    "    pos = genes.iloc[index]['transcription_start_site']\n",
    "    predicted_reference_epigenome[gene] = query_epigenome(chr, pos, num_bins=896)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('/home/s1mi/enformer_rat_data/output/hg38_tss_predictions.h5', 'w') as hf:\n",
    "    for key, value in tss_predictions.items():\n",
    "        hf[key] = value\n",
    "with h5py.File('/home/s1mi/enformer_rat_data/output/hg38_reference_epigenome_tss.h5', 'w') as hf:\n",
    "    for key, value in predicted_reference_epigenome.items():\n",
    "        hf[key] = value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-python",
   "language": "python",
   "name": "ml-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

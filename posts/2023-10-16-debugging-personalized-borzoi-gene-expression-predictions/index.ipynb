{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Debugging Borzoi Personalized Predictions Test\n",
    "author: Sabrina Mi\n",
    "date: 10/16/2023\n",
    "execute:\n",
    "  code-fold: true\n",
    "---\n",
    "\n",
    "Use human gene ENSG00000161011 to compute CAGE tracks for personalized genomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 03:13:50.091055: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-17 03:13:52.900195: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-10-17 03:13:57.864020: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NOT_INITIALIZED: initialization error\n",
      "2023-10-17 03:13:57.864167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: polaris-login-01\n",
      "2023-10-17 03:13:57.864189: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: polaris-login-01\n",
      "2023-10-17 03:13:57.864601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 470.103.4\n",
      "2023-10-17 03:13:57.864664: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 470.103.4\n",
      "2023-10-17 03:13:57.864677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 470.103.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "# Borzoi Setup\n",
    "prefix = '/home/s1mi/borzoi_tutorial/'\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pysam\n",
    "import pyfaidx\n",
    "import tensorflow as tf\n",
    "\n",
    "from baskerville import seqnn\n",
    "from baskerville import gene as bgene\n",
    "from baskerville import dna\n",
    "\n",
    "from borzoi_helpers import *\n",
    "\n",
    "params_file = prefix + 'params_pred.json'\n",
    "targets_file = prefix + 'targets_human.txt' #Subset of targets_human.txt\n",
    "\n",
    "seq_len = 524288\n",
    "n_folds = 1       #To use only one model fold, set to 'n_folds = 1'. To use all four folds, set 'n_folds = 4'.\n",
    "rc = True         #Average across reverse-complement prediction\n",
    "\n",
    "#Read model parameters\n",
    "\n",
    "with open(params_file) as params_open :\n",
    "    \n",
    "    params = json.load(params_open)\n",
    "    \n",
    "    params_model = params['model']\n",
    "    params_train = params['train']\n",
    "\n",
    "#Read targets\n",
    "\n",
    "targets_df = pd.read_csv(targets_file, index_col=0, sep='\\t')\n",
    "target_index = targets_df.index\n",
    "\n",
    "#Create local index of strand_pair (relative to sliced targets)\n",
    "if rc :\n",
    "    strand_pair = targets_df.strand_pair\n",
    "    \n",
    "    target_slice_dict = {ix : i for i, ix in enumerate(target_index.values.tolist())}\n",
    "    slice_pair = np.array([\n",
    "        target_slice_dict[ix] if ix in target_slice_dict else ix for ix in strand_pair.values.tolist()\n",
    "    ], dtype='int32')\n",
    "\n",
    "#Initialize model ensemble\n",
    "\n",
    "models = []\n",
    "for fold_ix in range(n_folds) :\n",
    "    \n",
    "    model_file = prefix + \"saved_models/f\" + str(fold_ix) + \"/model0_best.h5\"\n",
    "\n",
    "    seqnn_model = seqnn.SeqNN(params_model)\n",
    "    seqnn_model.restore(model_file, 0)\n",
    "    seqnn_model.build_slice(target_index)\n",
    "    if rc :\n",
    "        seqnn_model.strand_pair.append(slice_pair)\n",
    "    seqnn_model.build_ensemble(rc, '0')\n",
    "    \n",
    "    models.append(seqnn_model)\n",
    "\n",
    "fasta_open = pysam.Fastafile(prefix + 'hg38.fa')\n",
    "transcriptome = bgene.Transcriptome(prefix + 'gencode41_basic_nort.gtf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_gene = 'ENSG00000161011'\n",
    "chrom = 'chr5'\n",
    "gene_start = 179820905\n",
    "gene_end = 179838078\n",
    "tss = gene_start\n",
    "center = (gene_start + gene_end) // 2\n",
    "start = center - seq_len // 2\n",
    "end = center + seq_len // 2\n",
    "individuals = ['NA20521', 'NA18934']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_keys = [gene_key for gene_key in transcriptome.genes.keys() if search_gene in gene_key]\n",
    "\n",
    "gene = transcriptome.genes[gene_keys[0]]\n",
    "\n",
    "#Determine output sequence start\n",
    "seq_out_start = start + seqnn_model.model_strides[0]*seqnn_model.target_crops[0]\n",
    "seq_out_len = seqnn_model.model_strides[0]*seqnn_model.target_lengths[0]\n",
    "\n",
    "#Determine output positions of gene exons\n",
    "gene_slice = gene.output_slice(seq_out_start, seq_out_len, seqnn_model.model_strides[0], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_variants_in_vcf_file(cyvcf2_object, interval_object, samples, mode=\"phased\"):\n",
    "    start = max(interval_object['start'], 0)\n",
    "    query = f\"{interval_object['chr']}:{start}-{interval_object['end']}\"\n",
    "    variants_dictionary = {}\n",
    "    variants_dictionary['chr'] = interval_object['chr']\n",
    "    variants_dictionary['positions'] = tuple(variant.POS for variant in cyvcf2_object(query))\n",
    "    if mode == 'phased':\n",
    "        delim = '|'\n",
    "    elif mode == 'unphased':\n",
    "        delim = '/'\n",
    "    for i, sample in enumerate(samples):\n",
    "        if sample in cyvcf2_object.samples:\n",
    "            variants_dictionary[sample] = tuple([variant.genotypes[i][0:2], variant.gt_bases[i].split(delim)] for variant in cyvcf2_object(query))\n",
    "    return variants_dictionary\n",
    "\n",
    "def predict_on_sequence(models, sample_input):\n",
    "    prediction_output = {}\n",
    "    for haplotype, sequence_encoding in sample_input.items():\n",
    "        prediction = predict_tracks(models, sequence_encoding)\n",
    "        prediction_output[haplotype] = prediction\n",
    "    return prediction_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cyvcf2\n",
    "target_interval = {'chr': chrom, 'start': start, 'end': end}\n",
    "vcf_dir = \"/grand/TFXcan/imlab/data/1000G/vcf_snps_only\"\n",
    "path_to_vcf = os.path.join(vcf_dir, f\"ALL.{chrom}.shapeit2_integrated_SNPs_v2a_27022019.GRCh38.phased.vcf.gz\")\n",
    "vcf_chr = cyvcf2.cyvcf2.VCF(path_to_vcf, samples=individuals)\n",
    "variants_array = find_variants_in_vcf_file(vcf_chr, target_interval, individuals, mode=\"phased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate_sequence(sequence_one_hot, start, poses, alts):\n",
    "    \n",
    "    #Induce mutation(s)\n",
    "    sequence_one_hot_mut = np.copy(sequence_one_hot)\n",
    "\n",
    "    for pos, alt in zip(poses, alts) :\n",
    "        alt_ix = -1\n",
    "        if alt == 'A' :\n",
    "            alt_ix = 0\n",
    "        elif alt == 'C' :\n",
    "            alt_ix = 1\n",
    "        elif alt == 'G' :\n",
    "            alt_ix = 2\n",
    "        elif alt == 'T' :\n",
    "            alt_ix = 3\n",
    "\n",
    "        sequence_one_hot_mut[pos-start-1] = 0.\n",
    "        sequence_one_hot_mut[pos-start-1, alt_ix] = 1.\n",
    "    return sequence_one_hot_mut\n",
    "\n",
    "def replace_variants_in_reference_sequence(variants_array, individuals):\n",
    "    poses = variants_array['positions']\n",
    "    variant_encoded = {}\n",
    "    for individual in individuals:\n",
    "        alts_1 = [variants_array[individual][i][1][0] for i in range(len(poses))]\n",
    "        alts_2 = [variants_array[individual][i][1][1] for i in range(len(poses))]\n",
    "        sequence_one_hot = process_sequence(fasta_open, chrom, gene_start, gene_end)\n",
    "        haplotype1_encoded = mutate_sequence(sequence_one_hot, gene_start, poses, alts_1)\n",
    "        haplotype2_encoded = mutate_sequence(sequence_one_hot, gene_start, poses, alts_2)\n",
    "        variant_encoded[individual] = {'haplotype1': haplotype1_encoded, 'haplotype2': haplotype2_encoded}\n",
    "    return variant_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_variants_encoded = replace_variants_in_reference_sequence(variants_array, individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for individual in individuals:\n",
    "    sample_input = samples_variants_encoded[individual]\n",
    "    sample_predictions = predict_on_sequence(models, sample_input)\n",
    "    with h5py.File(f'{individual}/test.h5', \"w\") as hf:\n",
    "        for hap in sample_predictions.keys():\n",
    "            hf[hap]= np.squeeze(sample_predictions[hap], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16352, 7611)\n",
      "(1, 16352, 7611)\n",
      "(4, 16352, 7611)\n",
      "(1, 16352, 7611)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "old_predictions = {}\n",
    "new_predictions = {}\n",
    "for individual in individuals:\n",
    "    predictions_file = os.path.join(\"/grand/TFXcan/imlab/users/sabrina/borzoi-personalized-test\", individual, f'chr5_179567347_180091635_predictions.h5')\n",
    "    with h5py.File(predictions_file, \"r\") as hf:\n",
    "        haplo1 = hf['haplotype1'][:]\n",
    "        haplo2 = hf['haplotype2'][:]\n",
    "        print(haplo1.shape)\n",
    "    old_predictions[individual] = {'haplotype1': haplo1, 'haplotype2': haplo2}\n",
    "    with h5py.File(os.path.join(individual, 'test.h5'), 'r') as hf:\n",
    "        haplo1 = hf['haplotype1'][:]\n",
    "        haplo2 = hf['haplotype2'][:]\n",
    "        print(haplo1.shape)\n",
    "    new_predictions[individual] = {'haplotype1': haplo1, 'haplotype2': haplo2}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_CAGE_predictions = []\n",
    "new_CAGE_predictions = []\n",
    "tss_bin = (tss - seq_out_start) // 32\n",
    "for individual in individuals:\n",
    "    old_CAGE_haplo1 = np.mean(old_predictions[individual]['haplotype1'][:, tss_bin-1:tss_bin + 2, [870,871]])\n",
    "    old_CAGE_haplo2 = np.mean(old_predictions[individual]['haplotype2'][:, tss_bin-1:tss_bin + 2, [870,871]])\n",
    "    old_CAGE = (old_CAGE_haplo1 + old_CAGE_haplo2) / 2\n",
    "    new_CAGE_haplo1 = np.mean(new_predictions[individual]['haplotype1'][:, tss_bin-1:tss_bin + 2, [870,871]])\n",
    "    new_CAGE_haplo2 = np.mean(new_predictions[individual]['haplotype2'][:, tss_bin-1:tss_bin + 2, [870,871]])\n",
    "    new_CAGE = (new_CAGE_haplo1 + new_CAGE_haplo2) / 2    \n",
    "    old_CAGE_predictions.append(old_CAGE)\n",
    "    new_CAGE_predictions.append(new_CAGE)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41.0625, 41.0]\n",
      "[42.78125, 42.78125]\n"
     ]
    }
   ],
   "source": [
    "print(old_CAGE_predictions)\n",
    "print(new_CAGE_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_one_hot = process_sequence(fasta_open, chrom, gene_start, gene_end)\n",
    "reference_prediction = predict_tracks(models, sequence_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.8\n"
     ]
    }
   ],
   "source": [
    "reference_CAGE = np.mean(reference_prediction[..., tss_bin-1:tss_bin + 2, [870,871]])\n",
    "print(reference_CAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it looks like the new predictions are closer to reference, it could mean our bug was how we indexed variants into the reference sequence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "borzoi",
   "language": "python",
   "name": "borzoi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

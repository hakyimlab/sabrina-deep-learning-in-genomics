{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: EpigenomeXcan test on Br rats\n",
    "author: Sabrina Mi\n",
    "date: 7/10/2024\n",
    "description: Calculate associations between predicted gene expression in adipose tissue and BMI to identify significant gene, while I figure out high to scale up predicting epigenome step.\n",
    "---\n",
    "\n",
    "CPU times: \n",
    "\n",
    "1. Compute founders matrix (n_genes, 8)\n",
    "    * 3190 genes: ~110s\n",
    "2. Compute samples matrix (n_samples, n_genes, 8):\n",
    "    * 3190 genes, 1 sample: ~4s\n",
    "    * 3190 genes, 10 samples: ~40s\n",
    "\n",
    "GPU times, combined steps 1 & 2:\n",
    "1. Old code\n",
    "    * 3190 genes, 340 samples: 32 minutes\n",
    "2. New code removing inner foor loop\n",
    "    * 1592 genes, 340 samples: 2 min 30 sec\n",
    "    * 1575 genes, 340 samples: 2 min 5 sec\n",
    "    * 19696 genes, 340 samples: 22 min 30 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-09 00:40:22.787012: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-09 00:40:30.021105: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/soft/libraries/trt/TensorRT-8.5.2.2.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.16.2-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/pe/gcc/11.2.0/snos/lib64:/opt/cray/pe/papi/6.0.0.14/lib64:/opt/cray/libfabric/1.11.0.4.125/lib64:/dbhome/db2cat/sqllib/lib64:/dbhome/db2cat/sqllib/lib64/gskit:/dbhome/db2cat/sqllib/lib32:/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/soft/libraries/trt/TensorRT-8.5.2.2.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.16.2-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/soft/perftools/darshan/darshan-3.4.4/lib:/opt/cray/pe/papi/7.0.1.2/lib64:/opt/cray/libfabric/1.15.2.0/lib64\n",
      "2024-08-09 00:40:30.023520: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/soft/libraries/trt/TensorRT-8.5.2.2.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.16.2-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/pe/gcc/11.2.0/snos/lib64:/opt/cray/pe/papi/6.0.0.14/lib64:/opt/cray/libfabric/1.11.0.4.125/lib64:/dbhome/db2cat/sqllib/lib64:/dbhome/db2cat/sqllib/lib64/gskit:/dbhome/db2cat/sqllib/lib32:/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/soft/libraries/trt/TensorRT-8.5.2.2.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.16.2-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/soft/perftools/darshan/darshan-3.4.4/lib:/opt/cray/pe/papi/7.0.1.2/lib64:/opt/cray/libfabric/1.15.2.0/lib64\n",
      "2024-08-09 00:40:30.023528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "import bisect\n",
    "columns = ['ACI', 'BN', 'BUF', 'F344', 'M520', 'MR', 'WKY', 'WN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_annot = pd.read_csv(\"/eagle/AIHPC4Edu/sabrina/Br_predictions/HS_founder_epigenomes/gene_mapping.txt\")\n",
    "genes_by_chrom = gene_annot.groupby('chromosome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_dir = \"/eagle/AIHPC4Edu/sabrina/Br_genotype_probabilities\"\n",
    "reference_dir = \"/eagle/AIHPC4Edu/sabrina/Br_predictions/HS_founder_epigenomes/human\"\n",
    "output_dir = \"/eagle/AIHPC4Edu/sabrina/Br_prediction_from_founders\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizing my haplotype probabilities storage is redundant, taking up extra space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "probabilities_file = f'{probabilities_dir}/chr20_probabilities.h5'\n",
    "with h5py.File(probabilities_file, 'a') as hf:\n",
    "    for dataset_name in list(hf.keys()):\n",
    "        if hf[dataset_name].shape[1] == 8:\n",
    "            continue\n",
    "        elif hf[dataset_name].shape[1] == 9:\n",
    "            # Read the dataset\n",
    "            data = hf[dataset_name][:]\n",
    "            positions = data[:, 0]  # Assuming the positions column is the first column\n",
    "            new_data = data[:, 1:]  # All columns except the first one\n",
    "            \n",
    "            # Create a temporary dataset without the positions column\n",
    "            temp_dataset_name = f\"temp_{dataset_name}\"\n",
    "            hf.create_dataset(temp_dataset_name, data=new_data)\n",
    "            \n",
    "            # Delete the original dataset\n",
    "            del hf[dataset_name]\n",
    "        \n",
    "            # Rename the temporary dataset to the original dataset name\n",
    "            hf.move(temp_dataset_name, dataset_name)\n",
    "        \n",
    "    hf.create_dataset('positions', data=positions)\n",
    "        # Store the positions vector as metadata (attribute) for the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split genes by chromosome number\n",
    "for individual, query haplotype probabilities at each gene tss, so we return an 8 x n_gene matrix. by batching by genes n_genes should be in the 1-2K range. stack to return a 3d array with dimensions n_samples x 8 x n_gene\n",
    "next query reference epigenome at each gene [446:450, CAGE_index], return 8 x n_gene matrix\n",
    "matrix multiply with something like tf.transpose(tf.tensordot(_W, _X, axes=[[1],[1]]),[1,0,2])\n",
    "https://stackoverflow.com/questions/41870228/understanding-tensordot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reference_epigenome_matrix(chr, genes_df, track = 5278):\n",
    "    reference_file = f'{reference_dir}/{chr}_genes.h5'\n",
    "    with h5py.File(reference_file, 'r') as ref:\n",
    "        rows = []\n",
    "        for gene in genes_df['gene']:\n",
    "            founder_predictions = ref[gene][:, 446:450, track]\n",
    "            rows.append(founder_predictions)\n",
    "        ref_matrix = np.stack(rows, axis=0)\n",
    "        ref_tensor = tf.reduce_mean(tf.convert_to_tensor(ref_matrix, dtype=tf.float32), axis=2)\n",
    "        return ref_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilities_matrix(chr, genes_df, individuals):\n",
    "    probabilities_file = f'{probabilities_dir}/{chr}_probabilities.h5'\n",
    "    with h5py.File(probabilities_file, 'r') as prob:\n",
    "        positions = prob['positions'][:]\n",
    "        population_prob = []\n",
    "        indices = []\n",
    "        for tss in genes_df['tss']:\n",
    "            index = bisect.bisect_left(positions, tss)\n",
    "            if index == 0:\n",
    "                indices += [0,0]\n",
    "            elif index == len(positions):\n",
    "                indices += [index-1,index-1]\n",
    "            else: # 0 < index < len(positions)\n",
    "                indices += [index-1, index]\n",
    "        for sample in individuals:\n",
    "            dataset = prob[sample][:]\n",
    "            sample_prob = tf.convert_to_tensor(dataset[indices], dtype = tf.float32)\n",
    "            sample_prob = tf.reshape(sample_prob, (-1, 2, sample_prob.shape[1]))\n",
    "            sample_prob = tf.reduce_mean(sample_prob, axis=1)\n",
    "            population_prob.append(sample_prob)\n",
    "        prob_tensor = tf.stack(population_prob, axis=0)\n",
    "        return prob_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_epigenome(chr, genes_df, individuals, output_file, track):\n",
    "    ref_tensor = reference_epigenome_matrix(chr, genes_df, track)\n",
    "    prob_tensor = probabilities_matrix(chr, genes_df, individuals)\n",
    "    epigenome_tensor = tf.einsum('ijk,jk->ij', prob_tensor, ref_tensor)\n",
    "    epigenome_df = pd.DataFrame(epigenome_tensor.numpy(), columns=genes_df['gene'], index = individuals)\n",
    "    epigenome_df.to_csv(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/eagle/AIHPC4Edu/sabrina/Br_genotype_probabilities/individuals.txt', 'r') as f:\n",
    "    individuals = f.read().splitlines()\n",
    "#pheno = pd.read_csv('/home/s1mi/enformer_rat_data/phenotypes/pheno.fam', sep = '\\t', index_col = 'IID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with CAGE Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chr, genes_df in genes_by_chrom:\n",
    "    tic = time.perf_counter()\n",
    "    predict_epigenome(chr, genes_df, individuals, f'{output_dir}/{chr}_CAGE_brain__predict.txt', track = 4980)\n",
    "    toc = time.perf_counter()\n",
    "    print(f'{chr}, {len(genes_df)} genes:', toc - tic, 'seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genomics",
   "language": "python",
   "name": "genomics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

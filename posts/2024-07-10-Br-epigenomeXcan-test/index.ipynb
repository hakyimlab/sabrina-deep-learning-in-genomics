{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: EpigenomeXcan test on Br rats\n",
    "author: Sabrina Mi\n",
    "date: 7/10/2024\n",
    "description: Calculate associations between predicted gene expression in adipose tissue and BMI to identify significant gene, while I figure out high to scale up predicting epigenome step.\n",
    "---\n",
    "\n",
    "CPU times: \n",
    "\n",
    "1. Compute founders matrix (n_genes, 8)\n",
    "    * 3190 genes: ~110s\n",
    "2. Compute samples matrix (n_samples, n_genes, 8):\n",
    "    * 3190 genes, 1 sample: ~4s\n",
    "    * 3190 genes, 10 samples: ~40s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 04:12:42.617981: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-25 04:12:49.066611: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/soft/libraries/trt/TensorRT-8.5.2.2.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.16.2-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/soft/libraries/trt/TensorRT-8.5.2.2.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.16.2-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/soft/perftools/darshan/darshan-3.4.4/lib:/opt/cray/pe/papi/7.0.1.2/lib64:/opt/cray/libfabric/1.15.2.0/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/comm_libs/nvshmem/lib:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/comm_libs/nccl/lib:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/math_libs/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/lib:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/extras/qd/lib:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/extras/CUPTI/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/lib64:/dbhome/db2cat/sqllib/lib64:/dbhome/db2cat/sqllib/lib64/gskit:/dbhome/db2cat/sqllib/lib32\n",
      "2024-07-25 04:12:49.067355: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/soft/libraries/trt/TensorRT-8.5.2.2.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.16.2-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/soft/libraries/trt/TensorRT-8.5.2.2.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.16.2-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/soft/perftools/darshan/darshan-3.4.4/lib:/opt/cray/pe/papi/7.0.1.2/lib64:/opt/cray/libfabric/1.15.2.0/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/comm_libs/nvshmem/lib:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/comm_libs/nccl/lib:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/math_libs/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/lib:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/extras/qd/lib:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/extras/CUPTI/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/lib64:/dbhome/db2cat/sqllib/lib64:/dbhome/db2cat/sqllib/lib64/gskit:/dbhome/db2cat/sqllib/lib32\n",
      "2024-07-25 04:12:49.067383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "import bisect\n",
    "columns = ['ACI', 'BN', 'BUF', 'F344', 'M520', 'MR', 'WKY', 'WN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_annot = pd.read_csv(\"/eagle/AIHPC4Edu/sabrina/Br_predictions/HS_founder_epigenomes/gene_mapping.txt\")\n",
    "genes_by_chrom = gene_annot.groupby('chromosome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_dir = \"/eagle/AIHPC4Edu/sabrina/Br_genotype_probabilities\"\n",
    "reference_dir = \"/eagle/AIHPC4Edu/sabrina/Br_predictions/HS_founder_epigenomes/human\"\n",
    "output_dir = \"/eagle/AIHPC4Edu/sabrina/Br_prediction_from_founders\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizing my haplotype probabilities storage is redundant, taking up extra space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "probabilities_file = f'{probabilities_dir}/chr1_probabilities.h5'\n",
    "with h5py.File(probabilities_file, 'a') as hf:\n",
    "    for dataset_name in list(hf.keys()):\n",
    "        if hf[dataset_name].shape == 8:\n",
    "            continue\n",
    "        # Read the dataset\n",
    "        data = hf[dataset_name][:]\n",
    "        positions = data[:, 0]  # Assuming the positions column is the first column\n",
    "        new_data = data[:, 1:]  # All columns except the first one\n",
    "        \n",
    "        # Create a temporary dataset without the positions column\n",
    "        temp_dataset_name = f\"temp_{dataset_name}\"\n",
    "        hf.create_dataset(temp_dataset_name, data=new_data)\n",
    "        \n",
    "        # Delete the original dataset\n",
    "        del hf[dataset_name]\n",
    "        \n",
    "        # Rename the temporary dataset to the original dataset name\n",
    "        hf.move(temp_dataset_name, dataset_name)\n",
    "        \n",
    "    hf.create_dataset('positions', data=positions)\n",
    "        # Store the positions vector as metadata (attribute) for the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split genes by chromosome number\n",
    "for individual, query haplotype probabilities at each gene tss, so we return an 8 x n_gene matrix. by batching by genes n_genes should be in the 1-2K range. stack to return a 3d array with dimensions n_samples x 8 x n_gene\n",
    "next query reference epigenome at each gene [446:450, CAGE_index], return 8 x n_gene matrix\n",
    "matrix multiply with something like tf.transpose(tf.tensordot(_W, _X, axes=[[1],[1]]),[1,0,2])\n",
    "https://stackoverflow.com/questions/41870228/understanding-tensordot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3190, 8)\n"
     ]
    }
   ],
   "source": [
    "for chr, genes_df in genes_by_chrom:\n",
    "    reference_file = f'{reference_dir}/{chr}_genes.h5'\n",
    "    with h5py.File(reference_file, 'r') as ref:\n",
    "        rows = []\n",
    "        for gene in genes_df['gene']:\n",
    "            founder_predictions = ref[gene][:, 446:450, 5278]\n",
    "            rows.append(founder_predictions)\n",
    "        ref_matrix = np.stack(rows, axis=0)\n",
    "        ref_tensor = tf.reduce_mean(tf.convert_to_tensor(ref_matrix, dtype=tf.float32), axis=2)\n",
    "        print(ref_tensor.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chr, genes_df in genes_by_chrom:\n",
    "    probabilities_file = f'{probabilities_dir}/{chr}_probabilities.h5'\n",
    "    with h5py.File(probabilities_file, 'r') as prob:\n",
    "        positions = prob['positions'][:]\n",
    "        individuals = list(prob.keys())\n",
    "        population_prob = []\n",
    "        for sample in individuals[10:20]:\n",
    "            rows = []\n",
    "            for tss in genes_df['tss']:\n",
    "                index = bisect.bisect_left(positions, tss)\n",
    "                tss_prob = (prob[sample][index-1,:] + prob[sample][index,:]) / 2\n",
    "                rows.append(tss_prob)\n",
    "            sample_prob = np.vstack(rows)\n",
    "            population_prob.append(sample_prob)\n",
    "        prob_matrix = np.stack(population_prob, axis=0)\n",
    "        prob_tensor = tf.convert_to_tensor(prob_matrix, dtype = tf.float32)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 3190, 8])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3190, 8])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_tensor.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

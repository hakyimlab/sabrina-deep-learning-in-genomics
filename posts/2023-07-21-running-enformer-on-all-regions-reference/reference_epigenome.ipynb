{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Running Enformer over reference genome\n",
    "description: We use all intervals along chromosomes 5 through 7 and 8 through 11\n",
    "author: Sabrina Mi\n",
    "date: 7/21/23\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect all intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "def write_intervals(chromosome, length):\n",
    "    start = 0\n",
    "    increment = 57344\n",
    "    end = 114688\n",
    "    with open(f\"metadata/chr{chromosome}_intervals.txt\", 'a') as f:\n",
    "        while end <= length:\n",
    "            f.write(f'\"chr{chromosome}_{start}_{end}\"\\n')\n",
    "            start += increment\n",
    "            end += increment\n",
    "        f.write(f'\"chr{chromosome}_{start}_{end}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "write_intervals(chromosome = 5, length = 181538259)\n",
    "write_intervals(chromosome = 6, length = 170805979)\n",
    "write_intervals(chromosome = 7, length = 159345973)\n",
    "# write_intervals(chromosome = 8, length = 145138636)\n",
    "# write_intervals(chromosome = 9, length = 138394717)\n",
    "# write_intervals(chromosome = 10, length = 133797422)\n",
    "# write_intervals(chromosome = 11, length = 135086622)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate Intervals\n",
    "\n",
    "Functions to concatenate enformer predictions can be expanded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import parsl\n",
    "from parsl.app.app import python_app\n",
    "from parsl.configs.local_threads import config\n",
    "parsl.load(config)\n",
    "\n",
    "L = 114688\n",
    "L2 = L // 2\n",
    "L4 = L // 4\n",
    "\n",
    "num_tracks = 5313\n",
    "num_bins = 896\n",
    "\n",
    "Q = num_bins // 4\n",
    "\n",
    "chr_lengths = {\"5\": 181538259, \"6\": 170805979, \"7\":159345973}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_chr_ints_list(chr_num, chr_len,):\n",
    "    intervals = []\n",
    "    \n",
    "    # determine number of tiles\n",
    "    num_tiles = chr_len // L2\n",
    "    if L2*(num_tiles)+L4 < chr_len:\n",
    "        num_tiles += 1\n",
    "\n",
    "    # create intervals\n",
    "    for i in range(num_tiles):\n",
    "        start = L2 * i\n",
    "        end = L2 * (i+2)\n",
    "        interval = f'chr{chr_num}_{start}_{end}'\n",
    "        intervals.append(interval)\n",
    "            \n",
    "    return intervals\n",
    "\n",
    "@python_app\n",
    "def concatenate_predictions(predictions_folder, concat_result_file, chr_num, chr_len):\n",
    "    intervals = get_chr_ints_list(chr_num, chr_len)\n",
    "    dest_data_name = f'chr{chr_num}'\n",
    "\n",
    "    # create empty dataset\n",
    "    with h5py.File(concat_result_file, \"w\") as dest_file: # Create or open the new big HDF5 file in write mode\n",
    "        last_interval_start = int(intervals[-1].split('_')[1])\n",
    "        num_last_bins = ((chr_len-(last_interval_start+L4)) // 128) + 1\n",
    "        data_length = (Q*3) + ((len(intervals)-2)*Q*2) + num_last_bins # Calculate the total length of the dataset\n",
    "        dest_file.create_dataset(name=dest_data_name, fillvalue=np.nan, shape=(data_length, 5313), chunks=(1000, 5313))\n",
    "        \n",
    "        # try removing chunks\n",
    "\n",
    "    # append to dataset\n",
    "    with h5py.File(concat_result_file, \"a\") as dest_file:\n",
    "\n",
    "        # first tile\n",
    "        first_tile = f'{predictions_folder}/{intervals[0]}_predictions.h5'\n",
    "        if os.path.exists(first_tile):\n",
    "            with h5py.File(first_tile, \"r\") as f: # Open first tile HDF5 file in read mode\n",
    "                tile_data = f[intervals[0]][()] # Access the dataset within the group\n",
    "                data_to_concat = tile_data[:Q*3] # Extract the first 3/4ths of the dataset\n",
    "        else:\n",
    "            data_to_concat = np.full((Q*2, 5313), np.nan) # first 2 Qs are nan\n",
    "            second_tile = f'{predictions_folder}/{intervals[1]}_predictions.h5'\n",
    "            if os.path.exists(second_tile):\n",
    "                with h5py.File(second_tile, \"r\") as f: # Open first tile HDF5 file in read mode\n",
    "                    tile_data = f[intervals[1]][()] # Access the np array within the group\n",
    "                    next_tileQ = tile_data[:Q] # Extract first Q\n",
    "                    data_to_concat = np.concatenate((data_to_concat, next_tileQ), axis=0) # 3rd Q is from second tile\n",
    "            else:\n",
    "                data_to_concat = np.full((Q*3, 5313), np.nan) # all 3 Qs are nan\n",
    "\n",
    "        dest_file[dest_data_name][:Q*3] = data_to_concat # add to dataset\n",
    "\n",
    "        # middle tiles\n",
    "        for i in range(1, len(intervals)-1):\n",
    "            curr_tile = f'{predictions_folder}/{intervals[i]}_predictions.h5'\n",
    "            if os.path.exists(curr_tile):        \n",
    "                with h5py.File(curr_tile, \"r\") as f: # Open each source HDF5 file in read mode\n",
    "                    tile_data = f[intervals[i]][()] # Access the dataset within the group\n",
    "                    data_to_concat = tile_data[Q:Q*3] # Extract the middle section of the dataset\n",
    "            else:\n",
    "                prev_tile = f'{predictions_folder}/{intervals[i-1]}_predictions.h5'\n",
    "                next_tile = f'{predictions_folder}/{intervals[i+1]}_predictions.h5'\n",
    "\n",
    "                if os.path.exists(prev_tile): # if prev tile exists\n",
    "                    with h5py.File(prev_tile, \"r\") as f:\n",
    "                        tile_data = f[intervals[i-1]][()]\n",
    "                        data_to_concat = tile_data[-Q:] # data starts with last Q from prev tile\n",
    "                else:\n",
    "                    data_to_concat = np.full((Q, 5313), np.nan) # datas first Q is nan\n",
    "                \n",
    "                if os.path.exists(next_tile): # if next tile exists\n",
    "                    with h5py.File(next_tile, \"r\") as f:\n",
    "                        tile_data = f[intervals[i+1]][()]\n",
    "                        data_to_concat = np.concatenate((data_to_concat, tile_data[:Q]), axis=0) # data ends with first Q from next tile\n",
    "                else:\n",
    "                    data_to_concat = np.concatenate((data_to_concat, np.full((Q, 5313), np.nan)), axis=0) # datas last Q is nan\n",
    "            \n",
    "            dest_file[dest_data_name][Q*(1+2*i):Q*(3+2*i)] = data_to_concat\n",
    "        \n",
    "\n",
    "        # last tile\n",
    "        last_tile = f'{predictions_folder}/{intervals[-1]}_predictions.h5'\n",
    "        if os.path.exists(last_tile):\n",
    "            with h5py.File(last_tile, \"r\") as f:\n",
    "                tile_data = f[intervals[-1]][()]\n",
    "                data_to_concat = tile_data[Q:Q+num_last_bins]\n",
    "        else:\n",
    "            prev_tile = f'{predictions_folder}/{intervals[-2]}_predictions.h5'\n",
    "            if os.path.exists(prev_tile):\n",
    "                with h5py.File(prev_tile, \"r\") as f:\n",
    "                    tile_data = f[intervals[-2]][()]\n",
    "                    if num_last_bins > Q:\n",
    "                        data_to_concat = tile_data[-Q:] # data starts with last Q from prev tile \n",
    "                        data_to_concat = np.concatenate((data_to_concat, np.full((num_last_bins-Q, 5313), np.nan)), axis=0) # ends with nan\n",
    "                    else:\n",
    "                        data_to_concat = tile_data[Q*3:(Q*3)+num_last_bins] # data ends with prev tile 3Q to end of chrom\n",
    "            else:\n",
    "                data_to_concat = np.full((num_last_bins, 5313), np.nan) # all last bins are nan\n",
    "        dest_file[dest_data_name][-num_last_bins:] = data_to_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "app_futures = []\n",
    "for chr in chr_lengths.keys():\n",
    "    predictions_folder = f\"/grand/TFXcan/imlab/users/sabrina/reftile_project/predictions_folder/chr{chr}_reference_overlapping_regions/predictions_2023-08-26/enformer_predictions/chr{chr}_reference/haplotype0\"\n",
    "    output = f\"/grand/TFXcan/imlab/users/lvairus/reftile_project/enformer-reference-epigenome/chr{chr}_cat.h5\"\n",
    "    app_futures.append(concatenate_predictions(predictions_folder, output, chr, chr_lengths[chr]))\n",
    "\n",
    "exec_futures = [q.result() for q in app_futures]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parsl",
   "language": "python",
   "name": "parsl"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Scaling up EpigenomeXcan: execution time, parallelization\"\n",
    "author: Sabrina Mi\n",
    "date: 1/3/2024\n",
    "---\n",
    "\n",
    "\n",
    "## Numpy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import interpolate\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "columns = ['POS', 'ACI', 'BN', 'BUF', 'F344', 'M520', 'MR', 'WKY', 'WN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/s1mi/Github/deep-learning-in-genomics/posts/2023-11-07-HS-founder-epigenomes/Br_expressed_genes.txt\", \"r\") as f:\n",
    "    gene_list = f.read().splitlines()\n",
    "with open(\"/home/s1mi/Github/deep-learning-in-genomics/posts/2023-11-07-HS-founder-epigenomes/metadata/intervals.txt\", \"r\") as f:\n",
    "    intervals = f.read().splitlines()\n",
    "mapping = pd.DataFrame({\"gene\": gene_list, \"interval\": intervals})\n",
    "split_intervals = mapping['interval'].str.split('_')\n",
    "mapping['chromosome'] = split_intervals.str[0]\n",
    "mapping['tss'] = split_intervals.str[1]\n",
    "mapping.drop(columns=['interval']).to_csv(\"/eagle/AIHPC4Edu/sabrina/Br_predictions/HS_founder_epigenomes/gene_mapping.txt\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_dir = \"/home/s1mi/Br_genotype_probabilities\"\n",
    "reference_dir = \"/eagle/AIHPC4Edu/sabrina/Br_predictions/HS_founder_epigenomes/human\"\n",
    "project_dir = \"/eagle/AIHPC4Edu/sabrina/Br_predictions/Br_epigenomes\"\n",
    "with open(\"/eagle/AIHPC4Edu/sabrina/Br_predictions/HS_founder_epigenomes/Br_samples.txt\", \"r\") as f:\n",
    "    individuals = f.read().splitlines()\n",
    "mapping = pd.read_csv(\"/eagle/AIHPC4Edu/sabrina/Br_predictions/HS_founder_epigenomes/gene_mapping.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_h5(path, predictions):\n",
    "    with h5py.File(path, \"w\") as hf:\n",
    "        for key, value in predictions.items():\n",
    "            hf[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_epigenome(chr, gene, start, end, prob):\n",
    "    bins = np.linspace(start, end, 896)\n",
    "    scaled_prob = []\n",
    "    for column in columns[1:]: \n",
    "        scaled_prob.append(np.interp(bins, prob['POS'], prob[column]))\n",
    "    pr_matrix = np.transpose(np.array(scaled_prob))\n",
    "    with h5py.File(os.path.join(reference_dir, f\"{chr}_genes.h5\"), \"r\") as hf:\n",
    "        matrix = hf[gene][:]\n",
    "    ref_matrix = np.transpose(matrix, axes=(1, 0, 2))\n",
    "    output = np.squeeze(pr_matrix[:, np.newaxis,:] @ ref_matrix, 1)\n",
    "    return output\n",
    "    \n",
    "\n",
    "def run_predictions(chr, prob, gene_annot):\n",
    "    predictions = {}\n",
    "    # interp = interpolate.interp1d(np.array(prob['POS']), np.array(prob.drop(columns=['POS'])), axis = 0)\n",
    "    start_time = time.perf_counter()\n",
    "    for index, row in gene_annot.iterrows():\n",
    "        gene = row['gene']\n",
    "        tss = row['tss']\n",
    "        start = tss - 57344\n",
    "        end = tss + 57344\n",
    "        # if (start >= prob.iloc[0,0] and end <= prob.iloc[-1,0]):\n",
    "        output = compute_epigenome(chr, gene, start, end, prob)\n",
    "        # Process genotype probabilities\n",
    "        predictions[gene] = output\n",
    "        if ((index + 1) % 100 == 0):\n",
    "            end_time = time.perf_counter()\n",
    "            print(index + 1, \"iterations:\", end_time - start_time, \"seconds\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 iterations: 30.23924341239035 seconds\n",
      "200 iterations: 58.741962409578264 seconds\n",
      "300 iterations: 86.70877797622234 seconds\n",
      "400 iterations: 114.42556379362941 seconds\n",
      "500 iterations: 142.98598795291036 seconds\n",
      "600 iterations: 171.59385333769023 seconds\n",
      "700 iterations: 199.60792813822627 seconds\n",
      "800 iterations: 227.5820687506348 seconds\n",
      "900 iterations: 254.93660536315292 seconds\n",
      "1000 iterations: 282.0493865776807 seconds\n",
      "1100 iterations: 309.8969806600362 seconds\n",
      "1200 iterations: 337.443421902135 seconds\n",
      "1300 iterations: 364.4411320472136 seconds\n",
      "1400 iterations: 391.4582523852587 seconds\n",
      "1500 iterations: 418.4635310182348 seconds\n",
      "1600 iterations: 467.45072995033115 seconds\n",
      "1700 iterations: 504.66587734408677 seconds\n",
      "1800 iterations: 541.083711117506 seconds\n",
      "1900 iterations: 574.226130806841 seconds\n",
      "2000 iterations: 608.9806590257213 seconds\n",
      "2100 iterations: 645.2934711230919 seconds\n",
      "2200 iterations: 675.4267345676199 seconds\n",
      "2300 iterations: 704.1126300338656 seconds\n",
      "2400 iterations: 732.3628087388352 seconds\n",
      "2500 iterations: 760.5593214053661 seconds\n",
      "2600 iterations: 789.4835011968389 seconds\n",
      "2700 iterations: 817.1563301282004 seconds\n",
      "2800 iterations: 845.1840146286413 seconds\n",
      "2900 iterations: 873.6965537844226 seconds\n",
      "3000 iterations: 901.7655613124371 seconds\n",
      "3100 iterations: 929.373354382813 seconds\n"
     ]
    }
   ],
   "source": [
    "genes_df = mapping[mapping['chromosome'] == 'chr1']\n",
    "with h5py.File(os.path.join(probabilities_dir, f\"chr1_probabilities.h5\"), \"r\") as input:\n",
    "        output_dir = os.path.join(project_dir, '000789972A')\n",
    "        # if not os.path.isdir(output_dir):\n",
    "        #     os.makedirs(output_dir)\n",
    "        prob = pd.DataFrame(input['000789972A'][:], columns=columns)\n",
    "        predictions = run_predictions('chr1', prob, genes_df)\n",
    "        write_h5(os.path.join(output_dir, f\"{chr}_genes.h5\"), predictions) \n",
    "# print(f\"{chr}:\", len(group), \"genes...\", (end_time - start_time) // 60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(project_dir, '000789972A')\n",
    "write_h5(os.path.join(output_dir, f\"{chr}_genes.h5\"), predictions) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy Test\n",
    "\n",
    "Using scipy's interp1d function to interpolate all 8 columns simultaneously. Generating the interpolation function costs about 20 seconds of runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_epigenome(gene, start, end, interp):\n",
    "    bins = np.linspace(start, end, 896)\n",
    "    pr_matrix = interp(bins)\n",
    "    with h5py.File(os.path.join(reference_dir, f\"chr1_genes.h5\"), \"r\") as hf:\n",
    "        matrix = hf[gene][:]\n",
    "    ref_matrix = np.transpose(matrix, axes=(1, 0, 2))\n",
    "    output = np.squeeze(pr_matrix[:, np.newaxis,:] @ ref_matrix, 1)\n",
    "    return output\n",
    "def run_predictions(prob, gene_annot):\n",
    "    predictions = {}\n",
    "    interp = interpolate.interp1d(np.array(prob['POS']), np.array(prob.drop(columns=['POS'])), axis = 0)\n",
    "    start_time = time.perf_counter()\n",
    "    for index, row in gene_annot.iterrows():\n",
    "        gene = row['gene']\n",
    "        tss = row['tss']\n",
    "        start = tss - 57344\n",
    "        end = tss + 57344\n",
    "        if (start >= prob.iloc[0,0] and end <= prob.iloc[-1,0]):\n",
    "            output = compute_epigenome(gene, start, end, interp)\n",
    "            # Process genotype probabilities\n",
    "            predictions[gene] = output\n",
    "            end_time = time.perf_counter()\n",
    "            if ((index + 1) % 200 == 0):\n",
    "                print(index + 1, \"iterations:\", end_time - start_time, \"seconds\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 iterations: 55.26515249814838 seconds\n",
      "400 iterations: 105.58576967194676 seconds\n",
      "600 iterations: 136.04839484021068 seconds\n",
      "800 iterations: 193.6581759629771 seconds\n",
      "1000 iterations: 254.47129005938768 seconds\n",
      "1200 iterations: 305.2362718908116 seconds\n",
      "1400 iterations: 355.68491036072373 seconds\n",
      "1600 iterations: 417.03871417138726 seconds\n",
      "1800 iterations: 469.593163177371 seconds\n",
      "2000 iterations: 522.8375322008505 seconds\n",
      "2200 iterations: 576.9442004561424 seconds\n",
      "2400 iterations: 629.3138183737174 seconds\n",
      "2600 iterations: 687.7609961153939 seconds\n",
      "2800 iterations: 739.739781155251 seconds\n",
      "3000 iterations: 797.2053201990202 seconds\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(os.path.join(probabilities_dir, f\"chr1_probabilities.h5\"), \"r\") as input:\n",
    "    prob = pd.DataFrame(input['000789972A'][:], columns=columns)\n",
    "    predictions = run_predictions(prob, genes_df)\n",
    "    # write_h5(os.path.join(output_dir, f\"{chr}_genes.h5\"), predictions) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Test\n",
    "\n",
    "Neither tensorflow or torch libraries offer interpolation for the purpose I need, the only step to incorporate GPU computation is in the final matrix multiplication. However, transferring matrix from CPU to GPU memory adds overhead, so we need to test if using GPU for matrix multiplication is fast enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_epigenome(gene, start, end, prob):\n",
    "    bins = tf.linspace(start, end, 896)\n",
    "    scaled_prob = []\n",
    "    for column in columns[1:]: \n",
    "        scaled_prob.append(np.interp(bins, prob['POS'], prob[column]))\n",
    "    pr_matrix = np.transpose(np.array(scaled_prob))\n",
    "    with h5py.File(os.path.join(reference_dir, f\"{chr}_genes.h5\"), \"r\") as hf:\n",
    "        matrix = hf[gene][:]\n",
    "    ref_matrix = np.transpose(matrix, axes=(1, 0, 2))\n",
    "    output = np.squeeze(pr_matrix[:, np.newaxis,:] @ ref_matrix, 1)\n",
    "    return output\n",
    "    \n",
    "\n",
    "def run_predictions(prob, gene_annot):\n",
    "    predictions = {}\n",
    "    # interp = interpolate.interp1d(np.array(prob['POS']), np.array(prob.drop(columns=['POS'])), axis = 0)\n",
    "    for _, row in gene_annot.iterrows():\n",
    "        gene = row['gene']\n",
    "        tss = row['tss']\n",
    "        start = tss - 57344\n",
    "        end = tss + 57344\n",
    "        # if (start >= prob.iloc[0,0] and end <= prob.iloc[-1,0]):\n",
    "        output = compute_epigenome(gene, start, end, prob)\n",
    "        # Process genotype probabilities\n",
    "        predictions[gene] = output\n",
    "\n",
    "    return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

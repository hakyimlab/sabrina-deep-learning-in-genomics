{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cyvcf2\n",
      "  Downloading cyvcf2-0.30.22-cp39-cp39-macosx_10_9_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/sabrinami/miniconda3/envs/tf/lib/python3.9/site-packages (from cyvcf2) (1.24.3)\n",
      "Collecting coloredlogs (from cyvcf2)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: click in /Users/sabrinami/miniconda3/envs/tf/lib/python3.9/site-packages (from cyvcf2) (8.1.4)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->cyvcf2)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, cyvcf2\n",
      "Successfully installed coloredlogs-15.0.1 cyvcf2-0.30.22 humanfriendly-10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "SCRIPTS=\"/Users/sabrinami/Github/shared_folder/enformer_pipeline/scripts/\" \n",
    "MODULES=SCRIPTS+\"modules/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Running the Enformer Pipeline locally\"\n",
    "description: \"We picked two regions and ran enformer on reference genome and personalized genome for one individual\"\n",
    "author: \"Sabrina Mi\"\n",
    "date: 7/25/23\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, re\n",
    "import pandas as pd # for manipulating dataframes\n",
    "import time\n",
    "##import parsl ## local runs, no parsl\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "# some locations and folders\n",
    "# whereis_script = os.path.dirname(__file__) #os.path.dirname(sys.argv[0]) # or os.path.dirname(__file__)\n",
    "script_path = SCRIPTS\n",
    "# batch_utils_path = os.path.join(script_path, 'modules')\n",
    "# sys.path.append(batch_utils_path)\n",
    "\n",
    "## MODULES should be the location of the modules\n",
    "sys.path.append(MODULES)\n",
    "\n",
    "import loggerUtils\n",
    "import directives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def enformer_predict(parameters):\n",
    "\n",
    "    params_path = parameters\n",
    "\n",
    "    if not os.path.isabs(params_path):\n",
    "        params_path = os.path.abspath(params_path)\n",
    "\n",
    "    p_two = os.path.join(script_path, 'modules', 'predictUtils_two.py')\n",
    "\n",
    "    with open(f'{params_path}') as f:\n",
    "        parameters = json.load(f)\n",
    "        # The rest of the script remains the same\n",
    "\n",
    "        prediction_data_name = parameters['prediction_data_name']\n",
    "        prediction_id = parameters['prediction_id']\n",
    "        run_date = parameters['date'] if parameters['date'] is not None else date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        if parameters['sub_dir'] == True:\n",
    "            project_dir = os.path.join(parameters['project_dir'], 'predictions_folder', f'{prediction_data_name}_{prediction_id}', f'predictions_{run_date}')\n",
    "        elif parameters['sub_dir'] == False:\n",
    "            project_dir = os.path.join(parameters['project_dir'], f'{prediction_data_name}_{prediction_id}', f'predictions_{run_date}')\n",
    "        else:\n",
    "            raise Exception('ERROR - `sub_dir` argument must be a boolean, either true or false')\n",
    "\n",
    "        interval_list_file = parameters['interval_list_file']\n",
    "        predictions_log_dir = os.path.join(project_dir, parameters['predictions_log_dir'])\n",
    "        job_log_dir = os.path.join(project_dir, parameters['write_log']['logdir'])\n",
    "        n_regions = parameters[\"n_regions\"]\n",
    "        batch_regions = int(parameters['batch_regions'])\n",
    "        use_parsl = parameters['use_parsl']\n",
    "        parsl_parameters = parameters['parsl_parameters']\n",
    "        sequence_source = parameters['sequence_source']\n",
    "        exclude_regions = parameters[\"exclude_regions\"]\n",
    "        reverse_complement = parameters[\"reverse_complement\"]\n",
    "    \n",
    "        metadata_dir = parameters['metadata_dir']\n",
    "        if not os.path.isdir(metadata_dir):\n",
    "            os.makedirs(metadata_dir)\n",
    "\n",
    "        output_dir = os.path.join(project_dir, parameters['output_dir'])\n",
    "        if not os.path.isdir(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        if int(n_regions) == -1:\n",
    "            n_regions = None\n",
    "        elif int(n_regions) > 0:\n",
    "            n_regions = (n_regions) if isinstance(n_regions, int) else None\n",
    "\n",
    "        # personalized parameters \n",
    "        individuals = parameters['individuals'] if sequence_source == 'personalized' else None\n",
    "        vcf_files_dict = parameters['vcf_files'] if sequence_source == 'personalized' else None\n",
    "\n",
    "        if sequence_source == 'personalized':\n",
    "             # use only the chromosomes that have been made available in the config file vcf params\n",
    "            print(f'INFO - Sequence source is {sequence_source}. Using a reference genome + vcf files.')\n",
    "            chromosomes = list(vcf_files_dict['files'].keys())\n",
    "\n",
    "            batch_individuals = parameters[\"batch_individuals\"]\n",
    "            n_individuals = int(parameters['n_individuals'])\n",
    "        # list of chromosomes (if the sequence source is reference)\n",
    "        elif sequence_source == 'reference':\n",
    "            print(f'INFO - Sequence source is {sequence_source}. Using a reference genome.')\n",
    "            chromosomes = [f'chr{i}' for i in range(1, 23)]\n",
    "            chromosomes.extend(['chrX'])\n",
    "\n",
    "        if reverse_complement:\n",
    "            print(f'INFO - Predicting on reverse complements too')\n",
    "\n",
    "    # # write the params_path to a config.json file in a predefined folder\n",
    "    # tmp_config_data = {'params_path': params_path}\n",
    "    # tmp_config_file = os.path.join(batch_utils_path, f'tmp_config_{prediction_data_name}_{prediction_id}.json')\n",
    "    # with open(tmp_config_file, mode='w') as cj:\n",
    "    #     json.dump(tmp_config_data, cj)\n",
    "\n",
    "    # modify parsl parameters to add the working directory\n",
    "    parsl_parameters['working_dir'] = project_dir\n",
    "\n",
    "    if not os.path.isdir(job_log_dir):\n",
    "        os.makedirs(job_log_dir)\n",
    "\n",
    "    # set parsl directives\n",
    "    if use_parsl:\n",
    "        directives.parsl_directives(use_parsl, parsl_parameters)\n",
    "    \n",
    "    # importing this module does not work; best to execute it here\n",
    "    predict_utils_one = os.path.join(script_path, 'modules', 'predictUtils_one.py')\n",
    "    exec(open(predict_utils_one).read(), globals(), globals())\n",
    "\n",
    "    # decorate the prediction function with or without parsl\n",
    "    prediction_fxn = return_prediction_function(use_parsl)\n",
    "\n",
    "    # determine what individuals to predict on and all that\n",
    "    if sequence_source == 'personalized':\n",
    "        \n",
    "        if isinstance(individuals, list):\n",
    "            id_list = individuals\n",
    "            pass\n",
    "        elif isinstance(individuals, type('str')):\n",
    "            if os.path.isfile(individuals):\n",
    "                if n_individuals == -1:\n",
    "                    id_list = pd.read_table(individuals, header=None)[0].tolist()[0:]\n",
    "                elif n_individuals > 0:\n",
    "                    id_list = pd.read_table(individuals, header=None)[0].tolist()[0:(n_individuals)]\n",
    "            else:\n",
    "                id_list = [individuals]\n",
    "        print(f'INFO - Found {len(id_list)} individuals to predict on')\n",
    "\n",
    "    elif sequence_source == 'reference':\n",
    "        id_list = [prediction_data_name]\n",
    "        print(f'INFO - Found one reference set named {id_list[0]} to predict on')\n",
    "    elif sequence_source == 'random':\n",
    "        id_list = [prediction_data_name]\n",
    "        print(f'INFO - Prediction will be on a randomly generated set')\n",
    "\n",
    "    # set log files to be put in a folder and touch the log files per sample\n",
    "    prediction_logfiles_folder = predictions_log_dir\n",
    "    if not os.path.isdir(prediction_logfiles_folder):\n",
    "        os.makedirs(prediction_logfiles_folder)\n",
    "        \n",
    "    # list of intervals to be predicted on\n",
    "    a = pd.read_table(interval_list_file, sep=' ', header=None).dropna(axis=0) #.drop_duplicates(subset=['region', 'sample', 'status', 'sequence_source'], keep='last')\n",
    "    list_of_regions = a[0].tolist()[0:(n_regions)] # a list of queries\n",
    "    print(f'INFO - Found {len(list_of_regions)} regions to be split into batches with at most {batch_regions} regions in each batch.')\n",
    "\n",
    "    # filter the list of chromosomes to be compatible with the available regions\n",
    "    chromosomes = list(set([r.split('_')[0] for r in list_of_regions]))\n",
    "    #print(f'INFO - Chromosomes to predict on are: {chromosomes}')\n",
    "\n",
    "    # should some regions be excluded?\n",
    "    if exclude_regions == True:\n",
    "        # seach for the invalid_regions.csv file\n",
    "        exclude_file = os.path.join(job_log_dir, 'invalid_queries.csv')\n",
    "        if os.path.isfile(exclude_file):\n",
    "            exclude_these_regions = pd.read_csv(exclude_file)['region'].tolist()\n",
    "            print(f'INFO - Found regions to be excluded from the input regions.')\n",
    "            list_of_regions = [l for l in list_of_regions if l not in exclude_these_regions]  \n",
    "            print(f'INFO - Updated number of regions to predict on is {len(list_of_regions)}')\n",
    "        else:\n",
    "            print(f'INFO - No regions to exclude yet. You either did not supply a file, this is the first run, or there are truly no regions to exclude')\n",
    "            exclude_these_regions = None\n",
    "    else:\n",
    "        exclude_file = None\n",
    "    \n",
    "    # batch the samples too\n",
    "    # if you have 1000 individuals, it may be too much\n",
    "    if len(id_list) > 5:\n",
    "        if batch_individuals is not None:\n",
    "            if isinstance(batch_individuals, int):\n",
    "                sample_batches = list(generate_batch_n_elems(id_list, n = batch_individuals)) # 5 samples in each batch\n",
    "                print(f'INFO - There are more than 10 individuals. Predictions will be done for every {batch_individuals} individuals.')\n",
    "            else:\n",
    "                raise Exception(f'ERROR - argument `batch_individuals` is not a str type. You supplied a {type(batch_individuals).__name__}')\n",
    "        else:\n",
    "            print(f'INFO - You have multiple individuals/samples and have not supplied how to batch them. For efficient use of resources, use the `batch_individuals` argument.')\n",
    "    else:\n",
    "        sample_batches = [id_list] # put the list in a list\n",
    "        print(f'INFO - There seem to be just one sample i.e. {sample_batches}. No need to batch.')\n",
    "\n",
    "    # to make this fast, pass multiple regions to one parsl app\n",
    "    sample_app_futures = []\n",
    "    for sample_list in sample_batches:\n",
    "        for chromosome in chromosomes:\n",
    "            #print(chromosome)\n",
    "            chr_list_of_regions = [r for r in list_of_regions if r.startswith(f\"{chromosome}_\")]\n",
    "            if sequence_source == 'personalized':\n",
    "                chr_vcf_file = os.path.join(vcf_files_dict['folder'], vcf_files_dict['files'][chromosome])\n",
    "            elif sequence_source == 'reference':\n",
    "                chr_vcf_file = None\n",
    "\n",
    "            if not chr_list_of_regions:\n",
    "                print(f'WARNING - {chromosome} sites are not available.')\n",
    "                continue\n",
    "\n",
    "            # I want many regions to be put in a parsl app\n",
    "            if len(chr_list_of_regions) > batch_regions:\n",
    "                region_batches = generate_batch_n_elems(chr_list_of_regions, n=batch_regions) # batch_regions total batches\n",
    "            else:\n",
    "                region_batches = [chr_list_of_regions]\n",
    "            \n",
    "            count = 0\n",
    "            for region_list in region_batches:\n",
    "                #print(len(sample_list))\n",
    "                #print(f'{len(region_list)} regions in {chromosome} for {len(sample_list)} samples')\n",
    "                sample_app_futures.append(prediction_fxn(batch_regions=list(region_list), samples=list(sample_list), path_to_vcf = chr_vcf_file, batch_num = count, script_path=script_path, output_dir=output_dir, prediction_logfiles_folder=prediction_logfiles_folder, sequence_source=sequence_source, tmp_config_path=params_path, p_two=p_two))   \n",
    "\n",
    "                count = count + 1 \n",
    "\n",
    "    if use_parsl == True:\n",
    "        print(f'INFO - Executing parsl futures for {len(sample_app_futures)} parsl apps')\n",
    "        exec_futures = [q.result() for q in sample_app_futures] \n",
    "        #print(sample_app_futures)\n",
    "        print(f'INFO - Finished predictions for all')\n",
    "    elif use_parsl == False:\n",
    "        print(f'INFO - Finished predictions for: {sample_app_futures} ...')\n",
    "\n",
    "    # just so I don't have to deal with having too many resources, I can request a small amount of resource\n",
    "    check_fxn = return_check_function(use_parsl)\n",
    "    SUMMARY_FILE = os.path.join(job_log_dir, f'{prediction_data_name}_{prediction_id}_{run_date}.summary')\n",
    "    summary_exec = []\n",
    "    for sample in id_list:\n",
    "        if os.path.isfile(os.path.join(prediction_logfiles_folder, f\"{sample}_log.csv\")):\n",
    "            summary_exec.append(check_fxn(sample=sample, predictions_folder=output_dir, log_folder=prediction_logfiles_folder, interval_list_file=interval_list_file, exclude_csv=exclude_file, sequence_source=sequence_source))\n",
    "\n",
    "    if use_parsl:\n",
    "        summary_exec = [q.result() for q in summary_exec]\n",
    "        parsl.clear() # end parsl\n",
    "\n",
    "    #summary_exec = list(set(summary_exec))\n",
    "    for i, qr in enumerate(summary_exec):\n",
    "        loggerUtils.write_logger(log_msg_type=qr['logtype'], logfile=SUMMARY_FILE, message=qr['logmessage'])\n",
    "\n",
    "    # regex the summary file and save the failed ones e.t.c to csv\n",
    "    # --- there is a better way to do this but for now, this will do\n",
    "\n",
    "    warning_pattern = r\"^\\[WARNING.*For\\s(\\w+|\\d+).*\"\n",
    "    success_pattern = r\"^\\[INFO.*For\\s(\\w+|\\d+).*\"\n",
    "    with open(SUMMARY_FILE, 'r') as f:\n",
    "        lines = list(set(f.readlines()))\n",
    "    # print(line)\n",
    "    warning_result = [re.search(warning_pattern, l).group(1) for l in lines if not re.search(warning_pattern, l) is None]\n",
    "    success_result = [re.search(success_pattern, l).group(1) for l in lines if not re.search(success_pattern, l) is None]\n",
    "\n",
    "    pd.DataFrame(list(set(warning_result))).to_csv(os.path.join(metadata_dir, f'{prediction_data_name}_{prediction_id}_{run_date}.unsuccessful_predictions.csv'), index=False, header=False)\n",
    "\n",
    "    pd.DataFrame(list(set(success_result))).to_csv(os.path.join(metadata_dir, f'{prediction_data_name}_{prediction_id}_{run_date}.successful_predictions.csv'), index=False, header=False)\n",
    "\n",
    "    # collect the successfule predictions\n",
    "    # successful_predictions = list(set([q['sample'] for q in summary_exec if q['logtype'] == 'INFO']))\n",
    "    # unsuccessful_predictions = list(set([q['sample'] for q in summary_exec if q['logtype'] == 'WARNING']))\n",
    "    # pd.DataFrame({'successful_predictions':successful_predictions}).to_csv(os.path.join(metadata_dir, f'{prediction_data_name}_{prediction_id}_{run_date}.successful_predictions.csv'), index=False, header=False)\n",
    "    # pd.DataFrame({'unsuccessful_predictions':unsuccessful_predictions}).to_csv(os.path.join(metadata_dir, f'{prediction_data_name}_{prediction_id}_{run_date}.unsuccessful_predictions.csv'), index=False, header=False)\n",
    "\n",
    "    print(f'INFO - Check {SUMMARY_FILE} for a summary of the entire run.')\n",
    "    print(f'INFO - Check `{metadata_dir}` for successful and unsucessful predictions.')\n",
    "\n",
    "    # == After predictions are complete, a json file will be written out to help with aggregation\n",
    "    print(f'INFO - Writing `aggregation_config_{prediction_data_name}_{prediction_id}.json` file to {metadata_dir}')\n",
    "    agg_dt = {'predictions_folder': project_dir, 'enformer_prediction_path': f'{output_dir}', 'prediction_logfiles_folder':prediction_logfiles_folder, 'prediction_data_name':prediction_data_name, 'sequence_source': sequence_source, 'run_date':run_date, 'prediction_id':prediction_id, 'individuals': None if sequence_source in ['reference', 'random'] else individuals, 'n_individuals':n_individuals if sequence_source == 'personalized' else None}\n",
    "\n",
    "    with(open(f'{metadata_dir}/aggregation_config_{prediction_data_name}_{prediction_id}.json', mode='w')) as wj:\n",
    "        json.dump(agg_dt, wj)\n",
    "\n",
    "    # remove temporatry config file\n",
    "    # print(f\"INFO - Cleaning up: Removing temporary config file at {tmp_config_file}\")\n",
    "    # os.remove(tmp_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Sequence source is reference. Using a reference genome.\n",
      "INFO - Found one reference set named reference_enformer_minimal to predict on\n",
      "INFO - Found 2 regions to be split into batches with at most 5 regions in each batch.\n",
      "INFO - No regions to exclude yet. You either did not supply a file, this is the first run, or there are truly no regions to exclude\n",
      "INFO - There seem to be just one sample i.e. [['reference_enformer_minimal']]. No need to batch.\n",
      "Using this config file: /Users/sabrinami/Github/shared_folder/enformer_pipeline/run_locally.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:kipoi_utils.external.related.mixins:Unrecognized fields for DataLoaderDescription: {'postprocessing'}. Available fields are {'info', 'output_schema', 'type', 'args', 'dependencies', 'writers', 'path', 'defined_as'}\n",
      "WARNING:kipoi_utils.external.related.mixins:Unrecognized fields for DataLoaderDescription: {'postprocessing'}. Available fields are {'info', 'output_schema', 'type', 'args', 'dependencies', 'writers', 'path', 'defined_as'}\n",
      "WARNING:kipoi_utils.external.related.mixins:Unrecognized fields for DataLoaderDescription: {'postprocessing'}. Available fields are {'info', 'output_schema', 'type', 'args', 'dependencies', 'writers', 'path', 'defined_as'}\n",
      "[INFO: 07/24/2023 10:43:13 AM] [CACHE] (fasta) [CacheInfo(hits=1, misses=1, maxsize=5, currsize=1) for chr1_65419_71585]\n",
      "INFO:cache_log:[CACHE] (fasta) [CacheInfo(hits=1, misses=1, maxsize=5, currsize=1) for chr1_65419_71585]\n",
      "[INFO: 07/24/2023 10:43:58 AM] [CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=1, maxsize=5, currsize=1)]\n",
      "[INFO: 07/24/2023 10:43:58 AM] [CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=1, maxsize=5, currsize=1)]\n",
      "INFO:cache_log:[CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=1, maxsize=5, currsize=1)]\n",
      "[INFO: 07/24/2023 10:43:58 AM] [CACHE] (fasta) [CacheInfo(hits=1, misses=1, maxsize=5, currsize=1) for chr1_450740_451678]\n",
      "[INFO: 07/24/2023 10:43:58 AM] [CACHE] (fasta) [CacheInfo(hits=1, misses=1, maxsize=5, currsize=1) for chr1_450740_451678]\n",
      "[INFO: 07/24/2023 10:43:58 AM] [CACHE] (fasta) [CacheInfo(hits=1, misses=1, maxsize=5, currsize=1) for chr1_450740_451678]\n",
      "INFO:cache_log:[CACHE] (fasta) [CacheInfo(hits=1, misses=1, maxsize=5, currsize=1) for chr1_450740_451678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample reference_enformer_minimal chr1_65419_71585 haplotype0 predictions are of the correct shape:  (896, 5313)\n",
      "Sample reference_enformer_minimal chr1_65419_71585 haplotypes predictions have been saved.\n",
      "Sample reference_enformer_minimal chr1_65419_71585 haplotypes predictions have been logged.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO: 07/24/2023 10:44:42 AM] [CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=1, maxsize=5, currsize=1)]\n",
      "[INFO: 07/24/2023 10:44:42 AM] [CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=1, maxsize=5, currsize=1)]\n",
      "[INFO: 07/24/2023 10:44:42 AM] [CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=1, maxsize=5, currsize=1)]\n",
      "[INFO: 07/24/2023 10:44:42 AM] [CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=1, maxsize=5, currsize=1)]\n",
      "INFO:cache_log:[CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=1, maxsize=5, currsize=1)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample reference_enformer_minimal chr1_450740_451678 haplotype0 predictions are of the correct shape:  (896, 5313)\n",
      "Sample reference_enformer_minimal chr1_450740_451678 haplotypes predictions have been saved.\n",
      "Sample reference_enformer_minimal chr1_450740_451678 haplotypes predictions have been logged.\n",
      "[INFO] (time) to predict on batch 0 is 90.45835176800006\n",
      "INFO - Finished predictions for: [[0, 0]] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING: 07/24/2023 10:44:42 AM] WARNING - For reference_enformer_minimal, either all predictions don't match all logged queries in the interval list files minus excluded regions or vice versa. This can happen if you have supplied a list of intervals but have chosen to predict on a subset. If this is the case, this behavior is normal. If you are unsure, please re-run the enformer prediction pipeline with the same parameters. You may supply a csv file of regions to exclude if available, but this should not matter.\n",
      "WARNING:summary_log:WARNING - For reference_enformer_minimal, either all predictions don't match all logged queries in the interval list files minus excluded regions or vice versa. This can happen if you have supplied a list of intervals but have chosen to predict on a subset. If this is the case, this behavior is normal. If you are unsure, please re-run the enformer prediction pipeline with the same parameters. You may supply a csv file of regions to exclude if available, but this should not matter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Check /Users/sabrinami/Desktop/2022-23/tutorials/enformer_pipeline_test/predictions_folder/reference_enformer_minimal_some_regions/predictions_2023-07-24/job_logs/reference_enformer_minimal_some_regions_2023-07-24.summary for a summary of the entire run.\n",
      "INFO - Check `/Users/sabrinami/Github/shared_folder/enformer_pipeline/metadata` for successful and unsucessful predictions.\n",
      "INFO - Writing `aggregation_config_reference_enformer_minimal_some_regions.json` file to /Users/sabrinami/Github/shared_folder/enformer_pipeline/metadata\n"
     ]
    }
   ],
   "source": [
    "enformer_predict('run_locally.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups and Datasets in the HDF5 file:\n",
      "chr1_65419_71585\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "file_path=\"/Users/sabrinami/Desktop/2022-23/tutorials/enformer_pipeline_test/predictions_folder/reference_enformer_minimal_some_regions/predictions_2023-07-24/enformer_predictions/reference_enformer_minimal/haplotype0/chr1_65419_71585_predictions.h5\"\n",
    "with h5py.File(file_path, \"r\") as file:\n",
    "    # List all the groups and datasets in the file\n",
    "    print(\"Groups and Datasets in the HDF5 file:\")\n",
    "    for name in file:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(file_path, \"r\") as file:\n",
    "    # Step 2: Access datasets and attributes within the file\n",
    "    dataset_name = \"chr1_65419_71585\"\n",
    "    dataset = file[dataset_name]\n",
    "\n",
    "    # Example: Read the entire dataset into a NumPy array\n",
    "    data = dataset[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(896, 5313)\n",
      "[[0.5291589  0.5204935  0.57351285 ... 0.00935043 0.01473221 0.01198051]\n",
      " [0.65397567 0.6207656  0.7431343  ... 0.01434602 0.02783546 0.02128439]\n",
      " [0.5326947  0.5510576  0.767584   ... 0.01038613 0.03195343 0.0286333 ]\n",
      " ...\n",
      " [0.00713983 0.0077101  0.00634603 ... 0.00115762 0.01039891 0.00428226]\n",
      " [0.00585703 0.00676019 0.00490488 ... 0.0047263  0.0377146  0.01976843]\n",
      " [0.00355603 0.00391064 0.00305343 ... 0.00085077 0.00762693 0.00313292]]\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Sequence source is personalized. Using a reference genome + vcf files.\n",
      "INFO - Found 2 individuals to predict on\n",
      "INFO - Found 2 regions to be split into batches with at most 5 regions in each batch.\n",
      "INFO - No regions to exclude yet. You either did not supply a file, this is the first run, or there are truly no regions to exclude\n",
      "INFO - There seem to be just one sample i.e. [['HG00096', 'HG00097']]. No need to batch.\n",
      "Using this config file: /Users/sabrinami/Github/shared_folder/enformer_pipeline/run_local_personalized.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO: 07/24/2023 03:25:05 PM] [CACHE] (fasta) [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2) for chr1_65419_71585]\n",
      "[INFO: 07/24/2023 03:25:05 PM] [CACHE] (fasta) [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2) for chr1_65419_71585]\n",
      "[INFO: 07/24/2023 03:25:05 PM] [CACHE] (fasta) [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2) for chr1_65419_71585]\n",
      "[INFO: 07/24/2023 03:25:05 PM] [CACHE] (fasta) [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2) for chr1_65419_71585]\n",
      "[INFO: 07/24/2023 03:25:05 PM] [CACHE] (fasta) [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2) for chr1_65419_71585]\n",
      "INFO:cache_log:[CACHE] (fasta) [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2) for chr1_65419_71585]\n",
      "/Users/sabrinami/Github/shared_folder/enformer_pipeline/scripts/modules/sequencesUtils.py:177: UserWarning: no intervals found for b'/Users/sabrinami/Library/CloudStorage/Box-Box/imlab-data/Reference-Data/1000G/vcf_snps_only/ALL.chr1.shapeit2_integrated_SNPs_v2a_27022019.GRCh38.phased.vcf.gz' at chr1:-128105-265111\n",
      "  variants_dictionary['positions'] = tuple(variant.POS for variant in cyvcf2_object(query))\n",
      "[E::hts_parse_region] Coordinates must be > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample HG00096 chr1_65419_71585 haplotype1 predictions are of the correct shape:  (896, 5313)\n",
      "Sample HG00096 chr1_65419_71585 haplotype2 predictions are of the correct shape:  (896, 5313)\n",
      "Sample HG00096 chr1_65419_71585 haplotypes predictions have been saved.\n",
      "Sample HG00096 chr1_65419_71585 haplotypes predictions have been logged.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO: 07/24/2023 03:34:49 PM] [CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2)]\n",
      "[INFO: 07/24/2023 03:34:49 PM] [CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2)]\n",
      "[INFO: 07/24/2023 03:34:49 PM] [CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2)]\n",
      "[INFO: 07/24/2023 03:34:49 PM] [CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2)]\n",
      "[INFO: 07/24/2023 03:34:49 PM] [CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2)]\n",
      "[INFO: 07/24/2023 03:34:49 PM] [CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2)]\n",
      "INFO:cache_log:[CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2)]\n",
      "[INFO: 07/24/2023 03:34:50 PM] [CACHE] (fasta) [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2) for chr1_450740_451678]\n",
      "[INFO: 07/24/2023 03:34:50 PM] [CACHE] (fasta) [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2) for chr1_450740_451678]\n",
      "[INFO: 07/24/2023 03:34:50 PM] [CACHE] (fasta) [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2) for chr1_450740_451678]\n",
      "[INFO: 07/24/2023 03:34:50 PM] [CACHE] (fasta) [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2) for chr1_450740_451678]\n",
      "[INFO: 07/24/2023 03:34:50 PM] [CACHE] (fasta) [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2) for chr1_450740_451678]\n",
      "[INFO: 07/24/2023 03:34:50 PM] [CACHE] (fasta) [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2) for chr1_450740_451678]\n",
      "[INFO: 07/24/2023 03:34:50 PM] [CACHE] (fasta) [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2) for chr1_450740_451678]\n",
      "INFO:cache_log:[CACHE] (fasta) [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2) for chr1_450740_451678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample HG00097 chr1_65419_71585 haplotype1 predictions are of the correct shape:  (896, 5313)\n",
      "Sample HG00097 chr1_65419_71585 haplotype2 predictions are of the correct shape:  (896, 5313)\n",
      "Sample HG00097 chr1_65419_71585 haplotypes predictions have been saved.\n",
      "Sample HG00097 chr1_65419_71585 haplotypes predictions have been logged.\n",
      "Sample HG00096 chr1_450740_451678 haplotype1 predictions are of the correct shape:  (896, 5313)\n",
      "Sample HG00096 chr1_450740_451678 haplotype2 predictions are of the correct shape:  (896, 5313)\n",
      "Sample HG00096 chr1_450740_451678 haplotypes predictions have been saved.\n",
      "Sample HG00096 chr1_450740_451678 haplotypes predictions have been logged.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO: 07/24/2023 03:38:37 PM] [CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2)]\n",
      "[INFO: 07/24/2023 03:38:37 PM] [CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2)]\n",
      "[INFO: 07/24/2023 03:38:37 PM] [CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2)]\n",
      "[INFO: 07/24/2023 03:38:37 PM] [CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2)]\n",
      "[INFO: 07/24/2023 03:38:37 PM] [CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2)]\n",
      "[INFO: 07/24/2023 03:38:37 PM] [CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2)]\n",
      "[INFO: 07/24/2023 03:38:37 PM] [CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2)]\n",
      "[INFO: 07/24/2023 03:38:37 PM] [CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2)]\n",
      "INFO:cache_log:[CACHE] (model) at batch 0: [CacheInfo(hits=1, misses=2, maxsize=5, currsize=2)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample HG00097 chr1_450740_451678 haplotype1 predictions are of the correct shape:  (896, 5313)\n",
      "Sample HG00097 chr1_450740_451678 haplotype2 predictions are of the correct shape:  (896, 5313)\n",
      "Sample HG00097 chr1_450740_451678 haplotypes predictions have been saved.\n",
      "Sample HG00097 chr1_450740_451678 haplotypes predictions have been logged.\n",
      "[INFO] (time) to predict on batch 0 is 1503.2652588659985\n",
      "INFO - Finished predictions for: [[0, 0, 0, 0]] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING: 07/24/2023 03:38:38 PM] WARNING - For HG00096, either all predictions don't match all logged queries in the interval list files minus excluded regions or vice versa. This can happen if you have supplied a list of intervals but have chosen to predict on a subset. If this is the case, this behavior is normal. If you are unsure, please re-run the enformer prediction pipeline with the same parameters. You may supply a csv file of regions to exclude if available, but this should not matter.\n",
      "[WARNING: 07/24/2023 03:38:38 PM] WARNING - For HG00096, either all predictions don't match all logged queries in the interval list files minus excluded regions or vice versa. This can happen if you have supplied a list of intervals but have chosen to predict on a subset. If this is the case, this behavior is normal. If you are unsure, please re-run the enformer prediction pipeline with the same parameters. You may supply a csv file of regions to exclude if available, but this should not matter.\n",
      "WARNING:summary_log:WARNING - For HG00096, either all predictions don't match all logged queries in the interval list files minus excluded regions or vice versa. This can happen if you have supplied a list of intervals but have chosen to predict on a subset. If this is the case, this behavior is normal. If you are unsure, please re-run the enformer prediction pipeline with the same parameters. You may supply a csv file of regions to exclude if available, but this should not matter.\n",
      "[WARNING: 07/24/2023 03:38:38 PM] WARNING - For HG00097, either all predictions don't match all logged queries in the interval list files minus excluded regions or vice versa. This can happen if you have supplied a list of intervals but have chosen to predict on a subset. If this is the case, this behavior is normal. If you are unsure, please re-run the enformer prediction pipeline with the same parameters. You may supply a csv file of regions to exclude if available, but this should not matter.\n",
      "[WARNING: 07/24/2023 03:38:38 PM] WARNING - For HG00097, either all predictions don't match all logged queries in the interval list files minus excluded regions or vice versa. This can happen if you have supplied a list of intervals but have chosen to predict on a subset. If this is the case, this behavior is normal. If you are unsure, please re-run the enformer prediction pipeline with the same parameters. You may supply a csv file of regions to exclude if available, but this should not matter.\n",
      "[WARNING: 07/24/2023 03:38:38 PM] WARNING - For HG00097, either all predictions don't match all logged queries in the interval list files minus excluded regions or vice versa. This can happen if you have supplied a list of intervals but have chosen to predict on a subset. If this is the case, this behavior is normal. If you are unsure, please re-run the enformer prediction pipeline with the same parameters. You may supply a csv file of regions to exclude if available, but this should not matter.\n",
      "WARNING:summary_log:WARNING - For HG00097, either all predictions don't match all logged queries in the interval list files minus excluded regions or vice versa. This can happen if you have supplied a list of intervals but have chosen to predict on a subset. If this is the case, this behavior is normal. If you are unsure, please re-run the enformer prediction pipeline with the same parameters. You may supply a csv file of regions to exclude if available, but this should not matter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Check /Users/sabrinami/Desktop/2022-23/tutorials/enformer_pipeline_test/predictions_folder/personalized_enformer_minimal_some_regions/predictions_2023-07-24/job_logs/personalized_enformer_minimal_some_regions_2023-07-24.summary for a summary of the entire run.\n",
      "INFO - Check `/Users/sabrinami/Desktop/2022-23/tutorials/enformer_pipeline/metadata` for successful and unsucessful predictions.\n",
      "INFO - Writing `aggregation_config_personalized_enformer_minimal_some_regions.json` file to /Users/sabrinami/Desktop/2022-23/tutorials/enformer_pipeline/metadata\n"
     ]
    }
   ],
   "source": [
    "enformer_predict('run_local_personalized.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups and Datasets in the HDF5 file:\n",
      "chr1_65419_71585\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "file_path=\"/Users/sabrinami/Desktop/2022-23/tutorials/enformer_pipeline_test/predictions_folder/personalized_enformer_minimal_some_regions/predictions_2023-07-24/enformer_predictions/HG00096/haplotype1/chr1_65419_71585_predictions.h5\"\n",
    "with h5py.File(file_path, \"r\") as file:\n",
    "    # List all the groups and datasets in the file\n",
    "    print(\"Groups and Datasets in the HDF5 file:\")\n",
    "    for name in file:\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(896, 5313)\n",
      "[[0.5291589  0.5204935  0.57351285 ... 0.00935043 0.01473221 0.01198051]\n",
      " [0.65397567 0.6207656  0.7431343  ... 0.01434602 0.02783546 0.02128439]\n",
      " [0.5326947  0.5510576  0.767584   ... 0.01038613 0.03195343 0.0286333 ]\n",
      " ...\n",
      " [0.00713983 0.0077101  0.00634603 ... 0.00115762 0.01039891 0.00428226]\n",
      " [0.00585703 0.00676019 0.00490488 ... 0.0047263  0.0377146  0.01976843]\n",
      " [0.00355603 0.00391064 0.00305343 ... 0.00085077 0.00762693 0.00313292]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(file_path, \"r\") as file:\n",
    "    # Step 2: Access datasets and attributes within the file\n",
    "    dataset_name = \"chr1_65419_71585\"\n",
    "    dataset = file[dataset_name]\n",
    "\n",
    "    # Example: Read the entire dataset into a NumPy array\n",
    "    data = dataset[()]\n",
    "print(data.shape)\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Code Snippets for Borzoi Personalized Prediction\n",
    "author: Sabrina Mi\n",
    "date: 9/27/2023\n",
    "execute:\n",
    "  include: false\n",
    "---\n",
    "\n",
    "## Step 1: VCF to One-hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cyvcf2\n",
    "import pysam\n",
    "import h5py\n",
    "import os\n",
    "from borzoi_helpers import process_sequence, predict_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/home/s1mi/Github/deep-learning-in-genomics/posts/2023-08-31-Br-personalized-prediction-on-more-genes/gene_list.txt\", \"r\") as file:\n",
    "    rn7_gene_list = file.read().splitlines()\n",
    "rn7_hg38_ortho = pd.read_csv(\"/home/s1mi/enformer_rat_data/annotation/rn7_hg38.ortholog_genes.txt\", sep=\"\\t\", index_col=\"ensembl_gene_id\")\n",
    "hg38_annot = pd.read_csv(\"/home/s1mi/enformer_rat_data/annotation/hg38.gene.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho_genes = list(set(rn7_gene_list).intersection(rn7_hg38_ortho.index))\n",
    "## convert to hg38\n",
    "gene_df = rn7_hg38_ortho.loc[ortho_genes]\n",
    "## annotate hg38 genes\n",
    "gene_df = gene_df.merge(hg38_annot, left_on=\"hsapiens_homolog_ensembl_gene\", right_on=\"ensembl_gene_id\", how=\"inner\")\n",
    "gene_df = gene_df[[\"ensembl_gene_id\", \"chromosome_name\", \"transcript_start\", \"transcript_end\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_variants_in_vcf_file(cyvcf2_object, interval_object, samples, mode=\"phased\"):\n",
    "    start = max(interval_object['start'], 0)\n",
    "    query = f\"{interval_object['chr']}:{start}-{interval_object['end']}\"\n",
    "    variants_dictionary = {}\n",
    "    variants_dictionary['chr'] = interval_object['chr']\n",
    "    variants_dictionary['positions'] = tuple(variant.POS for variant in cyvcf2_object(query))\n",
    "    if mode == 'phased':\n",
    "        delim = '|'\n",
    "    elif mode == 'unphased':\n",
    "        delim = '/'\n",
    "    for i, sample in enumerate(samples):\n",
    "        if sample in cyvcf2_object.samples:\n",
    "            variants_dictionary[sample] = tuple([variant.genotypes[i][0:2], variant.gt_bases[i].split(delim)] for variant in cyvcf2_object(query))\n",
    "    return variants_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mapping_dictionary(variants_array, samples, interval_start):\n",
    "    import numpy as np\n",
    "    A = np.array([1,0,0,0], dtype=np.float32)\n",
    "    C = np.array([0,1,0,0], dtype=np.float32)\n",
    "    G = np.array([0,0,1,0], dtype=np.float32)\n",
    "    T = np.array([0,0,0,1], dtype=np.float32)\n",
    "    seq_dict = {'A': A, 'C': C, 'G': G, 'T': T}\n",
    "\n",
    "    # collect common information\n",
    "    samples_haplotype_map = {}\n",
    "    samples_haplotype_map['positions'] = tuple((variants_array['positions'][i]) - interval_start for i in range(len(variants_array['positions'])))\n",
    "    for sample in samples:\n",
    "        samples_haplotype_map[sample] = {}\n",
    "        samples_haplotype_map[sample]['haplotype1'] = tuple(seq_dict[variants_array[sample][i][1][0]] for i in range(0, len(variants_array[sample])))\n",
    "        samples_haplotype_map[sample]['haplotype2'] = tuple(seq_dict[variants_array[sample][i][1][1]] for i in range(0, len(variants_array[sample])))\n",
    "    return samples_haplotype_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(region, seq_len=524288):\n",
    "    center_bp = (region['end'] + region['start']) // 2\n",
    "    start = center_bp - seq_len // 2\n",
    "    end = center_bp + seq_len // 2\n",
    "    return {\"chr\": region['chr'], \"start\": start, \"end\": end}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_variants_in_reference_sequence(query_sequences_encoded, mapping_dict, samples):\n",
    "    import copy\n",
    "    import numpy as np\n",
    "    positions = mapping_dict['positions']\n",
    "    variant_encoded = {}\n",
    "    for sample in samples:\n",
    "        haplotype1_encoded = np.copy(query_sequences_encoded)\n",
    "        haplotype2_encoded = np.copy(query_sequences_encoded)\n",
    "        for i, position in enumerate(positions):\n",
    "            haplotype1_encoded[position] = mapping_dict[sample][\"haplotype1\"][i]\n",
    "            haplotype2_encoded[position] = mapping_dict[sample][\"haplotype2\"][i]\n",
    "        variant_encoded[sample] = {\"haplotype1\": haplotype1_encoded, \"haplotype2\": haplotype2_encoded}\n",
    "    return variant_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_dir):\n",
    "    import os\n",
    "    import h5py\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "\n",
    "    import baskerville\n",
    "    from baskerville import seqnn\n",
    "    from baskerville import dna\n",
    "    from baskerville import gene as bgene\n",
    "\n",
    "    import json\n",
    "\n",
    "    import pysam\n",
    "    params_file = os.path.join(model_dir, 'params_pred.json') \n",
    "    targets_file = os.path.join(model_dir, 'targets_human.txt') \n",
    "\n",
    "    n_folds = 4       #To use only one model fold, change to 'n_folds = 1'\n",
    "    rc = True         #Average across reverse-complement prediction\n",
    "    #Read model parameters\n",
    "\n",
    "    with open(params_file) as params_open :\n",
    "        \n",
    "        params = json.load(params_open)\n",
    "        \n",
    "        params_model = params['model']\n",
    "\n",
    "    #Read targets\n",
    "\n",
    "    targets_df = pd.read_csv(targets_file, index_col=0, sep='\\t')\n",
    "    target_index = targets_df.index\n",
    "\n",
    "    #Create local index of strand_pair (relative to sliced targets)\n",
    "    if rc :\n",
    "        strand_pair = targets_df.strand_pair\n",
    "        \n",
    "        target_slice_dict = {ix : i for i, ix in enumerate(target_index.values.tolist())}\n",
    "        slice_pair = np.array([\n",
    "            target_slice_dict[ix] if ix in target_slice_dict else ix for ix in strand_pair.values.tolist()\n",
    "        ], dtype='int32')\n",
    "\n",
    "    #Initialize model ensemble\n",
    "\n",
    "    models = []\n",
    "    for fold_ix in range(n_folds) :\n",
    "        \n",
    "        model_file = os.path.join(model_dir, \"saved_models/f\" + str(fold_ix) + \"/model0_best.h5\")\n",
    "\n",
    "        seqnn_model = seqnn.SeqNN(params_model)\n",
    "        seqnn_model.restore(model_file, 0)\n",
    "        seqnn_model.build_slice(target_index)\n",
    "        if rc :\n",
    "            seqnn_model.strand_pair.append(slice_pair)\n",
    "        seqnn_model.build_ensemble(rc, '0')\n",
    "        \n",
    "        models.append(seqnn_model)\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_sequence(models, sample_input):\n",
    "    prediction_output = {}\n",
    "    for haplotype, sequence_encoding in sample_input.items():\n",
    "        prediction = predict_tracks(models, sequence_encoding)\n",
    "        prediction_output[haplotype] = prediction\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_open = pysam.Fastafile('/home/s1mi/borzoi_tutorial/hg38.fa')\n",
    "model_dir = '/home/s1mi/borzoi_tutorial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enformer_predict_on_region(target_interval, samples, path_to_vcf,  output_dir):\n",
    "    models = get_model(model_dir)\n",
    "    sequence_one_hot_ref = process_sequence(fasta_open, target_interval[\"chr\"], target_interval[\"start\"], target_interval[\"end\"])\n",
    "    vcf_chr = cyvcf2.cyvcf2.VCF(path_to_vcf, samples=samples)\n",
    "    variants_array = find_variants_in_vcf_file(vcf_chr, target_interval, samples, mode=\"phased\")\n",
    "    mapping_dict = create_mapping_dictionary(variants_array, samples, target_interval[\"start\"])\n",
    "    samples_variants_encoded = replace_variants_in_reference_sequence(sequence_one_hot_ref, mapping_dict, samples)\n",
    "    for sample in samples:\n",
    "        sample_input = samples_variants_encoded[sample]\n",
    "        sample_predictions = predict_on_sequence(models, sample_input)\n",
    "        sample_output = {}\n",
    "        output_path = os.path.join(output_dir, sample, f'{target_interval[\"chr\"]}_{target_interval[\"start\"]}_{target_interval[\"end\"]}_predictions.h5')\n",
    "        if not os.path.exists(output_path): os.makedirs(output_path, exist_ok=True)\n",
    "        with h5py.File(output_path, \"w\") as hf:\n",
    "            for hap in sample_predictions.keys():\n",
    "                sample_output[hap]= np.squeeze(sample_predictions[hap], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n"
     ]
    }
   ],
   "source": [
    "gene_annot = gene_df.iloc[0]\n",
    "samples = [\"NA21143\", \"NA21144\"]\n",
    "vcf_dir = \"/grand/TFXcan/imlab/data/1000G/vcf_snps_only\"\n",
    "output_dir = \"/home/s1mi/Github/deep-learning-in-genomics/posts/2023-09-26-borzoi-personalized-test/\"\n",
    "interval_object = {'chr': 'chr' + gene_annot[\"chromosome_name\"], 'start': gene_annot[\"transcript_start\"], 'end': gene_annot[\"transcript_end\"]}\n",
    "target_interval = resize(interval_object)\n",
    "path_to_vcf = os.path.join(vcf_dir, f\"ALL.{interval_object['chr']}.shapeit2_integrated_SNPs_v2a_27022019.GRCh38.phased.vcf.gz\")\n",
    "enformer_predict_on_region(interval_object, samples, path_to_vcf, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "borzoi",
   "language": "python",
   "name": "borzoi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "hash": "00361ff1b0e55517fcdf5eb39d73d487",
  "result": {
    "markdown": "---\ntitle: Week 1\ndescription: 'This week we learned the basics of VS Code, Python, and NumPy.'\nauthor: Sabrina Mi\ndate: 6/23/2023\n---\n\nHere's my notes from the [w3 NumPy tutorial](https://www.w3schools.com/python/numpy/numpy_intro.asp):\n\n## Data Types in Numpy:\n\nBy default, Python has strings, integer, float, boolean, and complex. NumPy has extra data types:\n\n* i: integer\n* b: boolean\n* u: unsigned integer\n* f: float\n* c: complex float\n* m: timedelta\n* M: datetime\n* O: object\n* S: string\n* U: unicode string\n* V: void\n\n# PyTorch Notes\n\nPyTorch is a machine learning framework leveraging two key features:\n\n* Tensor computing on GPUs\n* Deep neural networks on an automatic differentiation engine\n\nDeep learning softwares including Enformer are built on top of PyTorch.\n\nWe covered the basics of the pytorch library in Python, starting with [Tensors](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html). Tensors function very similarly to NumPy arrays, but the operations can be run on GPUs. As a result, the tensor operations have counterparts in the numpy library.\n\nWe'll only write about new concepts:\n\n### Tensors\n\nBy default, tensors are intialized on CPU. To move a tensor from CPU to GPU, we run:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport numpy as np\n\ndata = [[1, 2], [3, 4]]\nx_data = torch.tensor(data)\nprint(x_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[1, 2],\n        [3, 4]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nif torch.cuda.is_available():\n  tensor = tensor.to('cuda')\n  print(f\"Device tensor is stored on: {tensor.device}\")\n```\n:::\n\n\nTensors on the CPU and NumPy arrays can share their underlying memory\nlocations, and changing one will change\tthe other.\n\nBelow we convert a torch tensor to numpy array and add 1 to every element.\n\n```python\nt = torch.ones(5)\nprint(f\"t: {t}\")\nn = t.numpy()\nprint(f\"n: {n}\")\n\n```\n\n```python\nt.add_(1)\nprint(f\"t: {t}\")\nprint(f\"n: {n}\")\n\n```\n\n### Neural Networks\n\n\nWe use `torch.nn` and `torch.autograd` packages to develop neural networks.\n\nWe learned how to compute gradients for backward propagation with `torch.autograd` automatic differentiation. \n\nWe can `torch.nn` to first define a neural network in a `forward` function. Next, we chose a loss function, `nn.MSELoss` and run back prop as before. We update the weights with SGD update rule:\n\n```\nweight = weight - learning_rate * gradient\n\n```\n\n`torch.optim` includes various update rules that take parameters and learning rate as input.\n\n\nWe summarize the typical training procedure for a neural network:\n\n- Define the neural network that has some learnable parameters (or\n  weights)\n- Iterate over a dataset of inputs\n- Process input through the network\n- Compute the loss (how far is the output from being correct)\n- Propagate gradients back into the networkâ€™s parameters\n- Update the weights of the network, typically using a simple update rule:\n  ``weight = weight - learning_rate * gradient``\n\nLastly, we looked at training on the CIFAR-10 dataset. The process involved the same steps as before, but with a more complex network and more iterations in the training process. We also tested the model.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
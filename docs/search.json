[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/llm-in-molecular-biology-article/index.html",
    "href": "posts/llm-in-molecular-biology-article/index.html",
    "title": "Overview of LLM in Molecular Biology article by Serafim Batzoglou",
    "section": "",
    "text": "Read the paper here"
  },
  {
    "objectID": "posts/llm-in-molecular-biology-article/index.html#introduction-to-large-language-models",
    "href": "posts/llm-in-molecular-biology-article/index.html#introduction-to-large-language-models",
    "title": "Overview of LLM in Molecular Biology article by Serafim Batzoglou",
    "section": "Introduction to Large Language Models",
    "text": "Introduction to Large Language Models\n\nLarge language models like GPT-3 are trained on extensive text data and excel at generating coherent and contextually relevant text.\nThese models can be adapted for molecular biology tasks by encoding molecular information and training them on relevant datasets."
  },
  {
    "objectID": "posts/llm-in-molecular-biology-article/index.html#protein-structure-prediction",
    "href": "posts/llm-in-molecular-biology-article/index.html#protein-structure-prediction",
    "title": "Overview of LLM in Molecular Biology article by Serafim Batzoglou",
    "section": "Protein Structure Prediction",
    "text": "Protein Structure Prediction\n\nLarge language models can assist in predicting protein structures from amino acid sequences.\nLanguage models can generate plausible protein structures, aiding in protein folding predictions and protein engineering.\nChallenges include the vast conformational space and the need for high-quality training data to improve accuracy."
  },
  {
    "objectID": "posts/llm-in-molecular-biology-article/index.html#analyzing-dna-sequences",
    "href": "posts/llm-in-molecular-biology-article/index.html#analyzing-dna-sequences",
    "title": "Overview of LLM in Molecular Biology article by Serafim Batzoglou",
    "section": "Analyzing DNA Sequences",
    "text": "Analyzing DNA Sequences\n\nLanguage models can be trained to recognize patterns in DNA sequences and identify regulatory elements.\nThey can predict the effects of genetic mutations, aiding in understanding genetic diseases and personalized medicine.\nLarge language models have the potential to improve upon existing methods for DNA sequence analysis."
  },
  {
    "objectID": "posts/llm-in-molecular-biology-article/index.html#molecular-design",
    "href": "posts/llm-in-molecular-biology-article/index.html#molecular-design",
    "title": "Overview of LLM in Molecular Biology article by Serafim Batzoglou",
    "section": "Molecular Design",
    "text": "Molecular Design\n\nLanguage models can generate novel molecules with desired properties, contributing to drug discovery and material science.\nThey can suggest potential compounds based on specified criteria, accelerating the search for new drugs.\nLarge language models in molecular design can assist in exploring the chemical space and generating molecules that were not previously considered."
  },
  {
    "objectID": "posts/llm-in-molecular-biology-article/index.html#limitations-and-challenges",
    "href": "posts/llm-in-molecular-biology-article/index.html#limitations-and-challenges",
    "title": "Overview of LLM in Molecular Biology article by Serafim Batzoglou",
    "section": "Limitations and Challenges",
    "text": "Limitations and Challenges\n\nObtaining high-quality training data for molecular biology tasks remains a challenge.\nInterpreting the outputs of language models and understanding the underlying biological mechanisms is essential.\nIncorporating domain-specific knowledge into language models is crucial for accurate and reliable results."
  },
  {
    "objectID": "posts/comparing-enformer-prediction-averages/comparing-averages.html",
    "href": "posts/comparing-enformer-prediction-averages/comparing-averages.html",
    "title": "Comparing Enformer Prediction Averages",
    "section": "",
    "text": "import tensorflow as tf\n# Make sure the GPU is enabled \nassert tf.config.list_physical_devices('GPU'), 'Start the colab kernel with GPU: Runtime -&gt; Change runtime type -&gt; GPU'\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\nimport tensorflow_hub as hub # for interacting with saved models and tensorflow hub\nimport joblib\nimport gzip # for manipulating compressed files\nimport kipoiseq # for manipulating fasta files\nfrom kipoiseq import Interval # same as above, really\nimport pyfaidx # to index our reference genome file\nimport pandas as pd # for manipulating dataframes\nimport numpy as np # for numerical computations\nimport matplotlib.pyplot as plt # for plotting\nimport matplotlib as mpl # for plotting\nimport seaborn as sns # for plotting\nimport pickle # for saving large objects\nimport os, sys # functions for interacting with the operating system\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n2023-07-20 21:21:07.035539: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-07-20 21:21:12.080852: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /soft/compilers/cudatoolkit/cuda-11.6.2/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-11.6.2/lib64:/soft/libraries/trt/TensorRT-8.4.3.1.Linux.x86_64-gnu.cuda-11.6.cudnn8.4/lib:/soft/libraries/nccl/nccl_2.14.3-1+cuda11.6_x86_64/lib:/soft/libraries/cudnn/cudnn-11.6-linux-x64-v8.4.1.50/lib:/opt/cray/pe/gcc/11.2.0/snos/lib64:/opt/cray/pe/papi/6.0.0.14/lib64:/opt/cray/libfabric/1.11.0.4.125/lib64:/dbhome/db2cat/sqllib/lib64:/dbhome/db2cat/sqllib/lib64/gskit:/dbhome/db2cat/sqllib/lib32\n2023-07-20 21:21:12.083590: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /soft/compilers/cudatoolkit/cuda-11.6.2/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-11.6.2/lib64:/soft/libraries/trt/TensorRT-8.4.3.1.Linux.x86_64-gnu.cuda-11.6.cudnn8.4/lib:/soft/libraries/nccl/nccl_2.14.3-1+cuda11.6_x86_64/lib:/soft/libraries/cudnn/cudnn-11.6-linux-x64-v8.4.1.50/lib:/opt/cray/pe/gcc/11.2.0/snos/lib64:/opt/cray/pe/papi/6.0.0.14/lib64:/opt/cray/libfabric/1.11.0.4.125/lib64:/dbhome/db2cat/sqllib/lib64:/dbhome/db2cat/sqllib/lib64/gskit:/dbhome/db2cat/sqllib/lib32\n2023-07-20 21:21:12.083611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n\n\nNum GPUs Available:  1"
  },
  {
    "objectID": "posts/comparing-enformer-prediction-averages/comparing-averages.html#import-libraries",
    "href": "posts/comparing-enformer-prediction-averages/comparing-averages.html#import-libraries",
    "title": "Comparing Enformer Prediction Averages",
    "section": "",
    "text": "import tensorflow as tf\n# Make sure the GPU is enabled \nassert tf.config.list_physical_devices('GPU'), 'Start the colab kernel with GPU: Runtime -&gt; Change runtime type -&gt; GPU'\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\nimport tensorflow_hub as hub # for interacting with saved models and tensorflow hub\nimport joblib\nimport gzip # for manipulating compressed files\nimport kipoiseq # for manipulating fasta files\nfrom kipoiseq import Interval # same as above, really\nimport pyfaidx # to index our reference genome file\nimport pandas as pd # for manipulating dataframes\nimport numpy as np # for numerical computations\nimport matplotlib.pyplot as plt # for plotting\nimport matplotlib as mpl # for plotting\nimport seaborn as sns # for plotting\nimport pickle # for saving large objects\nimport os, sys # functions for interacting with the operating system\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n2023-07-20 21:21:07.035539: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-07-20 21:21:12.080852: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /soft/compilers/cudatoolkit/cuda-11.6.2/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-11.6.2/lib64:/soft/libraries/trt/TensorRT-8.4.3.1.Linux.x86_64-gnu.cuda-11.6.cudnn8.4/lib:/soft/libraries/nccl/nccl_2.14.3-1+cuda11.6_x86_64/lib:/soft/libraries/cudnn/cudnn-11.6-linux-x64-v8.4.1.50/lib:/opt/cray/pe/gcc/11.2.0/snos/lib64:/opt/cray/pe/papi/6.0.0.14/lib64:/opt/cray/libfabric/1.11.0.4.125/lib64:/dbhome/db2cat/sqllib/lib64:/dbhome/db2cat/sqllib/lib64/gskit:/dbhome/db2cat/sqllib/lib32\n2023-07-20 21:21:12.083590: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /soft/compilers/cudatoolkit/cuda-11.6.2/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-11.6.2/lib64:/soft/libraries/trt/TensorRT-8.4.3.1.Linux.x86_64-gnu.cuda-11.6.cudnn8.4/lib:/soft/libraries/nccl/nccl_2.14.3-1+cuda11.6_x86_64/lib:/soft/libraries/cudnn/cudnn-11.6-linux-x64-v8.4.1.50/lib:/opt/cray/pe/gcc/11.2.0/snos/lib64:/opt/cray/pe/papi/6.0.0.14/lib64:/opt/cray/libfabric/1.11.0.4.125/lib64:/dbhome/db2cat/sqllib/lib64:/dbhome/db2cat/sqllib/lib64/gskit:/dbhome/db2cat/sqllib/lib32\n2023-07-20 21:21:12.083611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n\n\nNum GPUs Available:  1"
  },
  {
    "objectID": "posts/comparing-enformer-prediction-averages/comparing-averages.html#define-paths",
    "href": "posts/comparing-enformer-prediction-averages/comparing-averages.html#define-paths",
    "title": "Comparing Enformer Prediction Averages",
    "section": "Define Paths",
    "text": "Define Paths\n\ntransform_path = 'gs://dm-enformer/models/enformer.finetuned.SAD.robustscaler-PCA500-robustscaler.transform.pkl'\nmodel_path = 'https://tfhub.dev/deepmind/enformer/1'\nfasta_file = '/home/s1mi/enformer_tutorial/genome.fa'\ntargets_txt = 'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_human.txt'\ndf_targets = pd.read_csv(targets_txt, sep='\\t')"
  },
  {
    "objectID": "posts/comparing-enformer-prediction-averages/comparing-averages.html#define-functions",
    "href": "posts/comparing-enformer-prediction-averages/comparing-averages.html#define-functions",
    "title": "Comparing Enformer Prediction Averages",
    "section": "Define Functions",
    "text": "Define Functions\n\n# @title `Enformer`, `EnformerScoreVariantsNormalized`, `EnformerScoreVariantsPCANormalized`,\nSEQUENCE_LENGTH = 393216\n\nclass Enformer:\n\n  def __init__(self, tfhub_url):\n    self._model = hub.load(tfhub_url).model\n\n  def predict_on_batch(self, inputs):\n    predictions = self._model.predict_on_batch(inputs)\n    return {k: v.numpy() for k, v in predictions.items()}\n\n  @tf.function\n  def contribution_input_grad(self, input_sequence,\n                              target_mask, output_head='human'):\n    input_sequence = input_sequence[tf.newaxis]\n\n    target_mask_mass = tf.reduce_sum(target_mask)\n    with tf.GradientTape() as tape:\n      tape.watch(input_sequence)\n      prediction = tf.reduce_sum(\n          target_mask[tf.newaxis] *\n          self._model.predict_on_batch(input_sequence)[output_head]) / target_mask_mass\n\n    input_grad = tape.gradient(prediction, input_sequence) * input_sequence\n    input_grad = tf.squeeze(input_grad, axis=0)\n    return tf.reduce_sum(input_grad, axis=-1)\n\n\nclass EnformerScoreVariantsRaw:\n\n  def __init__(self, tfhub_url, organism='human'):\n    self._model = Enformer(tfhub_url)\n    self._organism = organism\n\n  def predict_on_batch(self, inputs):\n    ref_prediction = self._model.predict_on_batch(inputs['ref'])[self._organism]\n    alt_prediction = self._model.predict_on_batch(inputs['alt'])[self._organism]\n\n    return alt_prediction.mean(axis=1) - ref_prediction.mean(axis=1)\n\n\nclass EnformerScoreVariantsNormalized:\n\n  def __init__(self, tfhub_url, transform_pkl_path,\n               organism='human'):\n    assert organism == 'human', 'Transforms only compatible with organism=human'\n    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n      transform_pipeline = joblib.load(f)\n    self._transform = transform_pipeline.steps[0][1]  # StandardScaler.\n\n  def predict_on_batch(self, inputs):\n    scores = self._model.predict_on_batch(inputs)\n    return self._transform.transform(scores)\n\n\nclass EnformerScoreVariantsPCANormalized:\n\n  def __init__(self, tfhub_url, transform_pkl_path,\n               organism='human', num_top_features=500):\n    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n      self._transform = joblib.load(f)\n    self._num_top_features = num_top_features\n\n  def predict_on_batch(self, inputs):\n    scores = self._model.predict_on_batch(inputs)\n    return self._transform.transform(scores)[:, :self._num_top_features]\n\n\n# TODO(avsec): Add feature description: Either PCX, or full names.\n\n\n# @title `variant_centered_sequences`\n\nclass FastaStringExtractor:\n\n    def __init__(self, fasta_file):\n        self.fasta = pyfaidx.Fasta(fasta_file)\n        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n    #import pd.Interval as Interval\n    def extract(self, interval: Interval, **kwargs) -&gt; str:\n        # Truncate interval if it extends beyond the chromosome lengths.\n        chromosome_length = self._chromosome_sizes[interval.chrom]\n        trimmed_interval = Interval(interval.chrom,\n                                    max(interval.start, 0),\n                                    min(interval.end, chromosome_length),\n                                    )\n        # pyfaidx wants a 1-based interval\n        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n                                          trimmed_interval.start + 1,\n                                          trimmed_interval.stop).seq).upper()\n        # Fill truncated values with N's.\n        pad_upstream = 'N' * max(-interval.start, 0)\n        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n        return pad_upstream + sequence + pad_downstream\n\n    def close(self):\n        return self.fasta.close()\n\n\ndef one_hot_encode(sequence):\n  return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)\n\n\n\n# @title `plot_tracks`\n\ndef plot_tracks(tracks, interval, height=1.5):\n  fig, axes = plt.subplots(len(tracks), 1, figsize=(20, height * len(tracks)), sharex=True)\n  for ax, (title, y) in zip(axes, tracks.items()):\n    ax.fill_between(np.linspace(interval.start, interval.end, num=len(y)), y)\n    ax.set_title(title)\n    sns.despine(top=True, right=True, bottom=True)\n  ax.set_xlabel(str(interval))\n  plt.tight_layout()\n\n\nimport Bio\n\nfrom Bio.Seq import Seq\ndef create_rev_complement(dna_string):\n    return(str(Seq(dna_string).reverse_complement()))\n\n\ndef prepare_for_quantify_prediction_per_TSS(predictions, gene, tss_df):\n\n  '''\n\n  Parameters:\n          predicitions (A numpy array): All predictions from the track\n          gene (a gene name, character): a gene\n          tss_df: a list of dataframe of genes and their transcription start sites\n  Returns:\n          A dictionary of cage experiment predictions and a list of transcription start sites\n\n  '''\n\n  output = dict()\n  for tdf in tss_df:\n    if gene not in tdf.genes.values:\n      continue\n    gene_tss_list = tdf[tdf.genes == gene].txStart_Sites.apply(str).values\n    gene_tss_list = [t.split(', ') for t in gene_tss_list]\n    gene_tss_list = [int(item) for nestedlist in gene_tss_list for item in nestedlist]\n    gene_tss_list = list(set(gene_tss_list))\n  output['cage_predictions'] = predictions[:, 5110] # a numpy array\n  output['gene_TSS'] = gene_tss_list # a list\n\n\n  return(output) # a dictionary\n\ndef quantify_prediction_per_TSS(low_range, TSS, cage_predictions):\n\n  '''\n  Parameters:\n          low_range (int): The lower interval\n          TSS (list of integers): A list of TSS for a gene\n          cage_predictions: A 1D numpy array or a vector of predictions from enformer corresponding to track 5110 or CAGE predictions\n  Returns:\n          A dictionary of gene expression predictions for each TSS for a gene\n    '''\n  tss_predictions = dict()\n  for tss in TSS:\n    bin_start = low_range + ((768 + 320) * 128)\n    count = -1\n    while bin_start &lt; tss:\n      bin_start = bin_start + 128\n      count += 1\n    if count &gt;= len(cage_predictions)-1:\n      continue\n    cage_preds = cage_predictions[count - 1] + cage_predictions[count] + cage_predictions[count + 1]\n    tss_predictions[tss] = cage_preds\n\n  return(tss_predictions)\n\ndef collect_intervals(chromosomes = [\"22\"], gene_list=None):\n\n  '''\n    Parameters :\n      chromosomes : a list of chromosome numbers; each element should be a string format\n      gene_list : a list of genes; the genes should be located on those chromosomes\n\n    Returns :\n      A dictionary of genes (from gene_list) and their intervals within their respective chromosomes\n  '''\n\n  gene_intervals = {} # Collect intervals for our genes of interest\n\n  for chrom in chromosomes:\n    with open(\"/home/s1mi/enformer_tutorial/gene_chroms/gene_\"+ chrom + \".txt\", \"r\") as chrom_genes:\n      for line in chrom_genes:\n        split_line = line.strip().split(\"\\t\")\n        gene_intervals[split_line[2]] = [\n                                          split_line[0],\n                                          int(split_line[3]),\n                                          int(split_line[4])\n                                        ]\n\n  if isinstance(gene_list, list): # if the user has supplied a list of genes they are interested in\n    use_genes = dict((k, gene_intervals[k]) for k in gene_list if k in gene_intervals)\n    return(use_genes)\n  elif isinstance(gene_list, type(None)):\n    return(gene_intervals)\n\n\ndef run_predictions(gene_intervals, tss_dataframe, individuals_list=None):\n  '''\n  Parameters :\n    gene_intervals : the results from calling `collect_intervals`\n    tss_dataframe : a list of the TSSs dataframes i.e. the TSS for the genes in the chromosomes\n    individuals_list : a list of individuals on which we want to make predictions; defaults to None\n\n  Returns :\n    A list of predictions; the first element is the predictions around the TSS for each gene. The second is the prediction across CAGE tracks\n  '''\n\n  gene_output = dict()\n  gene_predictions = dict()\n\n  for gene in gene_intervals.keys():\n    gene_interval = gene_intervals[gene]\n    target_interval = kipoiseq.Interval(\"chr\" + gene_interval[0],\n                                        gene_interval[1],\n                                        gene_interval[2]) # creates an interval to select the right sequences\n    target_fa = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))  # extracts the fasta sequences, and resizes such that it is compatible with the sequence_length\n    window_coords = target_interval.resize(SEQUENCE_LENGTH) # we also need information about the start and end locations after resizing\n    try:\n      cur_gene_vars = pd.read_csv(\"/home/s1mi/enformer_tutorial/individual_beds/chr\" + gene_interval[0] + \"/chr\" + gene_interval[0] + \"_\"+ gene + \".bed\", sep=\"\\t\", header=0) # read in the appropriate bed file for the gene\n    except:\n      continue\n    individual_results = dict()\n    individual_prediction = dict()\n\n    if isinstance(individuals_list, list) or isinstance(individuals_list, type(np.empty([1, 1]))):\n      use_individuals = individuals_list\n    elif isinstance(individuals_list, type(None)):\n      use_individuals = cur_gene_vars.columns[4:]\n\n    for individual in use_individuals:\n      print('Currently on gene {}, and predicting on individual {}...'.format(gene, individual))\n      # two haplotypes per individual\n      haplo_1 = list(target_fa[:])\n      haplo_2 = list(target_fa[:])\n\n      ref_mismatch_count = 0\n      for i,row in cur_gene_vars.iterrows():\n\n        geno = row[individual].split(\"|\")\n        if (row[\"POS\"]-window_coords.start-1) &gt;= len(haplo_2):\n          continue\n        if (row[\"POS\"]-window_coords.start-1) &lt; 0:\n          continue\n        if geno[0] == \"1\":\n          haplo_1[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n        if geno[1] == \"1\":\n          haplo_2[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n\n      # predict on the individual's two haplotypes\n      prediction_1 = model.predict_on_batch(one_hot_encode(\"\".join(haplo_1))[np.newaxis])['human'][0]\n      prediction_2 = model.predict_on_batch(one_hot_encode(\"\".join(haplo_2))[np.newaxis])['human'][0]\n\n      temp_predictions = [prediction_1[:, 5110], prediction_2[:, 5110]] # CAGE predictions we are interested in\n      individual_prediction[individual] = temp_predictions\n\n      # Calculate TSS CAGE expression which correspond to column 5110 of the predictions above\n      temp_list = list()\n\n      pred_prepared_1 = prepare_for_quantify_prediction_per_TSS(predictions=prediction_1, gene=gene, tss_df=tss_dataframe)\n      tss_predictions_1 = quantify_prediction_per_TSS(low_range = window_coords.start, TSS=pred_prepared_1['gene_TSS'], cage_predictions=pred_prepared_1['cage_predictions'])\n\n      pred_prepared_2 = prepare_for_quantify_prediction_per_TSS(predictions=prediction_2, gene=gene, tss_df=tss_dataframe)\n      tss_predictions_2 = quantify_prediction_per_TSS(low_range = window_coords.start, TSS=pred_prepared_2['gene_TSS'], cage_predictions=pred_prepared_2['cage_predictions'])\n\n      temp_list.append(tss_predictions_1)\n      temp_list.append(tss_predictions_2) # results here are a dictionary for each TSS for each haplotype\n\n      individual_results[individual] = temp_list # save for the individual\n\n    gene_output[gene] = individual_results\n    gene_predictions[gene] = individual_prediction\n\n  return([gene_output, gene_predictions])\n\n\ndef collect_target_intervals(gene_intervals):\n\n  '''\n  Returns a dictionary of Interval objects (from kipoiseq) for each gene corresponding to the locations of the gene\n  '''\n\n  target_intervals_dict = dict()\n\n  for gene in gene_intervals.keys():\n    gene_interval = gene_intervals[gene]\n    target_interval = kipoiseq.Interval(\"chr\" + gene_interval[0],\n                                        gene_interval[1],\n                                        gene_interval[2])\n    target_intervals_dict[gene] = target_interval\n\n  return(target_intervals_dict)\n\ndef prepare_for_plot_tracks(gene, individual, all_predictions, chromosome=['22']):\n\n  '''\n  This returns a dictionary of gene tracks and gene intervals, prepared for the function plot_tracks.\n\n  Parameters:\n    - gene\n    - individual\n    - all_predictions\n  '''\n\n  haplo_predictions = all_predictions[gene][individual]\n  gene_tracks = {gene + ' | ' + individual + ' | haplotype 1': np.log10(1 + haplo_predictions[0]),\n                gene + ' | ' + individual + ' | haplotype 2': np.log10(1 + haplo_predictions[1])}\n\n  gene_intervals = collect_intervals(chromosomes=chromosome, gene_list=[gene])\n  gene_intervals = collect_target_intervals(gene_intervals)\n\n  output = dict()\n  output['gene_tracks'] = gene_tracks\n  output['gene_intervals'] = gene_intervals[gene]\n\n  return(output)\n\ndef check_individuals(path_to_bed_file, list_of_individuals):\n\n  '''\n  Checks if an individual is missing in bed variation files.\n  These individuals should be removed prior to training\n  '''\n\n  myfile = open(path_to_bed_file, 'r')\n  myline = myfile.readline()\n  bed_names = myline.split('\\t')[4:]\n  myfile.close()\n\n  if set(list_of_individuals).issubset(set(bed_names)) == False:\n    missing = list(set(list_of_individuals).difference(bed_names))\n    print('This (or these) individual(s) is/are not present: {}'.format(missing))\n  else:\n    missing = []\n    print('All individuals are present in the bed file.')\n\n  return(missing)\n\n\ndef geno_to_seq(gene, individual):\n      # two haplotypes per individual\n  haplo_1 = list(target_fa[:])\n  haplo_2 = list(target_fa[:])\n\n  ref_mismatch_count = 0\n  for i,row in cur_gene_vars.iterrows():\n\n    geno = row[individual].split(\"|\")\n    if (row[\"POS\"]-window_coords.start-1) &gt;= len(haplo_2):\n      continue\n    if (row[\"POS\"]-window_coords.start-1) &lt; 0:\n      continue\n    if geno[0] == \"1\":\n      haplo_1[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n    if geno[1] == \"1\":\n      haplo_2[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n  return haplo_1, haplo_2\n\n      # predict on the individual's two haplotypes"
  },
  {
    "objectID": "posts/comparing-enformer-prediction-averages/comparing-averages.html#prepare-input-data",
    "href": "posts/comparing-enformer-prediction-averages/comparing-averages.html#prepare-input-data",
    "title": "Comparing Enformer Prediction Averages",
    "section": "Prepare input data",
    "text": "Prepare input data\nWe want to predict epigenome around ERAP2 TSS on chromosome 5.\n\nchr5_tss = pd.read_table('/home/s1mi/enformer_tutorial/tss_by_chr/chr5_tss_by_gene.txt', sep='\\t')\nerap2_variations = pd.read_table('/home/s1mi/enformer_tutorial/individual_beds/chr5/chr5_ERAP2.bed', sep='\\t')\ngeuvadis_gene_expression = pd.read_table('https://uchicago.box.com/shared/static/5vwc7pjw9qmtv7298c4rc7bcuicoyemt.gz', sep='\\t',\n                                         dtype={'gene_id': str, 'gene_name':str, 'TargetID':str, 'Chr':str})\ngeuvadis_gene_expression.head(5)\n\n\n\n\n\n\n\n\ngene_id\ngene_name\nTargetID\nChr\nCoord\nHG00096\nHG00097\nHG00099\nHG00100\nHG00101\n...\nNA20810\nNA20811\nNA20812\nNA20813\nNA20814\nNA20815\nNA20816\nNA20819\nNA20826\nNA20828\n\n\n\n\n0\nENSG00000223972.4\nDDX11L1\nENSG00000223972.4\n1\n11869\n0.320818\n0.344202\n0.354225\n0.478064\n-0.102815\n...\n1.008605\n0.384489\n0.581284\n0.513981\n0.667449\n0.350890\n0.186103\n-0.037976\n0.405439\n0.199143\n\n\n1\nENSG00000227232.3\nWASH7P\nENSG00000227232.3\n1\n29806\n33.714457\n20.185174\n18.095407\n24.100871\n29.018719\n...\n30.980194\n34.086207\n39.678442\n29.643513\n27.120420\n29.121624\n31.117198\n32.047074\n22.798959\n23.563874\n\n\n2\nENSG00000243485.1\nMIR1302-11\nENSG00000243485.1\n1\n29554\n0.240408\n0.157456\n0.218806\n0.320878\n0.067833\n...\n0.065940\n0.228784\n0.140642\n0.283905\n0.273821\n0.286311\n0.324060\n0.049574\n0.255288\n0.157440\n\n\n3\nENSG00000238009.2\nRP11-34P13.7\nENSG00000238009.2\n1\n133566\n0.328272\n0.327932\n0.090064\n0.420443\n0.220269\n...\n0.274071\n0.384179\n0.533693\n0.307221\n0.307367\n0.400278\n0.612321\n0.666633\n0.281138\n1.346129\n\n\n4\nENSG00000239945.1\nRP11-34P13.8\nENSG00000239945.1\n1\n91105\n0.332171\n-0.032164\n0.017323\n0.424677\n0.214025\n...\n0.347323\n0.346744\n0.073580\n0.400396\n0.470517\n0.069749\n0.299353\n0.090019\n0.282554\n-0.157170\n\n\n\n\n5 rows × 467 columns\n\n\n\n\ngene_intervals = collect_intervals(chromosomes=['5'], gene_list=['ERAP2'])\n\n\nmodel = Enformer(model_path) # here we load the model architecture.\n\nfasta_extractor = FastaStringExtractor(fasta_file) # we define a class called fasta_extractor to help us extra raw sequence data"
  },
  {
    "objectID": "posts/comparing-enformer-prediction-averages/comparing-averages.html#run-predictions",
    "href": "posts/comparing-enformer-prediction-averages/comparing-averages.html#run-predictions",
    "title": "Comparing Enformer Prediction Averages",
    "section": "Run Predictions",
    "text": "Run Predictions\nWe’ll pick one individual at random.\n\nrand_individual = np.random.choice(a=geuvadis_gene_expression.columns[6:-1], replace=False) # individuals we are interested in\nrand_individual\n\n'HG00133'\n\n\n\ngene = 'ERAP2'\ngene_interval = gene_intervals[gene]\ntarget_interval = kipoiseq.Interval(\"chr\" + gene_interval[0],\n                                        gene_interval[1],\n                                        gene_interval[2])\ntarget_fa = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))\nwindow_coords = target_interval.resize(SEQUENCE_LENGTH)\ncur_gene_vars = pd.read_csv(\"/home/s1mi/enformer_tutorial/individual_beds/chr\" + gene_interval[0] + \"/chr\" + gene_interval[0] + \"_\"+ gene + \".bed\", sep=\"\\t\", header=0) # read in the appropriate bed file for the gene\n\n\nhaplo_1, haplo_2 = geno_to_seq('ERAP2', rand_individual)\n\nhaplo_1_enc = one_hot_encode(\"\".join(haplo_1))[np.newaxis]\nhaplo_2_enc = one_hot_encode(\"\".join(haplo_2))[np.newaxis]\naverage_enc = np.add(haplo_1_enc, haplo_2_enc) / 2\n\n\nprediction_1 = model.predict_on_batch(haplo_1_enc)['human'][0]\nprediction_2 = model.predict_on_batch(haplo_2_enc)['human'][0]\n\npre_average = model.predict_on_batch(average_enc)['human'][0]\npost_average = (prediction_1 + prediction_2) / 2\n\n\nprint(pre_average)\nprint(post_average)\n\n[[0.0983258  0.09131803 0.06897802 ... 0.00298976 0.00920826 0.01195592]\n [0.05844229 0.05120772 0.0393694  ... 0.00664862 0.02121062 0.02394622]\n [0.00556148 0.00465787 0.00350308 ... 0.00303111 0.00745877 0.00867325]\n ...\n [0.00023319 0.00015917 0.00017393 ... 0.00074748 0.00243068 0.0029231 ]\n [0.00312253 0.00261029 0.00281582 ... 0.00084431 0.0019378  0.00236547]\n [0.05091214 0.03922845 0.03701665 ... 0.00821806 0.03188424 0.02977208]]\n[[0.09836499 0.09140344 0.06912695 ... 0.00298798 0.00929599 0.01202009]\n [0.05851313 0.05130395 0.03947318 ... 0.00664719 0.02141304 0.02404698]\n [0.00558177 0.0046772  0.00351959 ... 0.00302755 0.00748987 0.00869058]\n ...\n [0.00024067 0.00016469 0.0001784  ... 0.00074248 0.00242248 0.00288824]\n [0.00312049 0.00261045 0.00281425 ... 0.00084645 0.00193864 0.00237052]\n [0.05081689 0.03920878 0.03703882 ... 0.00822351 0.03193999 0.02979648]]"
  },
  {
    "objectID": "posts/comparing-enformer-prediction-averages/comparing-averages.html#comparing-across-tracks",
    "href": "posts/comparing-enformer-prediction-averages/comparing-averages.html#comparing-across-tracks",
    "title": "Comparing Enformer Prediction Averages",
    "section": "Comparing across tracks",
    "text": "Comparing across tracks\nWe compute correlation for each track.\n\ncors = []\nfor i in range(5313):\n    pre_track = pre_average[:, i]\n    post_track = post_average[:, i]\n    cor = np.corrcoef(pre_track, post_track)[0][1]\n    cors.append(cor)\n\nThe results from both methods are nearly identical.\n\nprint(min(cors), max(cors))\n\n0.9927959179299012 0.9999999436970823\n\n\nNow we examine the outliers.\n\ndiff = pre_average - post_average\nprint(diff)\n\n[[-3.9197505e-05 -8.5413456e-05 -1.4892966e-04 ...  1.7832499e-06\n  -8.7730587e-05 -6.4171851e-05]\n [-7.0843846e-05 -9.6220523e-05 -1.0377541e-04 ...  1.4314428e-06\n  -2.0241179e-04 -1.0076165e-04]\n [-2.0299107e-05 -1.9335654e-05 -1.6507460e-05 ...  3.5623088e-06\n  -3.1102449e-05 -1.7336570e-05]\n ...\n [-7.4755080e-06 -5.5219280e-06 -4.4692715e-06 ...  4.9936934e-06\n   8.1981998e-06  3.4861965e-05]\n [ 2.0379666e-06 -1.6158447e-07  1.5720725e-06 ... -2.1370361e-06\n  -8.4110070e-07 -5.0480012e-06]\n [ 9.5248222e-05  1.9673258e-05 -2.2165477e-05 ... -5.4491684e-06\n  -5.5748969e-05 -2.4404377e-05]]\n\n\n\nmses = []\nfor i in range(5313):\n    mse = np.sum(np.square(diff[:, i]))\n    mses.append(mse)\n\nquartiles = np.percentile(mses, [25, 50, 75])\nprint(\"Min: \", min(mses))\nprint(\"Q1: \", quartiles[0]) \nprint(\"Median: \", quartiles[1])\nprint(\"Q3: \", quartiles[2])\nprint(\"Max: \", max(mses))\n\nMin:  2.8938662e-06\nQ1:  0.007410927675664425\nMedian:  0.02212701365351677\nQ3:  0.07311876863241196\nMax:  142.65538\n\n\n\nplt.boxplot(mses)\nplt.show()\n\n\n\n\n\ndesc_index = [i for i, x in sorted(enumerate(mses), key=lambda x: x[1], reverse=True)]\nprint(\"Tracks from highest to lowest MSE:\", desc_index)\n\nmse_outs = [x for x in mses if x &gt; 1]\nprint(\"Number of tracks with MSE &gt; 1:\", len(mse_outs))\n\nTracks from highest to lowest MSE: [4694, 4675, 5181, 4266, 4004, 5239, 4093, 5240, 5235, 4477, 5241, 1628, 5237, 3194, 1359, 5243, 706, 3824, 3602, 707, 3234, 3299, 5244, 4758, 5236, 5242, 2111, 1471, 3615, 2067, 3248, 752, 2028, 820, 3674, 705, 1882, 3387, 2047, 2062, 2126, 1348, 2665, 3200, 2640, 2492, 764, 1410, 3577, 4506, 3693, 853, 2150, 3855, 2888, 713, 3690, 766, 2696, 3744, 3264, 2155, 4416, 1470, 1966, 1783, 2833, 5238, 3889, 2280, 4078, 4421, 817, 1812, 2138, 4711, 1630, 2508, 4428, 4196, 4246, 2440, 1578, 3482, 1629, 1520, 4695, 676, 1489, 4177, 3075, 4192, 688, 2921, 1046, 3891, 3074, 2061, 2667, 2869, 1879, 2832, 3750, 2110, 790, 2049, 1094, 5110, 1382, 1843, 690, 3069, 4641, 3789, 3132, 1414, 1649, 2055, 3991, 3501, 2385, 4444, 3024, 1612, 2407, 907, 3713, 859, 691, 1383, 1100, 5217, 714, 2791, 743, 1324, 2353, 284, 4493, 4765, 4767, 4323, 2743, 5138, 1576, 2009, 2389, 5257, 3530, 4074, 4273, 4764, 1095, 1668, 1175, 2502, 3127, 4411, 1214, 3607, 3040, 2349, 1579, 2300, 2765, 1329, 1734, 2823, 1981, 4760, 3181, 680, 745, 767, 1535, 4351, 681, 5109, 3072, 1093, 4649, 2282, 2526, 4805, 1818, 719, 1023, 3469, 2000, 5215, 2602, 4036, 1988, 677, 4018, 3242, 3620, 3951, 2735, 857, 3475, 2618, 1463, 4739, 3659, 5256, 1965, 1789, 4200, 2586, 1563, 3042, 1692, 3478, 1982, 1877, 3363, 952, 953, 3665, 2734, 3843, 2201, 4054, 1724, 3947, 1602, 4642, 1340, 1179, 796, 1220, 800, 2568, 695, 1194, 3582, 5214, 3389, 2226, 819, 4005, 3686, 2185, 4013, 2844, 4640, 1189, 4268, 4007, 1956, 4163, 5212, 4806, 1092, 3433, 1330, 1840, 3893, 821, 2386, 1967, 4759, 3979, 1113, 4385, 2785, 4080, 4469, 159, 754, 3851, 1555, 1196, 2298, 1455, 3816, 4285, 3239, 1238, 4488, 4801, 3954, 1184, 788, 4092, 1973, 1561, 2329, 1117, 2122, 1608, 760, 4263, 744, 772, 5216, 4184, 1392, 3266, 5198, 3872, 770, 2188, 3773, 1143, 1487, 1310, 127, 1321, 1192, 683, 3392, 747, 4568, 4811, 3457, 2493, 1224, 2060, 1716, 2092, 4766, 3696, 1229, 1871, 2221, 1797, 1115, 836, 4134, 5124, 4158, 3859, 1835, 879, 4378, 1167, 2771, 2840, 1905, 2339, 5144, 1606, 3271, 2448, 4822, 822, 2928, 3708, 4147, 1478, 1545, 3912, 5276, 700, 3915, 3967, 3062, 3959, 3836, 3109, 2395, 4742, 878, 4726, 4461, 3700, 2215, 3899, 3805, 183, 675, 715, 1185, 3439, 2827, 1468, 3630, 5295, 3923, 1221, 2308, 2866, 808, 3107, 4712, 5123, 3653, 1106, 679, 2868, 2991, 4291, 727, 3185, 2056, 2104, 529, 981, 443, 2098, 1848, 3738, 2266, 5084, 3787, 2604, 755, 1419, 2210, 1459, 3522, 5211, 3298, 3394, 4804, 5210, 1136, 2522, 1045, 2571, 3919, 2694, 1864, 773, 682, 2304, 5067, 2933, 1451, 1239, 161, 1866, 3869, 4276, 2381, 4234, 1006, 848, 2214, 1740, 1689, 2930, 4504, 4388, 2856, 4908, 3176, 2305, 1536, 275, 3172, 2457, 4380, 2066, 2628, 4172, 1097, 57, 3175, 3307, 2219, 288, 4038, 3925, 4424, 3995, 4576, 1215, 4763, 1132, 3964, 696, 5129, 1754, 804, 1207, 2792, 1235, 763, 3149, 1109, 1831, 3552, 2919, 4738, 1534, 65, 2652, 3148, 2713, 850, 806, 2291, 516, 2654, 3028, 710, 4161, 5139, 1715, 2988, 3183, 3484, 3122, 3246, 905, 3649, 2778, 2380, 4513, 1311, 2271, 144, 961, 1617, 2819, 2074, 4235, 1388, 2974, 4372, 1169, 789, 1762, 5274, 1531, 2033, 4212, 3443, 5126, 4512, 1323, 4264, 3605, 2752, 858, 1527, 2039, 2530, 2740, 1320, 2830, 4771, 686, 824, 2821, 1805, 3736, 3314, 3664, 4282, 2451, 3663, 1441, 4776, 79, 2228, 3881, 1222, 1577, 4379, 4786, 4774, 190, 1709, 2318, 5082, 4799, 5213, 3994, 4024, 811, 2588, 3164, 2357, 3874, 4901, 3139, 2135, 4812, 3280, 5255, 4431, 3487, 832, 1111, 4418, 2725, 1747, 1308, 1758, 2119, 3331, 3302, 356, 1161, 4727, 3504, 4261, 5209, 792, 1809, 1493, 1351, 1062, 4205, 3566, 2737, 2019, 1171, 1085, 1682, 2141, 2593, 4144, 692, 1268, 3743, 1817, 4027, 1996, 2936, 4709, 3946, 1212, 1528, 1066, 3386, 1122, 2352, 3260, 3929, 2976, 1435, 2243, 2932, 3244, 1445, 4014, 3345, 1160, 4344, 2971, 5143, 260, 4775, 3095, 5219, 2270, 1943, 2796, 2281, 3261, 2116, 3333, 1334, 1499, 4807, 2681, 1951, 1591, 3468, 4386, 1614, 3754, 392, 1140, 2661, 2614, 3494, 3037, 4269, 694, 968, 983, 1919, 1769, 2262, 4037, 1128, 1662, 4484, 5066, 1056, 2154, 2598, 1164, 833, 4084, 2091, 1104, 4850, 3213, 1050, 1744, 2285, 2296, 142, 4687, 1938, 4693, 1129, 2235, 1422, 3104, 674, 1657, 2454, 678, 955, 1227, 3678, 2848, 3220, 3982, 5055, 3429, 865, 1838, 5117, 2425, 1432, 785, 1916, 1613, 1228, 1049, 3391, 3671, 2960, 3112, 698, 5277, 2170, 5125, 724, 3927, 2017, 1906, 1961, 4645, 2917, 2504, 1107, 4728, 720, 84, 4778, 2776, 1481, 1855, 1670, 2045, 1822, 2871, 2317, 4103, 701, 4734, 1397, 4377, 3828, 3897, 3488, 3619, 2225, 1562, 4756, 1627, 831, 4854, 98, 2870, 3390, 2587, 1925, 1202, 709, 2931, 1051, 4089, 864, 1492, 3878, 3462, 1389, 4773, 1332, 4225, 3936, 1086, 3330, 3214, 2366, 89, 4781, 1218, 3519, 1316, 4907, 4823, 4299, 3159, 4839, 2005, 3657, 1815, 1342, 875, 2399, 2770, 3599, 3188, 5071, 708, 3199, 4430, 4735, 1516, 5197, 405, 826, 4787, 3539, 2379, 5081, 4061, 3592, 4217, 2273, 3438, 138, 4427, 2892, 2075, 3542, 758, 4265, 3312, 3838, 1585, 5301, 2211, 900, 3013, 508, 2574, 3930, 1504, 3970, 739, 3960, 4810, 1234, 771, 1679, 3697, 2475, 3987, 2446, 2567, 2337, 4633, 4772, 5202, 2716, 4148, 2782, 1987, 3810, 4105, 1991, 753, 3119, 281, 4391, 4692, 3835, 2096, 1121, 4267, 4194, 2152, 1408, 2692, 3193, 296, 2175, 3343, 1204, 4466, 728, 3171, 1165, 2952, 1079, 1134, 1210, 4689, 3440, 5042, 1846, 3650, 3870, 4095, 4814, 1118, 1347, 982, 3008, 3616, 4230, 3318, 2728, 4359, 2432, 1891, 1081, 2421, 1673, 3831, 956, 4029, 5300, 3688, 4354, 3090, 1968, 814, 2375, 125, 3647, 697, 1819, 3608, 2197, 999, 3083, 4283, 2265, 2582, 3687, 4872, 1053, 2240, 255, 4358, 1361, 1979, 4736, 4254, 963, 1706, 3110, 2345, 2528, 80, 4710, 107, 3285, 791, 2854, 1205, 2076, 2815, 4043, 1907, 3029, 2647, 2632, 2847, 4741, 1904, 4575, 4324, 4057, 3151, 2745, 1074, 1699, 2809, 2684, 1098, 1292, 1314, 2884, 147, 3518, 3916, 2701, 1645, 3255, 1370, 2650, 1366, 3971, 5148, 4373, 711, 2360, 4032, 2191, 1211, 1638, 2455, 4657, 909, 1457, 761, 5122, 2452, 1176, 3668, 1406, 4120, 2824, 5116, 2672, 2498, 3984, 2183, 3301, 3325, 4737, 3420, 1500, 1364, 1287, 4882, 2537, 1604, 5033, 725, 3725, 2071, 1488, 717, 3006, 4983, 3015, 4393, 1553, 3441, 2839, 1443, 5156, 3783, 2319, 1687, 984, 3277, 3032, 2464, 4296, 847, 4412, 3734, 1225, 3116, 3436, 2846, 2885, 4361, 3536, 2742, 186, 58, 2294, 1469, 4223, 1012, 4800, 1748, 1208, 2970, 1659, 887, 100, 4398, 3758, 2963, 4275, 846, 2525, 5083, 4491, 4899, 95, 2040, 3612, 4697, 1678, 2890, 4221, 3706, 2630, 1873, 3942, 3017, 339, 4130, 4745, 1573, 4124, 1037, 2606, 3944, 2051, 1880, 5128, 3692, 1526, 2862, 1557, 3589, 3245, 3321, 3770, 3411, 978, 736, 3917, 880, 2233, 2642, 1842, 2306, 2643, 4318, 2202, 1182, 1753, 3066, 1395, 3364, 1420, 4190, 3300, 3334, 2828, 1989, 1502, 787, 1644, 2351, 2552, 712, 4272, 2803, 899, 4274, 3023, 2072, 3329, 2916, 3890, 740, 1955, 1206, 3623, 4701, 3999, 2420, 3800, 4198, 1532, 3786, 1423, 2106, 3158, 4448, 4349, 3025, 4876, 4768, 3068, 3177, 4394, 3222, 2622, 2927, 2486, 4454, 4126, 4485, 4140, 64, 3521, 3564, 2310, 1407, 2121, 3063, 4463, 5014, 517, 1997, 2923, 882, 2057, 3554, 815, 4215, 3156, 1138, 2766, 3108, 3880, 3798, 4706, 3759, 2758, 131, 4883, 105, 3680, 2356, 1456, 1130, 3992, 1816, 2325, 2479, 4214, 1868, 2393, 687, 4747, 3598, 1133, 2518, 1650, 1621, 2617, 3401, 235, 3784, 2478, 3642, 4229, 4784, 2722, 1875, 3233, 4648, 3323, 1962, 3924, 2065, 957, 133, 3938, 990, 5145, 4319, 4292, 4350, 2523, 2646, 2626, 2615, 2757, 2230, 2303, 4696, 3102, 1026, 2891, 2140, 3601, 2068, 141, 4213, 2719, 1453, 3485, 1029, 1084, 185, 4750, 1915, 2575, 3775, 2512, 2392, 2333, 2938, 2445, 4834, 908, 3412, 3662, 4087, 4423, 2340, 4722, 1343, 2564, 75, 4861, 881, 3795, 4802, 735, 2103, 2003, 2777, 3875, 1237, 4419, 2292, 1137, 3508, 4207, 4370, 533, 1173, 1634, 1400, 2994, 4779, 4328, 3676, 994, 5121, 1792, 914, 2899, 4785, 4375, 2267, 4460, 1929, 1677, 3444, 1647, 1639, 160, 1958, 3054, 722, 55, 3383, 2940, 4673, 5168, 3550, 3492, 3030, 1082, 4915, 3747, 4117, 4777, 2369, 2391, 2995, 2539, 916, 4381, 5127, 2113, 1554, 4320, 958, 1717, 4363, 1513, 4677, 5275, 2169, 737, 1156, 1139, 4153, 4164, 1995, 3502, 3360, 588, 2481, 2946, 3144, 1135, 3163, 3020, 1022, 1041, 1052, 4434, 742, 980, 4732, 2658, 3350, 1126, 2176, 3031, 1071, 1032, 2924, 4302, 5023, 2560, 2750, 1452, 1756, 1365, 1190, 1226, 4202, 4671, 1544, 1230, 801, 1782, 3065, 2162, 5298, 2779, 4985, 2263, 2429, 2538, 3016, 702, 1213, 4790, 4414, 2662, 4376, 1057, 4473, 746, 1926, 3240, 1044, 2975, 2196, 2239, 4303, 2705, 2664, 1344, 1686, 1546, 3547, 3520, 3749, 1355, 2883, 5040, 5294, 5218, 4399, 2467, 2099, 1163, 2437, 1319, 3403, 1685, 1068, 5097, 4022, 2569, 973, 751, 1354, 4389, 2553, 2082, 3126, 1219, 1440, 689, 3909, 2001, 2732, 1187, 2874, 3990, 1154, 2178, 1807, 4762, 2052, 1890, 2261, 1170, 2231, 2145, 1549, 3883, 3474, 4682, 4676, 3814, 1076, 4237, 3846, 1070, 1142, 2484, 1808, 1322, 4086, 3497, 2962, 2010, 1153, 3793, 4132, 4294, 4797, 3305, 812, 3320, 4245, 3515, 1439, 1200, 1356, 4170, 1412, 2717, 5159, 2908, 2372, 704, 1509, 1103, 1203, 1708, 1725, 3849, 1168, 4761, 810, 4715, 4284, 113, 2715, 526, 1781, 2950, 1174, 1318, 3396, 2415, 4683, 3739, 2945, 1477, 783, 4048, 3586, 1920, 1625, 4255, 2384, 4077, 4467, 1387, 4670, 1474, 5022, 2286, 5311, 2194, 4824, 1851, 1075, 4729, 2756, 2163, 2069, 5106, 950, 3388, 3578, 1335, 5111, 3934, 1108, 3677, 2634, 3825, 816, 1069, 1490, 4426, 3178, 1337, 2258, 4407, 2245, 1123, 3130, 3479, 137, 2089, 2190, 825, 3931, 3972, 2880, 1849, 3399, 4060, 3296, 3997, 1327, 3980, 1188, 944, 273, 3756, 2533, 1078, 2867, 4880, 1131, 3989, 2363, 4343, 3001, 4856, 4075, 3661, 4094, 3670, 2217, 2763, 291, 4916, 2070, 1949, 4937, 4365, 3091, 4109, 1191, 4338, 2177, 4339, 3506, 2509, 2612, 3226, 940, 1497, 1572, 1743, 4162, 1934, 4088, 4835, 3010, 4123, 685, 769, 2426, 2795, 3324, 2410, 2669, 1151, 1467, 1719, 4769, 4413, 4009, 3920, 2521, 1141, 4046, 885, 2671, 4406, 1380, 4496, 4783, 1077, 2925, 3050, 4636, 891, 3648, 3291, 1837, 4439, 942, 4316, 1512, 3724, 4159, 1964, 3866, 2678, 3820, 2148, 904, 2338, 1889, 1674, 1892, 1376, 3358, 3086, 1850, 2510, 4733, 3380, 4307, 4053, 4242, 4744, 1231, 5108, 5068, 2834, 2608, 3631, 3007, 4447, 3882, 4417, 3593, 960, 3583, 2751, 5015, 3867, 114, 3579, 861, 2789, 3540, 1421, 2802, 2387, 2203, 2487, 2551, 4199, 2382, 3145, 3655, 3018, 1636, 3735, 2227, 935, 2783, 1853, 3806, 3352, 1199, 2624, 3174, 3604, 3336, 4840, 2123, 1664, 2548, 1510, 1005, 2698, 1217, 2274, 3622, 3666, 4321, 2434, 3131, 1030, 4238, 1485, 3004, 2998, 4410, 2596, 4116, 5074, 4471, 3169, 5061, 5248, 1600, 3968, 4180, 1119, 4836, 3850, 2907, 3853, 2327, 1976, 1054, 2659, 995, 2723, 2377, 2529, 4796, 2877, 1857, 1116, 3629, 967, 4026, 4397, 1908, 1080, 3818, 4820, 5105, 3385, 3958, 4851, 1693, 2841, 2229, 1695, 3751, 3276, 1437, 837, 3541, 3251, 1193, 1898, 3486, 1223, 3419, 1796, 5196, 3569, 1172, 4186, 4672, 2218, 3730, 3241, 1735, 2209, 2326, 5102, 4788, 1067, 1305, 4348, 1601, 1114, 3635, 2004, 868, 1599, 749, 1760, 1957, 3254, 1263, 2744, 3710, 2402, 2807, 4355, 2335, 1480, 1517, 4847, 4995, 3490, 2520, 684, 3507, 2328, 2249, 3402, 3748, 3988, 3224, 3120, 101, 4743, 1166, 2412, 3903, 3698, 4780, 5312, 2470, 3908, 729, 3675, 4425, 631, 4789, 1705, 4914, 4239, 2912, 3707, 4678, 3660, 3728, 3695, 1941, 1910, 5101, 3281, 4475, 5053, 1007, 4197, 1643, 2836, 2967, 1338, 3509, 2563, 866, 4156, 4740, 3155, 1186, 741, 1811, 2181, 1083, 1757, 4878, 3585, 4216, 3681, 1569, 1381, 2706, 3262, 4201, 2515, 2902, 1830, 1642, 5075, 3463, 1038, 5063, 1424, 1884, 2430, 845, 3525, 4865, 3535, 1326, 3633, 2729, 4011, 1825, 466, 4451, 1712, 2619, 3712, 5038, 2490, 2906, 3856, 4859, 3900, 1483, 2133, 4708, 1755, 2591, 1267, 1548, 1833, 1260, 3548, 2131, 56, 4097, 4817, 377, 4021, 4311, 3933, 4502, 849, 2081, 4912, 3310, 1847, 2992, 3289, 2843, 1201, 3797, 2331, 4494, 2120, 2565, 4932, 4826, 3140, 3587, 4813, 1127, 4259, 979, 2972, 3257, 2244, 2583, 4041, 1236, 2030, 2865, 3407, 3603, 1262, 1611, 5019, 3790, 1522, 206, 573, 1110, 332, 1195, 4042, 1159, 5034, 4667, 2623, 4008, 3378, 4705, 3637, 2981, 2453, 3208, 4658, 1346, 1240, 1974, 3864, 3898, 2845, 3197, 1671, 4511, 1345, 3682, 3128, 2941, 3913, 2826, 4685, 5296, 1903, 3121, 3053, 3082, 3667, 3563, 1265, 1945, 5025, 1150, 2501, 2935, 4472, 4464, 4183, 2699, 636, 5207, 1620, 4260, 2272, 1371, 2064, 2639, 3740, 1072, 4869, 5032, 5008, 2611, 2987, 4885, 3356, 4792, 2913, 4643, 1378, 1096, 3778, 3871, 2656, 135, 4127, 3821, 733, 1560, 1430, 3641, 1112, 2031, 2460, 112, 2697, 779, 4474, 3341, 2666, 3627, 949, 2179, 2500, 1688, 3807, 3129, 4010, 3961, 2182, 5029, 4122, 4279, 3804, 1610, 2354, 4111, 4669, 4524, 2136, 2253, 1399, 4100, 4346, 3393, 1377, 5062, 1583, 4894, 3823, 3737, 1183, 3295, 5245, 1641, 4219, 3372, 2799, 1514, 4312, 2986, 50, 1475, 336, 4830, 4166, 2348, 3041, 3572, 4831, 1575, 3073, 5086, 3035, 1458, 4189, 3611, 5056, 1773, 4174, 1963, 3901, 3366, 4073, 1969, 2409, 2090, 4314, 5247, 2260, 2648, 964, 4248, 1242, 2838, 5302, 1162, 4257, 1436, 2043, 177, 3993, 3581, 2881, 3640, 4881, 3173, 4770, 4107, 1731, 3860, 2775, 5118, 5246, 3203, 2418, 2408, 3571, 1665, 1558, 2876, 103, 1405, 3026, 2887, 3359, 895, 4505, 4731, 1281, 5272, 3309, 1736, 4236, 1742, 3802, 3826, 1386, 2724, 3143, 4470, 2496, 3703, 1105, 2651, 3799, 3243, 4420, 1061, 2549, 539, 1888, 3865, 4849, 5098, 3998, 1313, 2801, 716, 3638, 5142, 4176, 2739, 3840, 2978, 2109, 1102, 926, 3955, 3741, 4863, 1883, 2100, 3456, 506, 2984, 2077, 3852, 2436, 4401, 2378, 4889, 860, 948, 2346, 966, 1409, 4897, 3141, 2115, 484, 1396, 1737, 1683, 1277, 3099, 4300, 2894, 3205, 3699, 2143, 2443, 4347, 996, 3949, 4035, 3103, 2134, 1844, 1666, 4468, 3832, 1893, 1711, 1918, 3427, 3729, 2997, 4842, 3180, 5069, 3106, 1539, 5049, 2413, 4151, 2497, 3803, 2172, 2483, 4960, 2581, 4848, 1697, 1622, 3432, 3879, 4509, 765, 3275, 2733, 876, 2514, 1530, 2088, 2129, 4818, 4231, 2543, 1771, 3523, 2781, 140, 1917, 1416, 1269, 3060, 3854, 3863, 5041, 454, 3483, 3532, 2342, 867, 1152, 2323, 4067, 1523, 4486, 3061, 1425, 3348, 2034, 2784, 3614, 2297, 2896, 1927, 2400, 1547, 3952, 3317, 3048, 4364, 1800, 4618, 2321, 2390, 4047, 102, 2361, 1784, 3409, 1145, 1256, 3817, 2675, 2786, 827, 4429, 4226, 1661, 4668, 2164, 4131, 3922, 4637, 4071, 4681, 3311, 1588, 2855, 2601, 1294, 2714, 2053, 906, 3862, 4098, 1276, 3232, 4167, 2842, 3868, 2746, 1278, 2673, 2527, 1749, 2863, 4059, 3290, 989, 2013, 3546, 3965, 3055, 3829, 4445, 1787, 1358, 1158, 3771, 4335, 977, 3382, 1257, 4690, 4989, 4090, 721, 912, 3526, 2644, 2158, 2117, 1018, 4152, 3981, 3914, 4040, 4718, 4271, 1895, 2388, 359, 3742, 3632, 4563, 2554, 52, 2507, 2873, 2466, 2999, 2167, 4723, 3950, 2768, 897, 48, 1605, 2903, 3459, 1008, 3078, 1374, 4331, 3844, 4403, 4072, 1350, 3953, 1494, 1632, 3719, 1924, 5054, 3857, 1444, 1635, 2078, 4085, 894, 987, 3357, 1438, 1745, 2513, 1960, 802, 3166, 1375, 3943, 1942, 4930, 3705, 5036, 1411, 1778, 1829, 1120, 256, 1672, 4838, 2559, 3313, 3496, 1004, 1821, 3227, 4006, 2849, 2631, 2023, 53, 902, 3557, 1801, 1675, 4249, 3218, 2312, 4121, 155, 3113, 3764, 3012, 3791, 4455, 3537, 3781, 2720, 2128, 1352, 2469, 807, 2499, 3448, 5058, 2195, 4278, 3328, 5035, 969, 1874, 1765, 2083, 877, 2097, 3454, 1923, 3368, 3316, 4845, 4165, 2670, 2674, 2817, 2592, 2157, 2727, 1428, 4903, 2314, 1073, 4280, 2895, 2014, 5141, 4055, 4374, 1648, 2322, 917, 4141, 2979, 2250, 2307, 1284, 1770, 946, 3493, 1702, 2761, 2578, 4065, 1272, 1181, 4864, 1751, 248, 2142, 4252, 3406, 3101, 1759, 3278, 670, 2961, 4063, 4498, 4137, 3070, 3451, 5120, 514, 2929, 4383, 2726, 2922, 2362, 2534, 4220, 1739, 5090, 3656, 3763, 4210, 3559, 4384, 1616, 1233, 3544, 3397, 1589, 4211, 4033, 3966, 4500, 4112, 2788, 2680, 4333, 3446, 3654, 1564, 1984, 5096, 1533, 4079, 2204, 1581, 5094, 1426, 4146, 3701, 1404, 4483, 2708, 3727, 5158, 2439, 2814, 1928, 4841, 489, 4342, 3238, 2449, 2299, 3465, 3941, 109, 1325, 1403, 2688, 3191, 3187, 1506, 738, 522, 930, 5021, 4986, 3379, 3796, 1501, 4707, 2063, 976, 2237, 3361, 5153, 775, 1402, 4168, 1391, 1983, 3282, 1178, 4115, 2220, 1450, 2011, 3549, 3745, 1732, 3235, 4994, 2635, 2860, 3774, 4203, 1881, 1333, 2858, 3286, 5188, 2649, 3250, 4510, 937, 1556, 2748, 323, 803, 3760, 3841, 1859, 1243, 851, 4101, 4888, 3431, 4971, 4803, 5154, 3408, 2561, 2376, 1795, 2584, 4969, 933, 2822, 1786, 3228, 1055, 2125, 888, 3114, 4017, 1824, 1932, 3027, 4713, 4794, 1253, 2535, 4045, 3813, 723, 3417, 4679, 3425, 3097, 4725, 1280, 2473, 5006, 2012, 1886, 1921, 889, 4110, 2086, 219, 1198, 3088, 3019, 4145, 1009, 3911, 3500, 1867, 1972, 975, 2147, 2301, 4433, 776, 1000, 872, 92, 1640, 3098, 1839, 1798, 3315, 1998, 116, 3715, 786, 4001, 4819, 2829, 2016, 1803, 3209, 4232, 3561, 1025, 2171, 2590, 2423, 2818, 1714, 4827, 3134, 3201, 1363, 2160, 3445, 4703, 1447, 4322, 3374, 3045, 1681, 2166, 3767, 2980, 3480, 1862, 4714, 5065, 4169, 3512, 2161, 5191, 4392, 4310, 4415, 2002, 2311, 3888, 4301, 2594, 2610, 2238, 2254, 4206, 4481, 3702, 2394, 1680, 856, 3822, 4456, 1042, 4515, 2532, 1887, 4138, 1328, 4277, 3471, 3625, 2404, 3617, 1865, 5104, 1245, 3565, 3503, 1429, 929, 2754, 4871, 750, 1999, 4369, 4091, 2947, 915, 2901, 884, 4938, 2255, 2875, 1300, 2793, 4507, 5091, 3022, 1448, 4102, 2205, 797, 1090, 2293, 2831, 4435, 3986, 1336, 4290, 3002, 778, 3059, 4025, 230, 3892, 3146, 5115, 1684, 1043, 2355, 3371, 1288, 923, 2655, 2435, 3610, 2018, 693, 2438, 2234, 1180, 1341, 3292, 3511, 3236, 3047, 3096, 3570, 2477, 1099, 2046, 4717, 5271, 4345, 2695, 3858, 3794, 869, 2151, 4382, 3369, 1089, 3928, 2700, 1197, 2259, 2934, 1703, 2417, 1282, 1491, 1462, 1933, 5200, 843, 3636, 4286, 4487, 4691, 2663, 2747, 4934, 5093, 1975, 1484, 1479, 2130, 3449, 932, 2102, 2371, 5085, 2084, 1911, 96, 4149, 1772, 535, 3513, 1870, 3039, 4542, 2491, 2547, 2223, 2503, 2857, 4518, 3782, 4808, 730, 5150, 3160, 1914, 920, 2545, 2458, 1144, 928, 62, 4064, 2494, 3284, 106, 3733, 3340, 2101, 1878, 991, 2020, 2878, 970, 731, 3428, 5201, 2546, 1398, 2406, 854, 4988, 1746, 4656, 2289, 1091, 3093, 4289, 2050, 870, 1897, 4049, 782, 2812, 4313, 2207, 3043, 3215, 4052, 1427, 1726, 5203, 3414, 5079, 947, 795, 1978, 1543, 449, 3150, 4702, 5165, 2772, 5064, 4557, 777, 1353, 2427, 1959, 971, 2213, 4508, 3679, 2898, 2107, 4340, 3370, 2268, 344, 2977, 5297, 1570, 2474, 3212, 3957, 1246, 3691, 3071, 2800, 1950, 1034, 699, 4991, 841, 1289, 2044, 4887, 3138, 3464, 1854, 2054, 927, 2465, 1295, 1518, 1357, 2767, 115, 1728, 862, 1101, 2041, 2566, 1511, 1730, 2886, 1820, 4409, 4332, 1124, 3344, 4638, 1723, 3335, 2808, 1036, 3529, 871, 1694, 1028, 2645, 5073, 1503, 4437, 1777, 1011, 4368, 1752, 3206, 4843, 2118, 1733, 3962, 2949, 3935, 1060, 1952, 2861, 1297, 1271, 3210, 924, 1031, 2835, 2416, 3939, 1003, 1584, 1763, 2589, 110, 2517, 1690, 97, 3976, 4844, 4404, 2264, 3051, 3720, 4023, 3902, 4012, 4179, 2059, 1367, 6, 2173, 896, 5099, 5005, 1155, 1559, 227, 3718, 1472, 4039, 3036, 4050, 3421, 3157, 793, 2625, 3769, 1418, 3400, 863, 4791, 1593, 3395, 883, 3600, 1944, 4224, 2027, 919, 3808, 3442, 1521, 2026, 143, 1505, 3811, 3948, 2058, 2472, 5195, 2653, 3974, 1454, 3049, 3643, 2797, 4452, 5043, 87, 3588, 1571, 1519, 4362, 934, 1259, 1373, 1360, 3591, 988, 951, 1013, 3765, 1232, 4336, 369, 974, 5020, 5189, 1793, 1861, 2774, 2920, 2428, 2206, 5028, 1779, 1248, 3005, 1937, 5103, 734, 1667, 2636, 4450, 4015, 2609, 1542, 2364, 1930, 4949, 1486, 4816, 2685, 3634, 4387, 4360, 1261, 1394, 1764, 2621, 4443, 5010, 2189, 3717, 972, 3306, 108, 1087, 3273, 3983, 118, 4171, 799, 1390, 1827, 939, 2813, 4476, 3534, 3884, 993, 4287, 2558, 3785, 855, 3956, 1065, 3014, 3067, 2208, 910, 2114, 756, 5174, 4308, 925, 2942, 2383, 5048, 1790, 1254, 3977, 2811, 1633, 3258, 2442, 893, 2879, 4432, 2969, 1656, 3531, 2370, 551, 3533, 353, 2536, 3217, 3398, 5254, 4058, 3753, 985, 2037, 1619, 921, 3562, 1710, 5223, 3297, 4083, 4352, 4688, 559, 104, 4921, 1691, 938, 1296, 401, 4862, 3125, 1856, 4698, 3573, 901, 352, 4402, 2025, 839, 2682, 2048, 726, 4128, 3279, 4222, 1653, 1948, 1654, 2411, 4716, 3590, 3057, 3904, 3819, 4828, 2153, 269, 2837, 2108, 3624, 1741, 2531, 1566, 3219, 2476, 3287, 5092, 3162, 3225, 1700, 2283, 780, 892, 3937, 2444, 2914, 943, 2958, 3905, 2540, 4081, 3812, 2759, 2330, 1794, 3580, 3308, 3685, 1393, 768, 1258, 4070, 759, 2036, 898, 2367, 3270, 732, 94, 2600, 1476, 4940, 2073, 3969, 2806, 3809, 3087, 1767, 3672, 3645, 2721, 1791, 1947, 3777, 838, 2790, 4243, 2953, 774, 4297, 3646, 3384, 3268, 4943, 1718, 1802, 4795, 4240, 3009, 2247, 2712, 1209, 2433, 4441, 3064, 842, 3714, 3196, 2990, 3184, 1663, 3179, 1306, 2679, 3516, 1552, 4193, 4499, 1985, 1738, 1125, 2341, 3499, 2794, 4891, 3424, 1704, 497, 4721, 1651, 1349, 4028, 3906, 264, 3505, 2029, 4892, 1860, 188, 1331, 1149, 2256, 2212, 1362, 3437, 1912, 1713, 4034, 2132, 3267, 4479, 2599, 3726, 1063, 4357, 4793, 1385, 1596, 1250, 10, 4204, 3618, 2556, 2516, 617, 809, 1434, 2441, 4160, 72, 1565, 4829, 2820, 922, 1442, 3092, 4928, 886, 4945, 3711, 1431, 2585, 936, 3274, 1658, 2137, 4492, 1540, 5078, 354, 4944, 829, 4353, 1339, 3644, 2257, 42, 2488, 2080, 2127, 1473, 4062, 3283, 4051, 2414, 3263, 2159, 5253, 997, 4400, 4490, 4674, 1841, 2731, 3651, 3887, 4950, 4341, 3293, 4182, 90, 3381, 992, 1148, 2524, 4315, 1603, 1384, 3628, 2279, 5199, 3450, 5273, 3426, 757, 1646, 1909, 3476, 2709, 4605, 3779, 3723, 1828, 3575, 4031, 813, 2711, 4664, 2911, 1977, 3467, 805, 2730, 1529, 3460, 654, 2397, 1449, 5095, 1216, 1902, 1064, 2422, 2124, 1899, 2686, 4815, 4478, 5107, 3932, 2736, 451, 2024, 3288, 139, 5186, 2284, 1863, 3684, 1157, 1014, 2769, 2459, 3447, 874, 1990, 3304, 834, 2174, 1900, 1720, 1594, 2973, 2872, 2637, 5284, 2463, 1814, 1309, 2008, 1035, 3489, 1885, 4076, 61, 119, 2022, 911, 3080, 3477, 1913, 2926, 3886, 5007, 3003, 1631, 3038, 3985, 3327, 1515, 2956, 5184, 2710, 1992, 2968, 784, 4191, 1832, 3780, 2246, 3458, 3303, 4173, 2704, 77, 1953, 3000, 3472, 599, 5072, 931, 3339, 5050, 4118, 3338, 1482, 2095, 3349, 2762, 1550, 1508, 3294, 3732, 5031, 93, 4482, 4900, 4873, 1775, 3527, 3973, 4754, 1813, 60, 1940, 4639, 954, 3434, 3256, 634, 4253, 455, 2964, 3837, 1946, 2489, 3716, 3229, 1761, 1852, 3877, 2419, 1698, 4746, 3845, 2572, 1465, 2755, 3085, 1146, 2542, 156, 2405, 3830, 5161, 1721, 2825, 2555, 3413, 4367, 3347, 2187, 2804, 2948, 794, 2482, 1660, 762, 1379, 1858, 85, 1896, 3058, 2347, 1303, 1417, 3207, 4961, 3613, 962, 4288, 1249, 2550, 3147, 4825, 5112, 703, 2365, 2760, 1780, 1626, 3186, 3996, 1993, 3259, 4129, 2957, 3168, 890, 3652, 3551, 3165, 2094, 2690, 4680, 3105, 1433, 1707, 5155, 2753, 4143, 4030, 2186, 4178, 2989, 4833, 4334, 3567, 1264, 1826, 1027, 2965, 4611, 3190, 918, 2627, 4356, 3639, 1939, 2707, 998, 1971, 4366, 5278, 2864, 3861, 1936, 2657, 903, 3084, 718, 2505, 4898, 1286, 2805, 373, 2461, 4016, 3757, 4208, 2511, 1788, 1279, 5140, 2035, 4188, 1275, 1048, 3170, 1019, 3597, 395, 1244, 835, 3885, 3452, 4821, 5119, 781, 3731, 2193, 3375, 2149, 1291, 1247, 3453, 4442, 4069, 3216, 4371, 2541, 2199, 1586, 4002, 4281, 1021, 78, 1301, 3514, 4068, 2773, 4133, 2966, 4326, 2485, 3430, 3195, 4113, 3123, 3089, 959, 2287, 3052, 2320, 4910, 1582, 3044, 3921, 330, 3621, 852, 2613, 941, 2334, 4503, 2462, 2798, 4489, 1372, 1567, 2910, 1597, 1869, 2562, 2079, 2373, 1766, 1040, 1466, 1088, 1806, 1580, 2738, 3689, 1024, 1033, 1273, 4495, 1317, 4798, 2982, 2278, 233, 5190, 1607, 1598, 1002, 1266, 3377, 493, 2576, 2146, 1810, 3247, 3827, 4623, 49, 2741, 495, 2985, 3746, 1986, 4920, 4251, 873, 1285, 4501, 2241, 965, 2580, 2007, 374, 3192, 1935, 3136, 4597, 3337, 1059, 2687, 1251, 3553, 4304, 457, 4866, 4782, 3011, 488, 2629, 3418, 4155, 2718, 2038, 3709, 2557, 4730, 5220, 1304, 4099, 123, 5024, 1845, 3231, 2269, 3204, 3137, 2332, 2597, 2324, 1722, 2456, 4175, 2616, 1495, 5002, 657, 3495, 3545, 1980, 1241, 2236, 300, 3595, 4526, 5249, 3230, 1538, 945, 1701, 3362, 4187, 351, 3405, 3081, 1312, 3223, 2677, 3907, 2216, 4860, 1507, 2192, 2198, 2396, 2316, 3543, 287, 1537, 3410, 3555, 1750, 3755, 1293, 1020, 1252, 3926, 2480, 3761, 1669, 5279, 3978, 2603, 3265, 4422, 2579, 2882, 986, 4228, 4270, 1307, 2749, 4917, 2471, 2577, 3353, 3198, 149, 3945, 3584, 2607, 1804, 4019, 4663, 1922, 2336, 2859, 4114, 29, 4209, 2252, 4000, 2224, 4139, 3404, 3658, 818, 2032, 1415, 4607, 2350, 2424, 2683, 3115, 2276, 151, 4457, 913, 3669, 4809, 4990, 4610, 8, 2085, 4104, 122, 3376, 2605, 1727, 3772, 1255, 748, 2852, 1901, 3975, 5303, 2112, 2290, 1001, 1401, 2909, 2139, 136, 1177, 366, 3135, 1931, 220, 1302, 3848, 3596, 2315, 509, 3100, 3415, 1460, 4933, 1637, 4258, 3834, 3142, 2633, 3077, 5169, 5016, 1970, 2959, 3322, 3873, 4305, 5163, 3594, 4517, 2242, 4520, 4570, 3355, 2853, 246, 1776, 2570, 4227, 2168, 5171, 4686, 1369, 4306, 3560, 1496, 4858, 1010, 4405, 5229, 5222, 4993, 4066, 1592, 4551, 4631, 3161, 2944, 3721, 1270, 3752, 4634, 4020, 3367, 1498, 3021, 3326, 1551, 1039, 3272, 4931, 3766, 4984, 2087, 1016, 2951, 1446, 1058, 3558, 5172, 4929, 1015, 4317, 3683, 4, 379, 2703, 3498, 1368, 3124, 2900, 3574, 2447, 5166, 386, 2358, 658, 3896, 5221, 2954, 4624, 3895, 2544, 2519, 4870, 2904, 5009, 4142, 3076, 3354, 1615, 3524, 840, 150, 1595, 1147, 1525, 1729, 357, 4602, 3253, 1823, 3056, 4136, 1283, 1785, 1290, 1799, 4395, 2403, 3373, 4999, 2302, 1017, 4627, 2156, 1298, 5100, 4218, 3461, 1768, 4909, 4440, 5045, 2165, 2248, 2295, 3094, 1047, 5194, 134, 2144, 2996, 844, 5039, 1299, 5309, 4927, 4755, 3269, 3211, 4539, 4003, 2184, 5251, 1590, 4082, 211, 117, 1623, 2851, 2689, 3034, 3918, 1652, 3768, 2021, 4096, 5233, 2702, 153, 4913, 823, 798, 3046, 2450, 4660, 4968, 3422, 625, 2359, 828, 476, 3568, 2495, 3876, 4449, 5044, 1876, 2955, 5286, 380, 1676, 5113, 4603, 4157, 4644, 3435, 1461, 4480, 2042, 3466, 3249, 4185, 2015, 467, 3473, 3894, 2660, 4879, 3510, 3332, 1274, 99, 2288, 4601, 4700, 4853, 4604, 2691, 4108, 4436, 3153, 5177, 120, 3842, 4293, 2638, 2641, 3626, 4550, 412, 4329, 4552, 3033, 4545, 2787, 4626, 1315, 5204, 3152, 4886, 363, 3847, 2374, 2506, 158, 7, 170, 2780, 2313, 3189, 5185, 5012, 2816, 4926, 128, 4647, 4628, 2764, 4720, 70, 4902, 5004, 4659, 2277, 587, 3351, 629, 2105, 340, 4135, 2889, 4967, 3319, 1413, 3079, 3455, 2693, 2401, 4608, 3111, 2943, 5030, 4922, 3491, 277, 4724, 5299, 5051, 4465, 2468, 2344, 4250, 830, 3133, 3673, 76, 5070, 1696, 4875, 4905, 2232, 4599, 4615, 4877, 4595, 2905, 4585, 88, 4244, 2251, 3801, 132, 2893, 4906, 2937, 5307, 4947, 4154, 129, 3940, 4588, 2939, 5187, 2676, 4125, 3416, 307, 4941, 1836, 2918, 5281, 2398, 408, 4233, 4852, 33, 1834, 4497, 4256, 51, 3365, 3252, 333, 4150, 5151, 5152, 3221, 3154, 3528, 5157, 598, 2983, 4625, 2222, 4453, 2200, 5026, 3963, 1587, 3839, 4653, 2850, 3517, 121, 2180, 1894, 481, 4396, 83, 4056, 242, 3704, 4614, 63, 4458, 5027, 2368, 2573, 152, 468, 5060, 5205, 130, 5289, 4241, 1568, 5183, 4309, 3202, 4298, 4337, 2668, 3423, 5179, 4408, 2810, 4438, 4247, 2897, 2309, 4330, 1954, 585, 3481, 3815, 4868, 5018, 2343, 4666, 1541, 3606, 258, 4459, 527, 2620, 4514, 5288, 3118, 4598, 4591, 43, 3346, 3609, 166, 5131, 4613, 4704, 442, 2275, 425, 2993, 4957, 3788, 4617, 4446, 5192, 2915, 375, 3776, 4119, 528, 4619, 3556, 3167, 4327, 3538, 154, 5167, 2093, 4195, 3722, 555, 1655, 148, 5089, 3576, 3694, 3342, 420, 1574, 4596, 511, 5037, 68, 290, 337, 490, 4998, 5114, 5176, 453, 157, 4936, 35, 1872, 563, 3237, 4699, 4846, 4958, 126, 2431, 3470, 4837, 4521, 513, 293, 656, 1774, 4621, 4390, 604, 34, 4553, 4572, 601, 226, 4295, 499, 4904, 46, 4966, 5208, 666, 1524, 4630, 1994, 5080, 4594, 5136, 145, 639, 410, 5252, 4529, 111, 4884, 3182, 3910, 4181, 498, 660, 1609, 4106, 616, 4832, 5149, 164, 4573, 4462, 4565, 4519, 1464, 4584, 4044, 438, 542, 3762, 32, 4522, 315, 1618, 174, 346, 5011, 4635, 4579, 4651, 5304, 5269, 274, 4262, 1624, 503, 205, 572, 2006, 5225, 432, 4533, 5283, 398, 5234, 3117, 4655, 82, 237, 4752, 4590, 4976, 3792, 38, 566, 388, 4593, 4973, 5147, 4544, 4534, 69, 5, 5001, 5266, 427, 3833, 5224, 4632, 4867, 5178, 176, 28, 5292, 5182, 4528, 244, 668, 4918, 4979, 5231, 4662, 397, 4548, 303, 4530, 456, 163, 4753, 5087, 575, 4616, 441, 4972, 4646, 240, 5282, 641, 5230, 4592, 4546, 4562, 4549, 4325, 4924, 568, 4541, 4925, 182, 41, 4629, 565, 162, 86, 319, 4997, 396, 4574, 4537, 5047, 189, 231, 21, 4527, 4935, 491, 4567, 4956, 4580, 584, 2595, 4571, 168, 305, 4561, 59, 225, 645, 627, 4606, 4684, 215, 548, 4581, 262, 31, 5133, 5280, 4751, 4516, 4564, 5003, 4535, 4962, 4582, 44, 4569, 4952, 271, 4661, 562, 5265, 5135, 4953, 4612, 45, 4896, 4622, 124, 591, 4946, 91, 644, 5287, 4890, 651, 4919, 632, 4719, 5285, 30, 469, 618, 5270, 661, 4555, 5259, 5232, 54, 480, 4559, 2, 5134, 4951, 406, 5228, 272, 671, 389, 5146, 612, 5052, 5310, 5057, 413, 521, 461, 66, 4757, 4566, 4939, 5290, 578, 580, 322, 5226, 247, 350, 4547, 5173, 502, 416, 327, 4587, 505, 4855, 546, 27, 167, 4543, 261, 4538, 25, 655, 4650, 649, 5206, 5013, 5170, 345, 590, 223, 4749, 5291, 624, 4577, 603, 4560, 4652, 279, 4523, 390, 314, 5017, 81, 4978, 295, 214, 4558, 241, 400, 407, 421, 238, 200, 37, 19, 458, 268, 301, 417, 4609, 387, 5162, 403, 620, 199, 331, 263, 512, 662, 5160, 23, 602, 24, 338, 4748, 437, 3, 537, 589, 179, 492, 4525, 569, 18, 276, 583, 71, 286, 4536, 4665, 459, 541, 361, 229, 5264, 47, 239, 477, 4948, 4987, 12, 4586, 5059, 20, 524, 4554, 464, 669, 4532, 482, 4911, 5293, 623, 419, 440, 659, 619, 196, 254, 216, 16, 321, 309, 232, 577, 40, 446, 626, 556, 4955, 270, 552, 5132, 355, 4556, 510, 5164, 500, 518, 74, 221, 402, 605, 557, 4540, 553, 15, 358, 595, 310, 586, 5305, 278, 544, 622, 393, 297, 483, 426, 609, 423, 4996, 672, 313, 485, 4857, 343, 4942, 257, 561, 213, 181, 250, 318, 370, 430, 633, 567, 5267, 4992, 317, 581, 347, 4959, 197, 463, 175, 519, 222, 249, 424, 447, 17, 192, 383, 349, 328, 36, 462, 367, 292, 547, 218, 4874, 266, 234, 5262, 311, 434, 326, 385, 320, 203, 334, 4974, 253, 445, 613, 208, 504, 621, 39, 173, 404, 4531, 582, 348, 342, 11, 444, 282, 4893, 5227, 472, 289, 324, 452, 630, 474, 501, 384, 549, 647, 570, 663, 560, 475, 228, 614, 635, 294, 431, 251, 646, 169, 325, 465, 5308, 520, 195, 667, 610, 4600, 5175, 652, 184, 4620, 180, 259, 4982, 436, 283, 642, 496, 316, 178, 308, 538, 212, 187, 372, 594, 558, 422, 650, 523, 368, 473, 165, 329, 593, 5260, 252, 415, 394, 245, 171, 648, 530, 470, 471, 364, 611, 5046, 540, 428, 207, 172, 608, 571, 146, 5306, 217, 411, 204, 304, 299, 531, 435, 210, 73, 460, 4975, 414, 507, 365, 615, 4654, 600, 536, 14, 341, 478, 487, 4954, 202, 550, 191, 532, 515, 637, 376, 592, 13, 450, 525, 194, 4583, 486, 409, 596, 4977, 607, 606, 628, 67, 312, 267, 209, 391, 564, 280, 418, 224, 335, 193, 4578, 545, 378, 4963, 5193, 4964, 579, 198, 653, 265, 5263, 285, 638, 236, 4589, 5000, 479, 302, 201, 574, 362, 5261, 494, 382, 22, 0, 448, 306, 360, 543, 439, 26, 664, 429, 5250, 371, 534, 5137, 399, 597, 576, 298, 4965, 554, 4923, 643, 673, 243, 1, 381, 665, 433, 640, 4970, 4895, 9, 5180, 5258, 5130, 5077, 4981, 5268, 5088, 4980, 5076]\nNumber of tracks with MSE &gt; 1: 132\n\n\nLet’s take a closer look at the 10 tracks with highest MSE.\n\nprint(\"Lowest MSEs\")\nfor i in range(10):\n    track = desc_index[i]\n    print(f\"Track: {track}, MSE: {mses[track]}, Description: {df_targets.iloc[track, -1]}\")\n\nLowest MSEs\nTrack: 4694, MSE: 142.65538024902344, Description: CAGE:testis, adult, pool1\nTrack: 4675, MSE: 71.54290771484375, Description: CAGE:Clontech Human Universal Reference Total RNA, pool1\nTrack: 5181, MSE: 54.46543502807617, Description: CAGE:testis, adult, pool2\nTrack: 4266, MSE: 46.77195358276367, Description: CHIP:H3K27ac:epithelial cell of prostate male\nTrack: 4004, MSE: 39.536231994628906, Description: CHIP:H3K27ac:PC-3\nTrack: 5239, MSE: 30.45405387878418, Description: CAGE:CD14+ monocytes - treated with Group A streptococci,\nTrack: 4093, MSE: 23.13290786743164, Description: CHIP:H3K27ac:OCI-LY3\nTrack: 5240, MSE: 21.570188522338867, Description: CAGE:CD14+ monocytes - treated with lipopolysaccharide,\nTrack: 5235, MSE: 21.21177101135254, Description: CAGE:CD14+ monocytes - treated with BCG,\nTrack: 4477, MSE: 18.49295997619629, Description: CHIP:H3K27ac:RWPE2\n\n\nThey are all ChIP-Seq tracks.\nNext, we look at the distribution of differences for each track.\n\ndf_diff = pd.DataFrame(abs(diff))\nsums = df_diff.describe()\n\nWe first sort the tracks by biggest difference (absolute value) across the bins.\n\ndesc_col = sorted(range(5313), reverse = True, key = lambda x: sums.loc['max', x]) # diff\n\nNow we print summary statistics for tracks with the largest differences.\n\nsums.iloc[:, [i for i in desc_col[:10]]]\n\n\n\n\n\n\n\n\n4694\n4675\n5181\n5239\n5240\n5235\n5241\n4266\n5237\n5243\n\n\n\n\ncount\n8.960000e+02\n8.960000e+02\n8.960000e+02\n896.000000\n8.960000e+02\n896.000000\n8.960000e+02\n896.000000\n896.000000\n896.000000\n\n\nmean\n1.433734e-02\n1.051100e-02\n8.807106e-03\n0.020835\n2.031211e-02\n0.021861\n1.900812e-02\n0.034657\n0.020162\n0.010691\n\n\nstd\n3.989808e-01\n2.825342e-01\n2.465311e-01\n0.183282\n1.539082e-01\n0.152387\n1.278184e-01\n0.225957\n0.122659\n0.112028\n\n\nmin\n2.104789e-07\n5.960464e-08\n1.005828e-07\n0.000004\n1.490116e-07\n0.000002\n3.576279e-07\n0.000003\n0.000008\n0.000006\n\n\n25%\n3.366172e-05\n2.879463e-05\n3.946968e-05\n0.000419\n5.722363e-04\n0.000375\n5.093981e-04\n0.000543\n0.000455\n0.000258\n\n\n50%\n9.969529e-05\n8.479506e-05\n8.589961e-05\n0.001356\n2.253987e-03\n0.001337\n1.763301e-03\n0.001110\n0.001543\n0.000666\n\n\n75%\n3.051944e-04\n2.499809e-04\n2.402347e-04\n0.005375\n9.241708e-03\n0.007166\n7.548980e-03\n0.002978\n0.007542\n0.002136\n\n\nmax\n1.194086e+01\n8.453053e+00\n7.377880e+00\n4.790039\n4.033737e+00\n3.902428\n3.294865e+00\n3.184300\n3.029507\n2.972141\n\n\n\n\n\n\n\nHere we plot distributions for the tracks with largest (absolute) differences. Take note that this plot shows distributions of differences in both directions. This plot confirms the table above, that despite having outliers their mean difference is still very close to 0.\n\ndata = [diff[:,i] for i in desc_col[:10]]\nplt.boxplot(data)\nplt.show()"
  },
  {
    "objectID": "posts/hackathon-enformer-usage/Hackathon_enformer_usage_participant.html",
    "href": "posts/hackathon-enformer-usage/Hackathon_enformer_usage_participant.html",
    "title": "GENETIC MEDICINE DEEP LEARNING HACKATHON 2022",
    "section": "",
    "text": "Authors: Saideep Gona, Temidayo Adeluwa\nAcknowledgement: - Boxiang Liu - Festus Nyasimi (for providing us with Predixcan predictions)\nDate: Saturday April 2, 2022\nCopyright 2021 DeepMind Technologies Limited\nLicensed under the Apache License, Version 2.0 (the “License”); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n https://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
  },
  {
    "objectID": "posts/hackathon-enformer-usage/Hackathon_enformer_usage_participant.html#introduction",
    "href": "posts/hackathon-enformer-usage/Hackathon_enformer_usage_participant.html#introduction",
    "title": "GENETIC MEDICINE DEEP LEARNING HACKATHON 2022",
    "section": "Introduction",
    "text": "Introduction\nIn this notebook, we explore how Enformer can be used to predict the expression of protein-coding genes. We utilized some code from the original Enformer usage colab notebook. Here, we showcase how the Enformer model can be used to predict gene expression on a GEUVADIS/1000 genomes dataset, and compare the predictions with true expression.\n“Effective gene expression prediction from sequence by integrating long-range interactions”\nŽiga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R. Ledsam, Agnieszka Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, David R. Kelley\n\nSteps\nThis notebook demonstrates how to - Prepare inputs for Enformer to make predictions - Make predictions with Enformer and produce figures - Compare predictions with true expression"
  },
  {
    "objectID": "posts/hackathon-enformer-usage/Hackathon_enformer_usage_participant.html#setup",
    "href": "posts/hackathon-enformer-usage/Hackathon_enformer_usage_participant.html#setup",
    "title": "GENETIC MEDICINE DEEP LEARNING HACKATHON 2022",
    "section": "Setup",
    "text": "Setup\nGoogle Colab gives us some GPU access. This limited GPU is available to anyone with a Google account, who has signed up to use Colaboratory. We will begin by changing the runtime type to GPU. Follow the instruction below by clicking on “Runtime -&gt; Change runtime type -&gt; GPU” in the menu bar below the title of this notebook.\nStart the colab kernel with GPU: Runtime -&gt; Change runtime type -&gt; GPU\nBelow, we import tensorflow as tf, and check that the runtime has been changed to GPU.\nkipoiseq is a package that helps us to extract sequences from fasta files given some intervals. We will install the package.\n\nimport tensorflow as tf\n# Make sure the GPU is enabled \nassert tf.config.list_physical_devices('GPU'), 'Start the colab kernel with GPU: Runtime -&gt; Change runtime type -&gt; GPU'\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n\n\n# You can ignore the pyYAML error\n!echo $SHELL\n!echo $PATH\n\nBiopython is a python package that helps us do many bioinfomatic analysis in python\n\n#!pip install Biopython\n\n\nSetting up our environments\nWe need to have some packages imported to help us do cool stuff.\n\nimport tensorflow_hub as hub # for interacting with saved models and tensorflow hub\nimport joblib\nimport gzip # for manipulating compressed files\nimport kipoiseq # for manipulating fasta files\nfrom kipoiseq import Interval # same as above, really\nimport pyfaidx # to index our reference genome file\nimport pandas as pd # for manipulating dataframes\nimport numpy as np # for numerical computations\nimport matplotlib.pyplot as plt # for plotting\nimport matplotlib as mpl # for plotting\nimport seaborn as sns # for plotting\nimport pickle # for saving large objects\nimport os, sys # functions for interacting with the operating system\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nOptional\nYou may want to store your results. Google Drive gives about 15gb worth of storage space, used for all your files and emails.\nHere, you can mount your Google Drive using the next line of code. You will need to provide permission access.\nThe line after that will automatically create a folder called “Enformer_Hackathon_2022” in your Google Drive.\n\n#from google.colab import drive\n#drive.mount('/content/drive')\n\n\n#!mkdir -p \"/content/drive/MyDrive/Enformer_Hackathon_2022/results/\"\n\nNext,\nWe want to define some paths to save downloaded files for the duration of this notebook. These will be wiped off by Google as soon as we are done.\n\ntransform_path = 'gs://dm-enformer/models/enformer.finetuned.SAD.robustscaler-PCA500-robustscaler.transform.pkl'\nmodel_path = 'https://tfhub.dev/deepmind/enformer/1'\nfasta_file = '/home/s1mi/enformer_tutorial/genome.fa'\n\nWe may inspect the tracks used to train the model. The CAGE prediction corresponding to B lymphoblastoid cell line is index 5110. We use B lymphoblastoid cell line predictions here because that is the cell line used to generate GEUVADIS gene expression data. You can copy the https link, paste in another tab in your browser and look through the large txt file for other tracks.\n\n# Download targets from Basenji2 dataset\n# Cite: Kelley et al Cross-species regulatory sequence activity prediction. PLoS Comput. Biol. 16, e1008050 (2020).\ntargets_txt = 'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_human.txt'\ndf_targets = pd.read_csv(targets_txt, sep='\\t')\ndf_targets[df_targets.index==5110]\n\n\n\nDownload files\nWe need to download some files. Give it a moment. We will download the following files: - The reference genome fasta file (we will also index this file in the process) - A text file for the transcription start sites for each chromosome - Per chromosome files that has annotation for the genes - A compressed file that contains the variant bed files for the genes and their locations.\nCredit to Genome Reference Consortium: https://www.ncbi.nlm.nih.gov/grc\nSchneider et al 2017 http://dx.doi.org/10.1101/gr.213611.116: Evaluation of GRCh38 and de novo haploid genome assemblies demonstrates the enduring quality of the reference assembly\nMake a data directory, and download the necessary bed files and chromosome annotation files\nNB: You may decide to download these files into your “/content/drive/MyDrive/Enformer_Hackathon_2022/” directory. You don’t need to do this. But if you want permanent access to the files we use in this notebook, you can change the path from “/home/s1mi/enformer_tutorial/” to “/content/drive/MyDrive/Enformer_Hackathon_2022/”, and modify what you need accordingly.\nThe next line of code will download the reference genome fasta file and index this file.\n\n# reference genome and indexed\n#!wget -O - https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz | gunzip -c &gt; {fasta_file}\n#pyfaidx.Faidx(fasta_file)\n\nThe next lines of code will download the variation bed files, and we have created links to help us download the variation bed files for each chromosome, for each gene.\n\nchrom_bed_downloads = pd.read_csv(\"https://uchicago.box.com/shared/static/du77wf31li38tciv8imivwu57svae03p.csv\")\nchrom_bed_downloads.index = chrom_bed_downloads[\"chroms\"]\n\nchrom_bed_downloads.head(5)\n\nWe will define a function to help us download bed variation files for a given gene or list of genes\n\ndef download_chrom_beds(chromosome, genes, downloads_table=chrom_bed_downloads):\n  '''\n  Downloads bed/variation files for a chromosome and list of genes\n  '''\n\n  link = downloads_table.loc[str(chromosome), \"link\"]\n  chr_which = 'chr' + chromosome\n  for gene in genes:\n    if os.path.exists('/home/s1mi/enformer_tutorial/individual_beds/chr' + chromosome + '/chr' + chromosome + '_' + gene + '.bed'): # if the file is in the folder, no need to download again\n      continue\n    !curl -L {link} --output /home/s1mi/enformer_tutorial/chr_{chromosome}_bed.tar.gz && cd /home/s1mi/enformer_tutorial && tar -zxf /home/s1mi/enformer_tutorial/chr_{chromosome}_bed.tar.gz ./individual_beds/{chr_which}/{chr_which}_{gene}.bed\n\n    # remove the download tar.gz file\n    !rm /home/s1mi/enformer_tutorial/chr_{chromosome}_bed.tar.gz\n\nWe don’t need this function yet. But we can test out how it works.\nAssuming we want to download the variation files for ‘ERAP1’, which is located on chromosome 5…\nThis will download the bed file into /home/s1mi/enformer_tutorial/individual_beds/chr5/\n\ndownload_chrom_beds(chromosome = '5', genes=['ERAP1', 'ERAP2'])\n\nAnd when you need the file, you can read it in like…\n\nerap1_variations = pd.read_table('/home/s1mi/enformer_tutorial/individual_beds/chr5/chr5_ERAP1.bed', sep='\\t')\nerap1_variations.head(5)\n\nYou can pass in a list of genes as long as they are all located on that chromosome.\nIn the next block of code, we download the TSS for each chromosome and the genes in that chromosome, as wells as the per chromosome gene annotations. We need this information to estimate predictions.\n\n#!curl -L https://uchicago.box.com/shared/static/perc3uabzzd267cbp8zc0inwgrmur7pu.gz --output /home/s1mi/enformer_tutorial/chr_tss.tar.xz && cd /home/s1mi/enformer_tutorial/ && tar -zxf /home/s1mi/enformer_tutorial/chr_tss.tar.xz\n\n#!mkdir -p /home/s1mi/enformer_tutorial/gene_chroms #creates a folder to hold our files\n#!curl -L https://uchicago.box.com/shared/static/e2kiwrjlgqqio0pc37a2iz7l5bqbv57u.gz --output /home/s1mi/enformer_tutorial/gene_chroms/gene_chroms.tar.gz && cd /home/s1mi/enformer_tutorial/gene_chroms/ && tar -zxf /home/s1mi/enformer_tutorial/gene_chroms/gene_chroms.tar.gz\n\n\n\nHow do we want to go about using Enformer given all these files we just downloaded?\nAs we know, enformer’s input is a single strand genome sequence. Yet, we are interested in predicting on population level data which includes individual-specific variation. To get around this limitation, we will treat each individual as the sum of their haplotypes. Using the phased variant data around each gene (stored in the variant bed files) to modify the reference sequence, we can create two distinct haplotype sequences for each individual. The sum of both of Enformer’s haplotype predictions serves as an individual-specific, additive estimate which we can correlate with true predictions. Together, the files we downloaded give us all the information we need to build these haplotype sequences.\nAlthought enformer predicts a wide array of functional output, we will focus here on gene expression in lymphoblastoid cells allowing for correlation against ground truth Geuvadis gene expression data.\nThere are many functions that we have defined in the next code block. You can explore them later, but for now, simply run the block by clicking on the play button.\n\n\nCode\nNext, we have some functions that will help us along the way. Classes and methods defined in this code block can be found in the original Enformer usage colab notebook.\n\n# @title `Enformer`, `EnformerScoreVariantsNormalized`, `EnformerScoreVariantsPCANormalized`,\nSEQUENCE_LENGTH = 393216\n\nclass Enformer:\n\n  def __init__(self, tfhub_url):\n    self._model = hub.load(tfhub_url).model\n\n  def predict_on_batch(self, inputs):\n    predictions = self._model.predict_on_batch(inputs)\n    return {k: v.numpy() for k, v in predictions.items()}\n\n  @tf.function\n  def contribution_input_grad(self, input_sequence,\n                              target_mask, output_head='human'):\n    input_sequence = input_sequence[tf.newaxis]\n\n    target_mask_mass = tf.reduce_sum(target_mask)\n    with tf.GradientTape() as tape:\n      tape.watch(input_sequence)\n      prediction = tf.reduce_sum(\n          target_mask[tf.newaxis] *\n          self._model.predict_on_batch(input_sequence)[output_head]) / target_mask_mass\n\n    input_grad = tape.gradient(prediction, input_sequence) * input_sequence\n    input_grad = tf.squeeze(input_grad, axis=0)\n    return tf.reduce_sum(input_grad, axis=-1)\n\n\nclass EnformerScoreVariantsRaw:\n\n  def __init__(self, tfhub_url, organism='human'):\n    self._model = Enformer(tfhub_url)\n    self._organism = organism\n\n  def predict_on_batch(self, inputs):\n    ref_prediction = self._model.predict_on_batch(inputs['ref'])[self._organism]\n    alt_prediction = self._model.predict_on_batch(inputs['alt'])[self._organism]\n\n    return alt_prediction.mean(axis=1) - ref_prediction.mean(axis=1)\n\n\nclass EnformerScoreVariantsNormalized:\n\n  def __init__(self, tfhub_url, transform_pkl_path,\n               organism='human'):\n    assert organism == 'human', 'Transforms only compatible with organism=human'\n    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n      transform_pipeline = joblib.load(f)\n    self._transform = transform_pipeline.steps[0][1]  # StandardScaler.\n\n  def predict_on_batch(self, inputs):\n    scores = self._model.predict_on_batch(inputs)\n    return self._transform.transform(scores)\n\n\nclass EnformerScoreVariantsPCANormalized:\n\n  def __init__(self, tfhub_url, transform_pkl_path,\n               organism='human', num_top_features=500):\n    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n      self._transform = joblib.load(f)\n    self._num_top_features = num_top_features\n\n  def predict_on_batch(self, inputs):\n    scores = self._model.predict_on_batch(inputs)\n    return self._transform.transform(scores)[:, :self._num_top_features]\n\n\n# TODO(avsec): Add feature description: Either PCX, or full names.\n\n\n# @title `variant_centered_sequences`\n\nclass FastaStringExtractor:\n\n    def __init__(self, fasta_file):\n        self.fasta = pyfaidx.Fasta(fasta_file)\n        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n    #import pd.Interval as Interval\n    def extract(self, interval: Interval, **kwargs) -&gt; str:\n        # Truncate interval if it extends beyond the chromosome lengths.\n        chromosome_length = self._chromosome_sizes[interval.chrom]\n        trimmed_interval = Interval(interval.chrom,\n                                    max(interval.start, 0),\n                                    min(interval.end, chromosome_length),\n                                    )\n        # pyfaidx wants a 1-based interval\n        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n                                          trimmed_interval.start + 1,\n                                          trimmed_interval.stop).seq).upper()\n        # Fill truncated values with N's.\n        pad_upstream = 'N' * max(-interval.start, 0)\n        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n        return pad_upstream + sequence + pad_downstream\n\n    def close(self):\n        return self.fasta.close()\n\n\ndef one_hot_encode(sequence):\n  return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)\n\n\n\n# @title `plot_tracks`\n\ndef plot_tracks(tracks, interval, height=1.5):\n  fig, axes = plt.subplots(len(tracks), 1, figsize=(20, height * len(tracks)), sharex=True)\n  for ax, (title, y) in zip(axes, tracks.items()):\n    ax.fill_between(np.linspace(interval.start, interval.end, num=len(y)), y)\n    ax.set_title(title)\n    sns.despine(top=True, right=True, bottom=True)\n  ax.set_xlabel(str(interval))\n  plt.tight_layout()\n\nHere, we define some utility functions for ourselves, to help us make predictions and analyse our predictions.\n\nimport Bio\n\nfrom Bio.Seq import Seq\ndef create_rev_complement(dna_string):\n    return(str(Seq(dna_string).reverse_complement()))\n\n\ndef prepare_for_quantify_prediction_per_TSS(predictions, gene, tss_df):\n\n  '''\n\n  Parameters:\n          predicitions (A numpy array): All predictions from the track\n          gene (a gene name, character): a gene\n          tss_df: a list of dataframe of genes and their transcription start sites\n  Returns:\n          A dictionary of cage experiment predictions and a list of transcription start sites\n\n  '''\n\n  output = dict()\n  for tdf in tss_df:\n    if gene not in tdf.genes.values:\n      continue\n    gene_tss_list = tdf[tdf.genes == gene].txStart_Sites.apply(str).values\n    gene_tss_list = [t.split(', ') for t in gene_tss_list]\n    gene_tss_list = [int(item) for nestedlist in gene_tss_list for item in nestedlist]\n    gene_tss_list = list(set(gene_tss_list))\n  output['cage_predictions'] = predictions[:, 5110] # a numpy array\n  output['gene_TSS'] = gene_tss_list # a list\n\n\n  return(output) # a dictionary\n\ndef quantify_prediction_per_TSS(low_range, TSS, cage_predictions):\n\n  '''\n  Parameters:\n          low_range (int): The lower interval\n          TSS (list of integers): A list of TSS for a gene\n          cage_predictions: A 1D numpy array or a vector of predictions from enformer corresponding to track 5110 or CAGE predictions\n  Returns:\n          A dictionary of gene expression predictions for each TSS for a gene\n    '''\n  tss_predictions = dict()\n  for tss in TSS:\n    bin_start = low_range + ((768 + 320) * 128)\n    count = -1\n    while bin_start &lt; tss:\n      bin_start = bin_start + 128\n      count += 1\n    if count &gt;= len(cage_predictions)-1:\n      continue\n    cage_preds = cage_predictions[count - 1] + cage_predictions[count] + cage_predictions[count + 1]\n    tss_predictions[tss] = cage_preds\n\n  return(tss_predictions)\n\ndef collect_intervals(chromosomes = [\"22\"], gene_list=None):\n\n  '''\n    Parameters :\n      chromosomes : a list of chromosome numbers; each element should be a string format\n      gene_list : a list of genes; the genes should be located on those chromosomes\n\n    Returns :\n      A dictionary of genes (from gene_list) and their intervals within their respective chromosomes\n  '''\n\n  gene_intervals = {} # Collect intervals for our genes of interest\n\n  for chrom in chromosomes:\n    with open(\"/home/s1mi/enformer_tutorial/gene_chroms/gene_\"+ chrom + \".txt\", \"r\") as chrom_genes:\n      for line in chrom_genes:\n        split_line = line.strip().split(\"\\t\")\n        gene_intervals[split_line[2]] = [\n                                          split_line[0],\n                                          int(split_line[3]),\n                                          int(split_line[4])\n                                        ]\n\n  if isinstance(gene_list, list): # if the user has supplied a list of genes they are interested in\n    use_genes = dict((k, gene_intervals[k]) for k in gene_list if k in gene_intervals)\n    return(use_genes)\n  elif isinstance(gene_list, type(None)):\n    return(gene_intervals)\n\n\ndef run_predictions(gene_intervals, tss_dataframe, individuals_list=None):\n  '''\n  Parameters :\n    gene_intervals : the results from calling `collect_intervals`\n    tss_dataframe : a list of the TSSs dataframes i.e. the TSS for the genes in the chromosomes\n    individuals_list : a list of individuals on which we want to make predictions; defaults to None\n\n  Returns :\n    A list of predictions; the first element is the predictions around the TSS for each gene. The second is the prediction across CAGE tracks\n  '''\n\n  gene_output = dict()\n  gene_predictions = dict()\n\n  for gene in gene_intervals.keys():\n    gene_interval = gene_intervals[gene]\n    target_interval = kipoiseq.Interval(\"chr\" + gene_interval[0],\n                                        gene_interval[1],\n                                        gene_interval[2]) # creates an interval to select the right sequences\n    target_fa = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))  # extracts the fasta sequences, and resizes such that it is compatible with the sequence_length\n    window_coords = target_interval.resize(SEQUENCE_LENGTH) # we also need information about the start and end locations after resizing\n    try:\n      cur_gene_vars = pd.read_csv(\"/home/s1mi/enformer_tutorial/individual_beds/chr\" + gene_interval[0] + \"/chr\" + gene_interval[0] + \"_\"+ gene + \".bed\", sep=\"\\t\", header=0) # read in the appropriate bed file for the gene\n    except:\n      continue\n    individual_results = dict()\n    individual_prediction = dict()\n\n    if isinstance(individuals_list, list) or isinstance(individuals_list, type(np.empty([1, 1]))):\n      use_individuals = individuals_list\n    elif isinstance(individuals_list, type(None)):\n      use_individuals = cur_gene_vars.columns[4:]\n\n    for individual in use_individuals:\n      print('Currently on gene {}, and predicting on individual {}...'.format(gene, individual))\n      # two haplotypes per individual\n      haplo_1 = list(target_fa[:])\n      haplo_2 = list(target_fa[:])\n\n      ref_mismatch_count = 0\n      for i,row in cur_gene_vars.iterrows():\n\n        geno = row[individual].split(\"|\")\n        if (row[\"POS\"]-window_coords.start-1) &gt;= len(haplo_2):\n          continue\n        if (row[\"POS\"]-window_coords.start-1) &lt; 0:\n          continue\n        if geno[0] == \"1\":\n          haplo_1[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n        if geno[1] == \"1\":\n          haplo_2[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n\n      # predict on the individual's two haplotypes\n      prediction_1 = model.predict_on_batch(one_hot_encode(\"\".join(haplo_1))[np.newaxis])['human'][0]\n      prediction_2 = model.predict_on_batch(one_hot_encode(\"\".join(haplo_2))[np.newaxis])['human'][0]\n\n      temp_predictions = [prediction_1[:, 5110], prediction_2[:, 5110]] # CAGE predictions we are interested in\n      individual_prediction[individual] = temp_predictions\n\n      # Calculate TSS CAGE expression which correspond to column 5110 of the predictions above\n      temp_list = list()\n\n      pred_prepared_1 = prepare_for_quantify_prediction_per_TSS(predictions=prediction_1, gene=gene, tss_df=tss_dataframe)\n      tss_predictions_1 = quantify_prediction_per_TSS(low_range = window_coords.start, TSS=pred_prepared_1['gene_TSS'], cage_predictions=pred_prepared_1['cage_predictions'])\n\n      pred_prepared_2 = prepare_for_quantify_prediction_per_TSS(predictions=prediction_2, gene=gene, tss_df=tss_dataframe)\n      tss_predictions_2 = quantify_prediction_per_TSS(low_range = window_coords.start, TSS=pred_prepared_2['gene_TSS'], cage_predictions=pred_prepared_2['cage_predictions'])\n\n      temp_list.append(tss_predictions_1)\n      temp_list.append(tss_predictions_2) # results here are a dictionary for each TSS for each haplotype\n\n      individual_results[individual] = temp_list # save for the individual\n\n    gene_output[gene] = individual_results\n    gene_predictions[gene] = individual_prediction\n\n  return([gene_output, gene_predictions])\n\n\ndef collect_target_intervals(gene_intervals):\n\n  '''\n  Returns a dictionary of Interval objects (from kipoiseq) for each gene corresponding to the locations of the gene\n  '''\n\n  target_intervals_dict = dict()\n\n  for gene in gene_intervals.keys():\n    gene_interval = gene_intervals[gene]\n    target_interval = kipoiseq.Interval(\"chr\" + gene_interval[0],\n                                        gene_interval[1],\n                                        gene_interval[2])\n    target_intervals_dict[gene] = target_interval\n\n  return(target_intervals_dict)\n\ndef prepare_for_plot_tracks(gene, individual, all_predictions, chromosome=['22']):\n\n  '''\n  This returns a dictionary of gene tracks and gene intervals, prepared for the function plot_tracks.\n\n  Parameters:\n    - gene\n    - individual\n    - all_predictions\n  '''\n\n  haplo_predictions = all_predictions[gene][individual]\n  gene_tracks = {gene + ' | ' + individual + ' | haplotype 1': np.log10(1 + haplo_predictions[0]),\n                gene + ' | ' + individual + ' | haplotype 2': np.log10(1 + haplo_predictions[1])}\n\n  gene_intervals = collect_intervals(chromosomes=chromosome, gene_list=[gene])\n  gene_intervals = collect_target_intervals(gene_intervals)\n\n  output = dict()\n  output['gene_tracks'] = gene_tracks\n  output['gene_intervals'] = gene_intervals[gene]\n\n  return(output)\n\ndef check_individuals(path_to_bed_file, list_of_individuals):\n\n  '''\n  Checks if an individual is missing in bed variation files.\n  These individuals should be removed prior to training\n  '''\n\n  myfile = open(path_to_bed_file, 'r')\n  myline = myfile.readline()\n  bed_names = myline.split('\\t')[4:]\n  myfile.close()\n\n  if set(list_of_individuals).issubset(set(bed_names)) == False:\n    missing = list(set(list_of_individuals).difference(bed_names))\n    print('This (or these) individual(s) is/are not present: {}'.format(missing))\n  else:\n    missing = []\n    print('All individuals are present in the bed file.')\n\n  return(missing)\n\n\ndef plot_predixcan_vs_geuvadis(interested_gene, interested_individuals, geuvadis_expression, predixcan_expression):\n\n  '''\n  Show a plot and return correlation coefficient\n  '''\n  # from predixcan expression\n  df_predixcan = predixcan_expression[predixcan_expression.gene_name == interested_gene].loc[:,interested_individuals]\n  # from enformer\n  df_geuvadis = geuvadis_expression[geuvadis_expression.gene_name == interested_gene].loc[:,interested_individuals]\n\n  # concatenate both\n  df_all = pd.concat([df_predixcan, df_geuvadis], axis=0)\n  df_all.index = ['Predixcan', 'GEUVADIS']\n\n  # plotting\n  sns.regplot(x=df_all.iloc[0,:], y=df_all.iloc[1,:], color='red').set(title='Predixcan vs. GEUVADIS predictions on {} individuals for gene {}'.format(len(df_all.columns), interested_gene))\n\n  # correlation coefficient\n  corr_coef = np.corrcoef(x=df_all.iloc[0,:], y=df_all.iloc[1,:])[0][1]\n\n  return([df_all, corr_coef])\n\ndef plot_enformer_vs_predixcan(prediction_results, interested_gene, interested_individuals, predixcan_expression, how='sum'):\n\n  '''\n  Show a plot and return correlation coefficient\n  '''\n\n  enformer_predictions = dict()\n\n  for gene, individuals in prediction_results[0].items():\n    temp_individual = dict()\n    for individual, haplo_predictions in individuals.items():\n      temp = list()\n      for i in range(0, len(haplo_predictions[0])):\n        temp.append(list(haplo_predictions[0].values())[i] + list(haplo_predictions[1].values())[i])\n      if how == 'sum':\n        temp_individual[individual] = np.sum(temp)\n      elif how == 'max':\n        temp_individual[individual] = np.max(temp)\n    enformer_predictions[gene] = temp_individual\n\n  # from predixcan expression\n  df_predixcan = predixcan_expression[predixcan_expression.gene_name == interested_gene].loc[:,interested_individuals]\n  # from enformer\n  df_enformer = pd.DataFrame(enformer_predictions[interested_gene], index=[0]).loc[:, df_predixcan.columns]\n\n  # concatenate both\n  df_all = pd.concat([df_enformer, df_predixcan], axis=0)\n  df_all.index = ['Enformer', 'Predixcan']\n\n  # plotting\n  sns.regplot(x=df_all.iloc[0,:], y=df_all.iloc[1,:], color='red').set(title='Predixcan vs. Enformer predictions on {} individuals for gene {}'.format(len(df_all.columns), interested_gene))\n\n  # correlation coefficient\n  corr_coef_predix = np.corrcoef(x=df_all.iloc[0,:], y=df_all.iloc[1,:])[0][1]\n\n  return([df_all, corr_coef_predix])\n\n\ndef plot_enformer_vs_geuvadis(prediction_results, interested_gene, interested_individuals, geuvadis_expression, how='sum'):\n\n  '''\n  Show a plot and return correlation coefficient\n  '''\n\n  enformer_predictions = dict()\n\n  for gene, individuals in prediction_results[0].items():\n    temp_individual = dict()\n    for individual, haplo_predictions in individuals.items():\n      temp = list()\n      for i in range(0, len(haplo_predictions[0])):\n        temp.append(list(haplo_predictions[0].values())[i] + list(haplo_predictions[1].values())[i])\n      if how == 'sum':\n        temp_individual[individual] = np.sum(temp)\n      elif how == 'max':\n        temp_individual[individual] = np.max(temp)\n    enformer_predictions[gene] = temp_individual\n\n  # from geuvadis expression\n  df_geuvadis = geuvadis_expression[geuvadis_expression.gene_name == interested_gene].loc[:,interested_individuals]\n  #df_enformer = np.transpose(pd.DataFrame(enformer_predictions)).loc[:, df_geuvadis.columns]\n  df_enformer = pd.DataFrame(enformer_predictions[interested_gene], index=[0]).loc[:, df_geuvadis.columns]\n\n  # concatenate both\n  df_all = pd.concat([df_enformer, df_geuvadis], axis=0)\n  df_all.index = ['Enformer', 'GEUVADIS']\n\n  # plotting\n  sns.regplot(x=df_all.iloc[0,:], y=df_all.iloc[1,:], color='blue').set(title='Enformer vs. Geuvadis predictions on {} individuals for gene {}'.format(len(df_all.columns), interested_gene))\n\n  # correlation coefficient\n  corr_coef_geu = np.corrcoef(x=df_all.iloc[0,:], y=df_all.iloc[1,:])[0][1]\n\n  return([df_all, corr_coef_geu])"
  },
  {
    "objectID": "posts/hackathon-enformer-usage/Hackathon_enformer_usage_participant.html#make-predictions-on-the-geuvadis-dataset.",
    "href": "posts/hackathon-enformer-usage/Hackathon_enformer_usage_participant.html#make-predictions-on-the-geuvadis-dataset.",
    "title": "GENETIC MEDICINE DEEP LEARNING HACKATHON 2022",
    "section": "Make predictions on the GEUVADIS dataset.",
    "text": "Make predictions on the GEUVADIS dataset.\nHere, we will begin to make predictions. Excited?!\nWe still need the model itself. The model has been graciously hosted on Tensorflow Hub, which hosts many other models too. You can click on the link and explore. When you click the link, you can see that the model is about 892 Mb large. Quite big. We will use the url to the model to download and use it here.\nEarlier, we defined an Enformer class (see the codes section). We will load the model into this class. The model has been trained and the weights are freely available. All we need to do is to load this model and use it. Neat.\nWe also defined a class FastaStringExtractor, that can help us extract raw sequences from fasta files given the intervals we want. We will make use of this class too.\n\nmodel = Enformer(model_path) # here we load the model architecture.\n\nfasta_extractor = FastaStringExtractor(fasta_file) # we define a class called fasta_extractor to help us extra raw sequence data\n\n\nEXERCISE 1:\nFor evaluation, we need to sum the predictions around each unique TSS for a given gene. We will be using this a lot so it is important that we define what it means. Essentially, for a gene with one TSS, we take the sum of predicitions of the 128 bp output bin containing the TSS and its two immediate neighboring bins. We do this for each haplotype and each TSS to give TSS-level predictions.\nTo get individual-level estimates for a whole gene, we sum each haplotype TSS estimate to summarize TSS-level predictions per individual, and then take either the sum or max of TSS-level predictions to summarize at the gene level.\nThere are many genes and many individuals in our datasets. To make illustration simpler, we will use four genes, ERAP1, ERAP2, NUDT2, and PEX6, located on chromosome 5, 5, 9, and 6 respectively. We will use predictions for 10 randomly selected individuals located in the bed files.\n\ndownload_chrom_beds(chromosome = \"5\", genes = ['ERAP1', 'ERAP2'])\ndownload_chrom_beds(chromosome = \"9\", genes = ['NUDT2'])\ndownload_chrom_beds(chromosome = \"6\", genes = ['PEX6'])\n\nHere, we read into a dataframe the TSS (transcription start sites) per gene for the chromosomes we are interested in. The dataframe has three columns. The first contains the genes, and the second contains the TSS(s) for that gene, and the third contains the strand information. We are interested in genes located on chromosomes 5, 6 and 9.\n\nchr5_tss = pd.read_table('/home/s1mi/enformer_tutorial/tss_by_chr/chr5_tss_by_gene.txt', sep='\\t')\nchr6_tss = pd.read_table('/home/s1mi/enformer_tutorial/tss_by_chr/chr6_tss_by_gene.txt', sep='\\t')\nchr9_tss = pd.read_table('/home/s1mi/enformer_tutorial/tss_by_chr/chr9_tss_by_gene.txt', sep='\\t')\n\nchr9_tss.head(10)\n\n\nPreparing inputs for Enformer\nNow that we have downloaded the genetic information that we need, we want to prepare the inputs for Enformer.\nWe need the following - The genes we want to predict for - The genomic interval for these genes - Information about the transcription start sites for these genes - The individuals we want to predict for\nWe have a utility function that helps to define the intervals of a gene, and resize this interval to make it acceptable for Enformer. Enformer needs a specific, defined sequence length. We use the collect_intervals function. The result is a dictionary that contains chromosome and interval information for each gene.\nFor example, let’s explore ERAP1…\n\nERAP1_intervals = collect_intervals(chromosomes=['5'], gene_list=['ERAP1'])\nERAP1_target_intervals = collect_target_intervals(ERAP1_intervals)\nERAP1_intervals, ERAP1_target_intervals\n\nERAP1_target_intervals is an Interval object created using the kipoiseq package we installed earlier. It is used during predictions, and we don’t need to know the methods of this object for the purpose of the next questions.\nHowever, we have similar information in ERAP1_intervals, which is a python dictionary of lists. For the questions below, we will use the ERAP1_intervals object.\n\n\nQuestion 1a\nWhat is the size of this interval? Hint: Look at the ERAP1_intervals, and remember that Python is 0-based indexed. You need to access the key of this dictionary, which is the gene name, and for the value, which is a list, you can access the first element using 0, the second element using 1, and so on.\n\nERAP1_intervals['ERAP1'][2] - ERAP1_intervals['ERAP1'][1] # your answers go in the ...\n\n\n\nNote\nYou can roughly confirm this interval by going to the UCSC genome browser or Ensemble genome browser. We have provided a link for UCSC genome browser’s interval length for ERAP1 here. Click on this link, the answer is right at the top of the browser.\nEnformer takes in a defined sequence length. When we provide a gene and collect its intervals, we need to resize this interval to be acceptable for Enformer. Here, we will use the Intervals object define earlier, ERAP1_target_intervals.\n\nERAP1_target_interval_resized = ERAP1_target_intervals['ERAP1'].resize(SEQUENCE_LENGTH)\nERAP1_target_interval_resized\n\n\n\nQuestion 1b\nWhat is the length of this interval? Simply run the next line of code.\n\nERAP1_target_interval_resized.end - ERAP1_target_interval_resized.start\n\nEssentially, we resized the length of the gene and pad it with the native sequences to the left and to the right, such that the length of the input sequence is 393216, and we can imagine our gene right at the center of this wider interval. This is the same interval length used to train ENCODE data to build Enformer. Since this value is pre-define, we really cannot change it. This is information that Enformer uses to make very good predictions. Below, we confirm that this is true.\n\n(ERAP1_target_interval_resized.end - ERAP1_target_interval_resized.start) == SEQUENCE_LENGTH\n\n\n\nMaking predictions with Enformer\nWe will select 10 individuals (we have provided 10 randomly sample individuals for ease), and use four genes, ERAP1, ERAP2, NUDT2, and PEX6, located on chromosome 5, 5, 9, and 6 respectively\nWe will collect the intervals that correspond to these genes, collect the sequences for that interval from the reference fasta file, loop through each individual’s variations in the bed files we provided, switch around the variations for each haplotype and predict expression.\nEventually, for each individual, we should have predictions corresponding to each haplotype. We expect that since the haplotypes are different, the predictions should vary too.\nAdditionally, we need the TSS for these genes. Remember that we read in the dataframe earlier.\n\nexercise_1_genes = ['ERAP1', 'NUDT2', 'ERAP2', 'PEX6'] # our gene of interest\n#exercise_1_gene = ['NUDT2', 'ERAP2'] # our gene of interest\n\nexercise_1_individuals = ['NA11992', 'NA19235', 'NA20770', 'HG00232', 'HG00342', 'NA20502', 'NA19189', 'HG00108', 'HG00380', 'NA12872'] # individuals we are interested in\n\nexercise_1_chromosomes = ['5', '9', '6'] # the gene is on chromosome 5\n\nexercise_1_tss_dfs = [chr5_tss, chr9_tss, chr6_tss] # we use the TSS information\n\n\n\nQUESTION 2\nWhat is the id of the 8th individual? Hint: Python used 0-based indexing\n\nprint('The 8th individual is {}'.format(exercise_1_individuals[7])) # your code goes into the ellipsis\n\nIt is possible to have individuals not present in our variation bed files for some reasons. So, we will do some sanity checks.\nUsing the check_individuals functions, we will check if all these individuals are present in the bed file for that gene.\n\nmissing_1 = check_individuals(\"/home/s1mi/enformer_tutorial/individual_beds/chr9/chr9_NUDT2.bed\", list_of_individuals = exercise_1_individuals)\nmissing_2 = check_individuals(\"/home/s1mi/enformer_tutorial/individual_beds/chr5/chr5_ERAP2.bed\", list_of_individuals = exercise_1_individuals)\nmissing_3 = check_individuals(\"/home/s1mi/enformer_tutorial/individual_beds/chr5/chr5_ERAP1.bed\", list_of_individuals = exercise_1_individuals)\nmissing_4 = check_individuals(\"/home/s1mi/enformer_tutorial/individual_beds/chr6/chr6_PEX6.bed\", list_of_individuals = exercise_1_individuals)\nmissing_1, missing_2, missing_3, missing_4\n\nIt looks like all the individuals are present. Very nice! We are good to go.\nTo make predictions, we first collect the intervals for the genes we want to predict for.\n\nexercise_1_interval = collect_intervals(chromosomes=exercise_1_chromosomes, gene_list=exercise_1_genes) # here, we collect the intervals for that gene\nexercise_1_interval\n\nNext, we use the run_predictions function\n\nexercise_1_predictions = run_predictions(gene_intervals=exercise_1_interval, tss_dataframe=exercise_1_tss_dfs, individuals_list=exercise_1_individuals) # here we make predictions and save it.\n\nNB: If you intend to make predictions across many individuals and genes, it will be faster if you have larger GPU access. For now, we are using limited GPU. So, we have to limit our predictions.\nQuite fast right? Very nice.\nOur prediction object, exercise_1_predictions is a list of length two. - The first item in the list corresponds to the sum of predictions around each unique TSS, for each haplotype, for each individual, for each gene.\n\nThe second item in the list corresponds to the CAGE:B lymphoblastoid cell line predictions across all 128bp bins for each haplotype, for each individual, for the genes. We will use the second item for plotting the tracks.\n\nLet us take a look at the object.\n\nprint(\"The exercise_1_predictions object is a {} of length {}.\".format(type(exercise_1_predictions).__name__, len(exercise_1_predictions)))\n\n\nexercise_1_predictions[0]\n\n\n\nPlotting the CAGE:B lymphoblastoid cell line tracks\nNext, we will plot the tracks. We have already defined two helper functions, prepare_for_plot_tracks and plot_tracks to plot the expression along the TSS for a gene, for an individual and for each haplotype.\nFor NUDT2…\n\ntemp = prepare_for_plot_tracks(gene=exercise_1_genes[1], individual=exercise_1_individuals[0], all_predictions=exercise_1_predictions[1], chromosome=['9'])\nplot_tracks(tracks=temp['gene_tracks'], interval=temp['gene_intervals'])\n\nLooks nice!\nAlthough it looks like there is no variation in the predictions for the haplotypes, we can take a look at the actual prediction values across the TSS.\nThe columns are the transcription start sites, and the rows are the haplotypes for the individual. The entries are the sum of the predictions at the TSS, at TSS - 1, and at the TSS + 1.\nWe will look at the first individual, NA11992, for NUDT2…\n\npd.DataFrame(exercise_1_predictions[0][exercise_1_genes[1]][exercise_1_individuals[0]], index=['haplotype_1', 'haplotype_2'])\n\nWe will look at the first individual, NA11992, for PEX6…\n\npd.DataFrame(exercise_1_predictions[0][exercise_1_genes[3]][exercise_1_individuals[0]], index=['haplotype_1', 'haplotype_2'])\n\nMerely looking at the values, it looks like there are variations in the predictions across the haplotypes and the TSS. We expected some variations because we are predicting expression for each haplotype, which tend to have variations in them. Very nice!"
  },
  {
    "objectID": "posts/hackathon-enformer-usage/Hackathon_enformer_usage_participant.html#comparing-with-true-expression-from-geuvadis-and-with-predixcan",
    "href": "posts/hackathon-enformer-usage/Hackathon_enformer_usage_participant.html#comparing-with-true-expression-from-geuvadis-and-with-predixcan",
    "title": "GENETIC MEDICINE DEEP LEARNING HACKATHON 2022",
    "section": "Comparing with true expression from GEUVADIS and with Predixcan",
    "text": "Comparing with true expression from GEUVADIS and with Predixcan\nWe should read in the GEUVADIS and Predixcan predictions.\n\ngeuvadis_gene_expression = pd.read_table('https://uchicago.box.com/shared/static/5vwc7pjw9qmtv7298c4rc7bcuicoyemt.gz', sep='\\t',\n                                         dtype={'gene_id': str, 'gene_name':str, 'TargetID':str, 'Chr':str})\ngeuvadis_gene_expression.head(5)\n\n\npredixcan_gene_expression = pd.read_table('https://uchicago.box.com/shared/static/4k68u7x7rxjpoljfdva6qipjxwzd3l0g.txt', sep=' ')\npredixcan_gene_expression.head(5)\n\n\nQUESTION 3a\nWhat is the dimension/size/shape of the geuvadis_gene_expression dataframe? Hint: You can use the .shape method on a dataframe.\n\ngeuvadis_dimension = geuvadis_gene_expression.shape\n#print(\"The geuvadis_gene_expression dataframe has {} rows and {} columns\".format(*geuvadis_dimension))\n\n\n\nQUESTION 4b\nWhat is the dimension/size/shape of the predixcan_gene_expression dataframe? Hint: You can use the .shape method on a dataframe.\n\npredixcan_dimension = predixcan_gene_expression.shape\nprint(\"The predixcan_gene_expression dataframe has {} rows and {} columns\".format(*predixcan_dimension))\n\nWe select the individuals and the gene from the geuvadis_gene_expression dataframe.\n\nerap1_geuvadis_expression = geuvadis_gene_expression[geuvadis_gene_expression.gene_name == exercise_1_genes[0]].loc[:,exercise_1_individuals]\nnudt2_geuvadis_expression = geuvadis_gene_expression[geuvadis_gene_expression.gene_name == exercise_1_genes[1]].loc[:,exercise_1_individuals]\nerap2_geuvadis_expression = geuvadis_gene_expression[geuvadis_gene_expression.gene_name == exercise_1_genes[2]].loc[:,exercise_1_individuals]\npex6_geuvadis_expression = geuvadis_gene_expression[geuvadis_gene_expression.gene_name == exercise_1_genes[3]].loc[:,exercise_1_individuals]\n\n\nnudt2_geuvadis_expression\n\nWe will sum the prediction for both haplotypes for each TSS, and take the sum of the resulting values. The function used here can also take the max instead of the sums.\nWe have 3 utility functions to help us - plot_enformer_vs_guevadis - plot_predixcan_vs_geuvadis - plot_enformer_vs_predixcan (if you think this is necessary)\n\nerap1_vs_geu = plot_enformer_vs_geuvadis(prediction_results=exercise_1_predictions, geuvadis_expression=geuvadis_gene_expression,\n                            interested_gene=exercise_1_genes[0], interested_individuals=exercise_1_individuals, how='sum')\nprint('Correlation coefficient: {}'.format(erap1_vs_geu[1]))\n\n\npex6_vs_geu = plot_enformer_vs_geuvadis(prediction_results=exercise_1_predictions, geuvadis_expression=geuvadis_gene_expression,\n                            interested_gene=exercise_1_genes[3], interested_individuals=exercise_1_individuals, how='sum')\n\nprint('Correlation coefficient: {}'.format(pex6_vs_geu[1]))\n\n\nnudt_vs_geu = plot_enformer_vs_geuvadis(prediction_results=exercise_1_predictions, geuvadis_expression=geuvadis_gene_expression,\n                            interested_gene=exercise_1_genes[1], interested_individuals=exercise_1_individuals, how='sum')\n\nprint('Correlation coefficient: {}'.format(nudt_vs_geu[1]))\n\n\nerap2_vs_geu = plot_enformer_vs_geuvadis(prediction_results=exercise_1_predictions, geuvadis_expression=geuvadis_gene_expression,\n                            interested_gene=exercise_1_genes[2], interested_individuals=exercise_1_individuals, how='sum')\n\nprint('Correlation coefficient: {}'.format(erap2_vs_geu[1]))\n\nNow, we can see how Predixcan performs on these individuals\n\nerap1_predix = plot_predixcan_vs_geuvadis(interested_gene=exercise_1_genes[0], interested_individuals=exercise_1_individuals, geuvadis_expression=geuvadis_gene_expression, predixcan_expression=predixcan_gene_expression)\nprint('The correlation coefficient: {}'.format(erap1_predix[1]))\n\n\npex6_predix = plot_predixcan_vs_geuvadis(interested_gene=exercise_1_genes[3], interested_individuals=exercise_1_individuals, geuvadis_expression=geuvadis_gene_expression, predixcan_expression=predixcan_gene_expression)\nprint('The correlation coefficient: {}'.format(pex6_predix[1]))\n\n\nerap2_predix = plot_predixcan_vs_geuvadis(interested_gene=exercise_1_genes[2], interested_individuals=exercise_1_individuals, geuvadis_expression=geuvadis_gene_expression, predixcan_expression=predixcan_gene_expression)\nprint('The correlation coefficient: {}'.format(erap2_predix[1]))\n\n\nnudt2_predix = plot_predixcan_vs_geuvadis(interested_gene=exercise_1_genes[1], interested_individuals=exercise_1_individuals, geuvadis_expression=geuvadis_gene_expression, predixcan_expression=predixcan_gene_expression)\nprint('The correlation coefficient: {}'.format(nudt2_predix[1]))\n\nQuite neat and impressive!"
  },
  {
    "objectID": "posts/hackathon-enformer-usage/Hackathon_enformer_usage_participant.html#exercise-2",
    "href": "posts/hackathon-enformer-usage/Hackathon_enformer_usage_participant.html#exercise-2",
    "title": "GENETIC MEDICINE DEEP LEARNING HACKATHON 2022",
    "section": "EXERCISE 2",
    "text": "EXERCISE 2\nIn this exercise, you will get your hands dirty, and run Enformer on your gene(s) of interest.\n\nSelect your favorite gene(s). Note that the more genes you use, the longer it will take to run.\nRandomly select 10 individuals, just because we don’t have all the computational power.\nRun predictions\n\nWe only have data for a finite set of genes (sorry!). Here is a list of available genes you can use:\n\n!curl -L https://uchicago.box.com/shared/static/x8d7dx1ykefz49ep6sxot42v44sfvcv5.tsv --output /home/s1mi/enformer_tutorial/all_genes.tsv\n\nwith open(\"/home/s1mi/enformer_tutorial/all_genes.tsv\", \"r\") as ag:\n  all_genes = [line.strip() for line in ag]\nprint(len(all_genes))\nprint(\"First 5 genes all_genes:\", all_genes[0:5])\n\n\nSelect your genes\n\n\nmy_genes = ['ATAD3A']\nprint(\"My gene(s) is/are {}\".format(', '.join(my_genes)))\n\n\nRead in the TSS txt files where those chromosome are located. If you have genes located on more than one chromosome, copy the pd.read_table line for each chromosome you have, and replace the chromosome number (ellipses) as appropriate.\n\n\nmy_chromosomes = ['1'] # put in the chromosomes where the genes are located. Just the numbers will do, or you can put them in as a string type\n\n\nmy_tss_list = []\nfor chr in my_chromosomes:\n  chr = str(chr)\n  bed_file = '/home/s1mi/enformer_tutorial/tss_by_chr/chr{}_tss_by_gene.txt'.format(chr)\n  my_tss_list.append(pd.read_table(bed_file, sep='\\t')) # we read in the TSSs for each chromosome, and put them into a list\n\n\nRandomly select 10 individuals\n\n\n# let us set a seed to randomly select 10 individuals\nnp.random.seed(2021)  # replace ... with an integer you want\n\nnumber_of_individuals = 10\n\nmy_individuals = np.random.choice(a=geuvadis_gene_expression.columns[6:-1], size=number_of_individuals, replace=False) # individuals we are interested in\nmy_individuals\n\n\nWe want to make sure that we have complete variation information for all 10 individuals.\n\nFirst, we need to download the variation bed files for these individuals\n\ndownload_chrom_beds(chromosome='1', genes=my_genes) # remember that the genes should be on that chromosome, and you can use this code for each chromosome you have.\n\nRead in the variation bed files\n\nimport os\n\nmy_missing_list = list()\nfor chr in my_chromosomes:\n  for gene in my_genes:\n    chr = str(chr)\n    file_path = '/home/s1mi/enformer_tutorial/individual_beds/chr' + chr + '/chr' + chr + '_' + gene + '.bed'\n    if not os.path.exists(file_path):\n      continue\n    my_missing_list.append(check_individuals(file_path, my_individuals))\n\n\nQUESTION 4\nAre there missing individuals? All answers, based on your results are correct. If there are missing individuals, can you remove them? You can add new code blocks as you like.\n\nmy_missing_list\n\nIt looks like we are almost set to make predictions.\n\nMake predictions. First, we will collect the intervals for the genes we want, check the object and make sure we are on the right track. Next, we will call our run_predictions function.\n\n\nmy_intervals = collect_intervals(chromosomes= my_chromosomes, gene_list= my_genes) # here, we collect the intervals for that gene; replace ... with the right objects\nmy_intervals\n\n\nmy_predictions = run_predictions(gene_intervals= my_intervals, tss_dataframe= my_tss_list, individuals_list=my_individuals)\n\nAt this point, we will leave you to make your own plots…\n\natad3a_geuvadis_expression = geuvadis_gene_expression[geuvadis_gene_expression.gene_name == my_genes[0]].loc[:,my_individuals]\natad3a_vs_geu = plot_enformer_vs_geuvadis(prediction_results=my_predictions, geuvadis_expression=geuvadis_gene_expression, \n                            interested_gene=my_genes[0], interested_individuals=my_individuals, how='sum')\nprint('Correlation coefficient: {}'.format(atad3a_vs_geu[1]))\n\n\natad3a_predix = plot_predixcan_vs_geuvadis(interested_gene=my_genes[0], interested_individuals=my_individuals, geuvadis_expression=geuvadis_gene_expression, predixcan_expression=predixcan_gene_expression)\nprint('The correlation coefficient: {}'.format(atad3a_predix[1]))"
  },
  {
    "objectID": "posts/hackathon-enformer-usage/Hackathon_enformer_usage_participant.html#average-haplotype-vs-average-epigenome",
    "href": "posts/hackathon-enformer-usage/Hackathon_enformer_usage_participant.html#average-haplotype-vs-average-epigenome",
    "title": "GENETIC MEDICINE DEEP LEARNING HACKATHON 2022",
    "section": "Average Haplotype vs Average Epigenome",
    "text": "Average Haplotype vs Average Epigenome\nGet ERAP2 interval:\n\nERAP2_intervals = collect_intervals(chromosomes=['5'], gene_list=['ERAP2'])\nERAP2_target_intervals = collect_target_intervals(ERAP2_intervals)\nERAP2_intervals, ERAP2_target_intervals\n\nSelect random individual\n\nrand_individual = np.random.choice(a=geuvadis_gene_expression.columns[6:-1], size=1, replace=False) # individuals we are interested in\nrand_individual\n\nRun predictions (returns average epigenome):\n\nmy_predictions = run_predictions(gene_intervals= my_intervals, tss_dataframe= my_tss_list, individuals_list=my_individuals)"
  },
  {
    "objectID": "posts/polaris-intro/index.html",
    "href": "posts/polaris-intro/index.html",
    "title": "Getting started on Polaris",
    "section": "",
    "text": "More info can be found on:\n\nMain Page\nExample Job Scripts"
  },
  {
    "objectID": "posts/polaris-intro/index.html#references",
    "href": "posts/polaris-intro/index.html#references",
    "title": "Getting started on Polaris",
    "section": "",
    "text": "More info can be found on:\n\nMain Page\nExample Job Scripts"
  },
  {
    "objectID": "posts/polaris-intro/index.html#shortcuts",
    "href": "posts/polaris-intro/index.html#shortcuts",
    "title": "Getting started on Polaris",
    "section": "Shortcuts",
    "text": "Shortcuts\n\nLogin: ssh s1mi@polaris.alcf.anl.gov\nMain project storage: /lus/grand/projects/TFXcan\nInteractive Job: qsub -I -A TFXcan -l select=1 -l filesystems=home:grand -l walltime=1:00:00 -q debug"
  },
  {
    "objectID": "posts/polaris-intro/index.html#basic-job-script",
    "href": "posts/polaris-intro/index.html#basic-job-script",
    "title": "Getting started on Polaris",
    "section": "Basic Job Script",
    "text": "Basic Job Script\n#!/bin/bash\n#PBS -A $PROJECT\n#PBS -lwalltime=01:00:00\n#PBS -lselect=4\n#PBS -lsystem=polaris\n#PBS -lfilesystems=home:eagle\n\nrpn=4 # assume 1 process per GPU\nprocs=$((PBS_NODES*rpn))\n\n# job to “run” from your submission directory\ncd $PBS_O_WORKDIR\n\nmodule load &lt;something&gt;\n\nset +x # report all commands to stderr\nenv\nmpiexec -n $procs -ppn $rpn --cpu-bind core -genvall ./bin &lt;opts&gt;"
  },
  {
    "objectID": "posts/polaris-intro/index.html#conda-environments",
    "href": "posts/polaris-intro/index.html#conda-environments",
    "title": "Getting started on Polaris",
    "section": "Conda Environments",
    "text": "Conda Environments\nBefore activating and deactivating environments, we need to run module load conda.\nTo create an environment:\nmodule load conda\n\nexport http_proxy=http://proxy.alcf.anl.gov:3128\nexport https_proxy=$http_proxy\n\nconda create -n ml-python python=3.10\nconda activate ml-python\nconda install -y jupyter nb_conda ipykernel mpi\nconda install -y pytorch torchvision -c pytorch\nconda install -c conda-forge tensorflow\npython -m ipykernel install --user --name ml-python\nThe last line creates a new directory ~/.local/share/jupyter/kernels/ml-python. We add a couple lines to kernel.json so that we can use the new environment in the our Jupyter notebooks:\n{\n  \"argv\": [\n    \"/home/s1mi/.conda/envs/ml-python/bin/python\",\n    \"-m\",\n    \"ipykernel_launcher\",\n    \"-f\",\n    \"{connection_file}\"\n  ],\n  \"display_name\": \"ml-python\",\n  \"language\": \"python\",\n  \"env\": {\n    \"CONDA_PREFIX\": \"/home/s1mi/.conda/envs/ml-python\",\n    \"PATH\": \"/home/s1mi/.conda/envs/ml-python\",\n    \"http_proxy\": \"http://proxy.alcf.anl.gov:3128\",\n    \"https_proxy\": \"http://proxy.alcf.anl.gov:3128\"\n  },\n  \"metadata\": {\n    \"debugger\": true\n  }\n}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "deep-learning-in-genomics",
    "section": "",
    "text": "GENETIC MEDICINE DEEP LEARNING HACKATHON 2022\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nCheck predictions for a single individual, region, and haplotype\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nComparing Enformer Prediction Averages\n\n\n\n\n\nWe compare two methods of predicting the epigenome of an individual: the first was generated by running enformer on both haplotypes, then averaging the results, and the second by running enformer on the average of the haplotypes.\n\n\n\n\n\n\nJul 20, 2023\n\n\nSabrina Mi\n\n\n\n\n\n\n  \n\n\n\n\nA test run of the Enformer pipeline on Palmer Lab rat data\n\n\n\n\n\nThis post will outline the steps I took to prepare rat data for the enformer pipeline.\n\n\n\n\n\n\nJul 20, 2023\n\n\nSabrina Mi\n\n\n\n\n\n\n  \n\n\n\n\nGetting started on Polaris\n\n\n\n\n\nA couple snippets on the Polaris computing system\n\n\n\n\n\n\nJun 26, 2023\n\n\nSabrina Mi\n\n\n\n\n\n\n  \n\n\n\n\nWeek 1\n\n\n\n\n\nThis week we learned the basics of VS Code, Python, and NumPy.\n\n\n\n\n\n\nJun 23, 2023\n\n\nSabrina Mi\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nOverview of LLM in Molecular Biology article by Serafim Batzoglou\n\n\n\n\n\nWe’ll review key points from each section of Large Language Models in Molecular Biology\n\n\n\n\n\n\nJun 20, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/comparing-enformer-predictions-in-overlapping-intervals/results.html",
    "href": "posts/comparing-enformer-predictions-in-overlapping-intervals/results.html",
    "title": "Check predictions for a single individual, region, and haplotype",
    "section": "",
    "text": "# Import Libraries\nimport numpy as np\nimport h5py\n\n\npredictions_dir = \"/lus/grand/projects/TFXcan/imlab/users/sabrina/enformer_test/predictions_folder/reference_enformer_minimal_chr2_region/predictions_2023-07-20/enformer_predictions/reference_enformer_minimal/haplotype0/\"\ninterval = \"chr2_96267973_96382661\"\nf = h5py.File(predictions_dir + interval +'_predictions.h5', 'r')\nf[interval][()]\n\narray([[1.2541825e-01, 1.9915380e-01, 1.5378954e-01, ..., 8.7608267e-03,\n        2.8164219e-02, 3.3774886e-02],\n       [5.5592204e-03, 8.4977224e-03, 8.1731612e-03, ..., 1.4041504e-03,\n        5.0366633e-03, 4.5777112e-03],\n       [1.8070938e-02, 2.4811333e-02, 2.0492842e-02, ..., 1.0022735e-02,\n        3.5357766e-02, 2.8489195e-02],\n       ...,\n       [4.7709393e-05, 6.7506859e-05, 5.9531834e-05, ..., 3.9953634e-04,\n        1.4891241e-03, 1.1986542e-03],\n       [4.6880776e-04, 6.4554013e-04, 5.7156256e-04, ..., 1.3358767e-04,\n        4.6027626e-04, 2.6385681e-04],\n       [7.9046481e-04, 1.1395547e-03, 9.8084495e-04, ..., 1.1925452e-03,\n        3.0541462e-03, 2.2076559e-03]], dtype=float32)"
  },
  {
    "objectID": "posts/week-1/index.html",
    "href": "posts/week-1/index.html",
    "title": "Week 1",
    "section": "",
    "text": "Here’s my notes from the w3 NumPy tutorial:"
  },
  {
    "objectID": "posts/week-1/index.html#data-types-in-numpy",
    "href": "posts/week-1/index.html#data-types-in-numpy",
    "title": "Week 1",
    "section": "Data Types in Numpy:",
    "text": "Data Types in Numpy:\nBy default, Python has strings, integer, float, boolean, and complex. NumPy has extra data types:\n\ni: integer\nb: boolean\nu: unsigned integer\nf: float\nc: complex float\nm: timedelta\nM: datetime\nO: object\nS: string\nU: unicode string\nV: void"
  },
  {
    "objectID": "posts/test-run-of-enformer-pipeline-on-rat-data/index.html",
    "href": "posts/test-run-of-enformer-pipeline-on-rat-data/index.html",
    "title": "A test run of the Enformer pipeline on Palmer Lab rat data",
    "section": "",
    "text": "All of the data is publicly available\n\nReference Genome\nGenotypes\nGene Expression\n\nFirst, I checked that enformer could run on the rat data in python using code provided by deepmind.\n\n000789972A_chr1_1001138_1014540.py: Personalized enformer run on individual 000789972A on chromosome 1, 1001138 to 1014540.\nEnformerVCF.py: Helper functions provided by deepmind along with some VCF handling functions.\n\nNext, I needed to process the data for the pipeline. This included splitting the VCF by chromosome, indexing the new VCFs, and creating a list of individuals.\nI wrote my config json (/lus/grand/projects/TFXcan/imlab/users/sabrina/enformer_rat_pipeline/config_files/run_on_polaris_personalized.json on polaris):\n{\n  \"individuals\": \"/lus/grand/projects/TFXcan/imlab/users/sabrina/enformer_rat_pipeline/metadata/individuals.txt\",\n  \"project_dir\": \"/lus/grand/projects/TFXcan/imlab/users/sabrina/enformer_rat_pipeline/\",\n  \"interval_list_file\": \"/lus/grand/projects/TFXcan/imlab/users/sabrina/enformer_rat_pipeline/metadata/intervals.txt\",\n  \"prediction_data_name\": \"personalized_enformer_minimal\",\n  \"prediction_id\": \"some_regions\",\n  \"reverse_complement\": false,\n  \"date\": null,\n  \"exclude_regions\": true,\n  \"n_individuals\": 8,\n  \"batch_individuals\": 4,\n  \"vcf_files\": {\n    \"folder\": \"/home/s1mi/enformer_rat_data/BrainVCFs\",\n    \"files\": {\n      \"chr1\": \"chr1.vcf.gz\",\n      \"chr2\": \"chr2.vcf.gz\",\n      \"chr3\": \"chr3.vcf.gz\",\n      \"chr4\": \"chr4.vcf.gz\",\n      \"chr5\": \"chr5.vcf.gz\",\n      \"chr6\": \"chr6.vcf.gz\",\n      \"chr7\": \"chr7.vcf.gz\",\n      \"chr8\": \"chr8.vcf.gz\",\n      \"chr9\": \"chr9.vcf.gz\",\n      \"chr10\": \"chr10.vcf.gz\",\n      \"chr11\": \"chr11.vcf.gz\",\n      \"chr12\": \"chr12.vcf.gz\",\n      \"chr13\": \"chr13.vcf.gz\",\n      \"chr14\": \"chr14.vcf.gz\",\n      \"chr15\": \"chr15.vcf.gz\",\n      \"chr16\": \"chr16.vcf.gz\",\n      \"chr17\": \"chr17.vcf.gz\",\n      \"chr18\": \"chr18.vcf.gz\",\n      \"chr19\": \"chr19.vcf.gz\",\n      \"chr20\": \"chr20.vcf.gz\"\n    }\n  },\n  \"sub_dir\": true,\n  \"use_parsl\": true,\n  \"model_path\": \"/lus/grand/projects/TFXcan/imlab/data/enformer/raw\",\n  \"fasta_file\": \"/lus/grand/projects/TFXcan/imlab/users/sabrina/enformer_rat_data/rn7_genome.fasta\",\n  \"metadata_dir\": \"/lus/grand/projects/TFXcan/imlab/users/sabrina/enformer_rat_pipeline/metadata\",\n  \"output_dir\": \"enformer_predictions\",\n  \"tracks_to_save\":-1,\n  \"bins_to_save\": -1,\n  \"sequence_source\": \"personalized\",\n  \"predictions_log_dir\": \"predictions_log\",\n  \"batch_regions\": 3,\n  \"n_regions\": 12,\n  \"write_log\": {\n    \"logdir\": \"job_logs\",\n    \"logtypes\": {\n      \"memory\": true,\n      \"error\": true,\n      \"time\": false,\n      \"cache\": true\n    }\n  },\n  \"parsl_parameters\": {\n    \"job_name\": \"enformer_predict_regions_personalized\",\n    \"num_of_full_nodes\": 1,\n    \"walltime\": \"00:30:00\",\n    \"init_blocks\":1,\n    \"min_num_blocks\": 0,\n    \"max_num_blocks\": 4,\n    \"queue\": \"preemptable\",\n    \"account\": \"TFXcan\",\n    \"hpc\": \"polaris\",\n    \"provider\": \"highthroughput\",\n    \"worker_init\":\"source ~/.bashrc; module load conda; conda activate /lus/grand/projects/TFXcan/imlab/shared/software/conda_envs/enformer-predict-tools; which python; export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/lus/grand/projects/TFXcan/imlab/shared/software/conda_envs/enformer-predict-tools/lib\"\n  }\ncd /home/s1mi/Github/shared_folder/enformer_pipeline\n\nmodule load conda\n\nconda activate /lus/grand/projects/TFXcan/imlab/shared/software/conda_envs/enformer-predict-tools\n\npython3 scripts/enformer_predict.py --parameters /home/s1mi/enformer_rat_pipeline/config_files/run_on_polaris_personalized.json"
  }
]